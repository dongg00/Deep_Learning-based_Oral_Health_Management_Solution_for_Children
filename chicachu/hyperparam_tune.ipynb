{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbdc93e5",
   "metadata": {},
   "source": [
    "# 📌 서론\n",
    "\n",
    "본 연구에서는 시계열 및 랜드마크 기반 입력을 활용한 분류 모델의 성능을 극대화하기 위해 **단계적 최적화 절차**를 수행하였다.  \n",
    "먼저 **LOSO 교차검증(Baseline)**으로 초기 성능을 확인한 뒤, 단일 폴드를 기준으로 다음과 같은 과정을 순차적으로 진행하였다.\n",
    "\n",
    "1. **모델 아키텍처 변형** → GRU, BiGRU, GRU+Attention, CNN-GRU 등 비교  \n",
    "2. **입력 특징 조합 탐색** → 얼굴/손 랜드마크, 시계열 파생 특징(velocity, acceleration, segment_stats)  \n",
    "3. **Optuna 기반 하이퍼파라미터 탐색** → 성능과 학습 효율을 동시에 고려한 자동 최적화  \n",
    "4. **최종 LOSO 교차검증 재실행** → 베이스라인 대비 성능 향상 검증  \n",
    "\n",
    "추가적으로, 최고 성능 Fold 모델을 선별하여 단일 배포용으로 준비하고,  \n",
    "모바일 환경 호환성을 위해 **TFLite 변환(unroll GRU 등 호환성 전략 포함)**을 적용하였다.  \n",
    "\n",
    "이러한 체계적 절차는 **성능·효율성·배포 가능성**을 모두 고려한  \n",
    "**실전형 최적화 파이프라인**으로서, 향후 유사 문제 해결에도 재사용 가능한 프레임워크를 제시한다.\n",
    "\n",
    "---\n",
    "\n",
    "![Figure 1. 최종 최적화 파이프라인 개요](./image.png)  \n",
    "*Figure 1. 단계별 최적화 및 검증 과정을 요약한 전체 파이프라인 개요*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1bb41a",
   "metadata": {},
   "source": [
    "# # 실험 준비: 라이브러리 임포트 및 공통 함수\n",
    "\n",
    "🔹 1. 기본 유틸\n",
    "\n",
    "os, itertools, copy, time\n",
    "→ 파일/디렉토리 관리, 반복 처리, 객체 복사, 실행 시간 측정 등 기본 기능 제공\n",
    "\n",
    "🔹 2. 데이터 분석 & 시각화\n",
    "\n",
    "pandas, numpy → 데이터 처리 및 수치 연산\n",
    "\n",
    "seaborn, matplotlib.pyplot → 데이터 시각화 라이브러리\n",
    "\n",
    "koreanize_matplotlib → matplotlib 그래프에 한글 폰트 적용\n",
    "\n",
    "🔹 3. 머신러닝 / 딥러닝\n",
    "\n",
    "scipy.stats → 통계적 분석 도구\n",
    "\n",
    "sklearn.metrics.confusion_matrix, sklearn.model_selection.train_test_split\n",
    "→ 평가 지표 및 데이터 분할\n",
    "\n",
    "tensorflow → 딥러닝 모델 구현 및 학습\n",
    "\n",
    "optuna → 하이퍼파라미터 최적화\n",
    "\n",
    "🔹 4. 편의 기능\n",
    "\n",
    "tqdm → 진행 상황 시각화 (progress bar)\n",
    "\n",
    "IPython.display.Image, display → 노트북 내 이미지/객체 표시\n",
    "\n",
    "🔹 5. 프로젝트 내부 모듈\n",
    "\n",
    "config → 설정 파일\n",
    "\n",
    "runner, pipeline → 실행 및 파이프라인 관리\n",
    "\n",
    "feature_extraction, train, evaluate → 단계별 모듈\n",
    "(특징 추출 → 학습 → 평가)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb75af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "준비 완료. 다음 셀부터 실험을 시작하세요.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import itertools\n",
    "import copy\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import optuna\n",
    "from scipy import stats\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm  # 텍스트 진행바 (ipywidgets 불필요)\n",
    "\n",
    "from IPython.display import Image, display  # 이미지를 노트북에 표시하기 위해 추가\n",
    "\n",
    "import config\n",
    "import run as runner\n",
    "import main as pipeline\n",
    "import m1_feature_extraction_answer as feature_extraction\n",
    "import m2_train_answer as train\n",
    "import m3_evaluate_answer as evaluate\n",
    "\n",
    "# 시각화 한글 설정\n",
    "try:\n",
    "    import koreanize_matplotlib\n",
    "except ImportError:\n",
    "    print(\"koreanize-matplotlib이 설치되지 않았습니다. 그래프의 한글이 깨질 수 있습니다.\")\n",
    "\n",
    "print(\"준비 완료. 다음 셀부터 실험을 시작하세요.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1818afa",
   "metadata": {},
   "source": [
    "# # 단일 폴드 실행 함수 정의\n",
    "\n",
    "🧪 단일 폴드 실행 함수: run_single_fold\n",
    "\n",
    "지정한 fold 하나만 학습·검증·테스트하고 Macro F1를 반환합니다.\n",
    "(학습 데이터 일부만 쓰고 싶다면 data_subset_fraction으로 비율을 지정)\n",
    "\n",
    "⚙️ 시그니처\n",
    "run_single_fold(config_dict, fold_id=1, data_subset_fraction=1.0)\n",
    "\n",
    "🔢 매개변수\n",
    "\n",
    "config_dict: 전체 파이프라인 설정 딕셔너리 (paths, settings 등 포함)\n",
    "\n",
    "fold_id (int, default=1): 실행할 폴드 번호 (1-based)\n",
    "\n",
    "data_subset_fraction (float, default=1.0): 학습 데이터 샘플링 비율 (예: 0.3 → 30%)\n",
    "\n",
    "🧰 의존성\n",
    "\n",
    "데이터/유틸: numpy as np, copy, os\n",
    "\n",
    "스플릿: sklearn.model_selection.train_test_split\n",
    "\n",
    "내부 모듈: runner, train, evaluate\n",
    "\n",
    "설정 키:\n",
    "\n",
    "cfg['settings']['VALIDATION_SPLIT_FROM_TRAIN']\n",
    "\n",
    "cfg['settings'].get('SEED', 42)\n",
    "\n",
    "cfg['paths']['OUTPUT_DIR']\n",
    "\n",
    "🔁 동작 흐름\n",
    "\n",
    "설정 복사: config_dict를 deepcopy하여 내부에서 안전하게 사용\n",
    "\n",
    "피처 로딩/생성: runner.get_or_create_features(cfg) 결과(.npz)에서 X, y, groups, class_names 로드\n",
    "\n",
    "폴드 분할: groups 기준으로 fold_id에 해당하는 그룹을 테스트, 나머지를 train pool\n",
    "\n",
    "검증 분할: train pool에서 train/val 스플릿 (라벨 stratify)\n",
    "\n",
    "옵션 샘플링: data_subset_fraction < 1.0이면 학습 집합만 비율 샘플링\n",
    "\n",
    "학습: train.main(...)으로 모델 학습 및 best_model.h5 저장\n",
    "\n",
    "평가: evaluate.main(...)으로 테스트 세트 성능 산출 → report_df['macro avg', 'f1-score']\n",
    "\n",
    "반환: Macro F1 (float)\n",
    "\n",
    "📤 반환값\n",
    "\n",
    "Macro F1 (float): 테스트 세트의 macro-averaged F1-score\n",
    "\n",
    "🧯 예외/엣지 케이스 팁\n",
    "\n",
    "fold_id가 1 … len(unique_groups) 범위를 벗어나면 인덱싱 에러 가능\n",
    "\n",
    "data_subset_fraction은 (0, 1] 범위를 권장 (너무 작으면 stratify 실패 가능)\n",
    "\n",
    "runner.get_or_create_features가 실패하여 None/'' 반환 시 0.0 반환\n",
    "\n",
    "▶️ 사용 예시\n",
    "macro_f1 = run_single_fold(config, fold_id=2, data_subset_fraction=0.5)\n",
    "print(f\"Fold-2 Macro F1 = {macro_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b88fc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_fold(config_dict, fold_id: int = 1, data_subset_fraction: float = 1.0):\n",
    "    \"\"\"\n",
    "    지정된 단일 폴드(fold_id)에 대해서만 학습 및 평가를 실행하고,\n",
    "    테스트 세트의 Macro F1 Score를 반환하는 함수.\n",
    "    \n",
    "    Args:\n",
    "        config_dict (dict): 실험 설정 딕셔너리\n",
    "        fold_id (int): 실행할 폴드 번호 (1-based index)\n",
    "        data_subset_fraction (float): 학습 데이터 샘플링 비율 (0 < fraction ≤ 1.0)\n",
    "\n",
    "    Returns:\n",
    "        float: 테스트 세트 Macro F1 Score\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"--- 단일 폴드(Fold-{fold_id}) 테스트 시작 \"\n",
    "          f\"(데이터 샘플링: {data_subset_fraction*100:.1f}%) ---\")\n",
    "\n",
    "    # ✅ 설정 복사\n",
    "    cfg = copy.deepcopy(config_dict)\n",
    "\n",
    "    # ✅ 피처 로딩\n",
    "    features_path = runner.get_or_create_features(cfg)\n",
    "    if not features_path:\n",
    "        return 0.0\n",
    "\n",
    "    with np.load(features_path, allow_pickle=True) as data:\n",
    "        X, y, groups = data['X'], data['y'], data['groups']\n",
    "        class_names = data['class_names']\n",
    "\n",
    "    # ✅ 그룹 기반 train/test 분리\n",
    "    unique_groups = np.sort(np.unique(groups))\n",
    "    test_group_id = unique_groups[fold_id - 1]\n",
    "    test_indices = np.where(groups == test_group_id)[0]\n",
    "    train_indices = np.where(groups != test_group_id)[0]\n",
    "\n",
    "    # ✅ train/validation 분리\n",
    "    tr_idx, va_idx = train_test_split(\n",
    "        train_indices,\n",
    "        test_size=cfg['settings']['VALIDATION_SPLIT_FROM_TRAIN'],\n",
    "        random_state=cfg['settings'].get('SEED', 42),\n",
    "        stratify=y[train_indices]\n",
    "    )\n",
    "\n",
    "    X_train, y_train = X[tr_idx], y[tr_idx]\n",
    "    X_val, y_val = X[va_idx], y[va_idx]\n",
    "    X_test, y_test = X[test_indices], y[test_indices]\n",
    "\n",
    "    # ✅ 학습 데이터 샘플링 (옵션)\n",
    "    if data_subset_fraction < 1.0:\n",
    "        subset_indices, _ = train_test_split(\n",
    "            np.arange(len(X_train)),\n",
    "            train_size=data_subset_fraction,\n",
    "            random_state=cfg['settings'].get('SEED', 42),\n",
    "            stratify=y_train\n",
    "        )\n",
    "        X_train, y_train = X_train[subset_indices], y_train[subset_indices]\n",
    "        print(f\"학습 데이터가 {len(subset_indices)}개로 샘플링 되었습니다.\")\n",
    "\n",
    "    # ✅ 모델 저장 경로 설정\n",
    "    output_dir = cfg['paths']['OUTPUT_DIR']\n",
    "    fold_model_dir = os.path.join(output_dir, 'models', f'fold_{fold_id}')\n",
    "    os.makedirs(fold_model_dir, exist_ok=True)\n",
    "    model_save_path = os.path.join(fold_model_dir, 'best_model.h5')\n",
    "\n",
    "    # ✅ 학습 및 평가\n",
    "    train.main(\n",
    "        X_train, y_train,\n",
    "        X_val, y_val,\n",
    "        len(class_names),\n",
    "        model_save_path,\n",
    "        cfg\n",
    "    )\n",
    "\n",
    "    report_df = evaluate.main(\n",
    "        X_test, y_test,\n",
    "        class_names,\n",
    "        model_save_path,\n",
    "        fold_model_dir,\n",
    "        cfg\n",
    "    )\n",
    "\n",
    "    if report_df is None:\n",
    "        return 0.0\n",
    "\n",
    "    macro_f1 = report_df.loc['macro avg', 'f1-score']\n",
    "    print(f\"--- 단일 폴드(Fold-{fold_id}) 테스트 완료 | Macro F1: {macro_f1:.4f} ---\")\n",
    "\n",
    "    return macro_f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0eba5a5",
   "metadata": {},
   "source": [
    "# # 1단계: LOSO 교차 검증 성능 확인\n",
    "\n",
    "🧪 LOSO 교차 검증 결과 재사용/실행 및 성능 시각화\n",
    "\n",
    "이 셀은 LOSO 교차 검증 결과가 이미 있으면 재사용하고, 없으면 파이프라인을 실행하여 결과를 생성한 뒤\n",
    "각 폴드의 Macro F1 Score를 통계 및 시각화로 확인합니다.\n",
    "\n",
    "🔧 단계 요약\n",
    "\n",
    "설정 복사 & 출력 경로 지정\n",
    "\n",
    "config.MANUAL_CONFIG를 깊은 복사 후 OUTPUT_DIR만 실험 폴더로 교체\n",
    "\n",
    "결과 재사용 여부 판단\n",
    "\n",
    "final_loso_cv_report.csv가 있으면 재실행 생략\n",
    "\n",
    "없으면 runner.get_or_create_features로 특징 생성 → pipeline.run_pipeline 실행\n",
    "\n",
    "결과 로드 및 통계\n",
    "\n",
    "macro avg 행에서 f1-score만 추출하여 기술통계 출력\n",
    "\n",
    "시각화\n",
    "\n",
    "boxplot + stripplot으로 폴드별 Macro F1 분포 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10d2c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 기존 LOSO 결과 파일('outputs/step1_loso_test\\models\\final_loso_cv_report.csv')을 찾았습니다. 재실행을 건너뛰고 결과를 재사용합니다.\n",
      "\n",
      "[LOSO Macro F1 Score 통계]\n",
      "count    26.000000\n",
      "mean      0.770618\n",
      "std       0.139499\n",
      "min       0.473250\n",
      "25%       0.710164\n",
      "50%       0.768498\n",
      "75%       0.862431\n",
      "max       0.993389\n",
      "Name: f1-score, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1) 설정 및 경로 정의\n",
    "manual_run_config = copy.deepcopy(config.MANUAL_CONFIG)\n",
    "manual_run_config['paths']['OUTPUT_DIR'] = 'outputs/step1_loso_test'\n",
    "\n",
    "report_path = os.path.join(\n",
    "    manual_run_config['paths']['OUTPUT_DIR'],\n",
    "    'models',\n",
    "    'final_loso_cv_report.csv'\n",
    ")\n",
    "\n",
    "# 2) 기존 결과 확인 및 파이프라인 실행\n",
    "if os.path.exists(report_path):\n",
    "    print(f\"✅ 기존 LOSO 결과 파일('{report_path}')을 찾았습니다. \"\n",
    "          f\"재실행을 건너뛰고 결과를 재사용합니다.\")\n",
    "else:\n",
    "    print(\"⏳ 기존 LOSO 결과가 없습니다. 전체 교차 검증을 새로 시작합니다. \"\n",
    "          \"시간이 다소 소요될 수 있습니다...\")\n",
    "\n",
    "    # run.py와 동일하게 특징 파일 경로를 먼저 확보\n",
    "    features_path = runner.get_or_create_features(manual_run_config)\n",
    "    if not features_path:\n",
    "        raise FileNotFoundError(\"특징 파일 생성에 실패하여 중단합니다.\")\n",
    "\n",
    "    manual_run_config['paths']['FEATURES_PATH'] = features_path\n",
    "\n",
    "    # 파이프라인 실행\n",
    "    pipeline.run_pipeline(manual_run_config)\n",
    "    print(\"✅ LOSO 교차 검증 완료.\")\n",
    "\n",
    "# 3) 결과 파일 로드\n",
    "loso_report_df = pd.read_csv(report_path)\n",
    "\n",
    "# 4) Macro F1 Score 추출 및 통계 출력\n",
    "macro_f1_scores_df = (\n",
    "    loso_report_df\n",
    "    [loso_report_df['Unnamed: 0'] == 'macro avg'][['fold', 'f1-score']]\n",
    "    .set_index('fold')\n",
    ")\n",
    "print(\"\\n[LOSO Macro F1 Score 통계]\")\n",
    "print(macro_f1_scores_df['f1-score'].describe())\n",
    "\n",
    "# 5) 시각화\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x=macro_f1_scores_df['f1-score'], orient='h')\n",
    "sns.stripplot(x=macro_f1_scores_df['f1-score'], color=\".25\", orient='h')\n",
    "plt.title('모든 Fold에 대한 Macro F1 Score 분포')\n",
    "plt.xlabel('Macro F1 Score')\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23e206a",
   "metadata": {},
   "source": [
    "📊 Fold 대표성 검증 (부트스트랩 신뢰구간)\n",
    "\n",
    "이 셀은 특정 Fold 성능이 전체 평균과 얼마나 일치하는지 검증합니다.\n",
    "부트스트랩 방식으로 신뢰구간(95% CI)을 계산하여, 선택한 Fold가 대표성 있는 성능인지 확인합니다.\n",
    "\n",
    "🔧 단계 요약\n",
    "\n",
    "데이터 준비\n",
    "\n",
    "tuning_fold_id: 대표성 여부를 확인할 기준 Fold\n",
    "\n",
    "fold_score: 해당 Fold의 Macro F1\n",
    "\n",
    "others: 나머지 Fold들의 성능\n",
    "\n",
    "전체 평균 신뢰구간 (bootstrap)\n",
    "\n",
    "전체 Fold 점수를 대상으로 n_boot = 10,000번 샘플링\n",
    "\n",
    "평균 F1의 95% CI 계산 → 기준 Fold 점수가 포함되는지 확인\n",
    "\n",
    "Fold vs Others 평균 차이 (bootstrap)\n",
    "\n",
    "기준 Fold와 나머지 Fold 평균 차이를 반복 샘플링\n",
    "\n",
    "95% CI에 0이 포함되면 유의한 차이 없음\n",
    "\n",
    "포함되지 않으면 해당 Fold는 특이 Fold일 가능성 존재\n",
    "\n",
    "📌 출력 예시\n",
    "\n",
    "fold_7 점수: 0.8123\n",
    "\n",
    "다른 Fold 평균: 0.8051\n",
    "\n",
    "[전체 평균 F1 신뢰구간] 95% CI = [0.7901, 0.8234]\n",
    "\n",
    "✅ fold_7 점수는 전체 평균의 95% 신뢰구간 안에 있음 → 대표성 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79da8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold_7 점수: 0.7395\n",
      "다른 Fold 평균: 0.7719\n",
      "\n",
      "[전체 평균 F1 신뢰구간] 95% CI = [0.7165, 0.8222]\n",
      "✅ fold_7 점수는 전체 평균의 95% 신뢰구간 안에 있음 → 대표성 있음\n",
      "\n",
      "[fold_7 vs Others 평균 차이 신뢰구간] 95% CI = [-0.0854, 0.0214]\n",
      "✅ fold_7와 나머지 평균의 차이는 유의하지 않음 (p≈0.05 기준)\n"
     ]
    }
   ],
   "source": [
    "# === 1. 데이터 준비 ===\n",
    "scores = macro_f1_scores_df['f1-score'].values   # 전체 fold 점수\n",
    "tuning_fold_id = \"fold_7\"                        # 기준 fold (예: fold_7)\n",
    "fold_score = macro_f1_scores_df.loc[tuning_fold_id, 'f1-score']\n",
    "others = macro_f1_scores_df.drop(tuning_fold_id)['f1-score'].values\n",
    "\n",
    "print(f\"{tuning_fold_id} 점수: {fold_score:.4f}\")\n",
    "print(f\"다른 Fold 평균: {others.mean():.4f}\")\n",
    "\n",
    "# === 2. 전체 평균 신뢰구간 (bootstrap) ===\n",
    "n_boot = 10000\n",
    "boot_means = [\n",
    "    np.mean(np.random.choice(scores, size=len(scores), replace=True))\n",
    "    for _ in range(n_boot)\n",
    "]\n",
    "\n",
    "ci_lower, ci_upper = np.percentile(boot_means, [2.5, 97.5])\n",
    "print(f\"\\n[전체 평균 F1 신뢰구간] 95% CI = [{ci_lower:.4f}, {ci_upper:.4f}]\")\n",
    "\n",
    "if ci_lower <= fold_score <= ci_upper:\n",
    "    print(f\"✅ {tuning_fold_id} 점수는 전체 평균의 95% 신뢰구간 안에 있음 → 대표성 있음\")\n",
    "else:\n",
    "    print(f\"⚠️ {tuning_fold_id} 점수는 평균 신뢰구간 밖에 있음 → 특이값일 수 있음\")\n",
    "\n",
    "# === 3. Fold vs 나머지 평균 차이 신뢰구간 (bootstrap) ===\n",
    "diffs = []\n",
    "for _ in range(n_boot):\n",
    "    boot_others = np.mean(np.random.choice(others, size=len(others), replace=True))\n",
    "    diffs.append(fold_score - boot_others)\n",
    "\n",
    "ci_diff_lower, ci_diff_upper = np.percentile(diffs, [2.5, 97.5])\n",
    "print(f\"\\n[{tuning_fold_id} vs Others 평균 차이 신뢰구간] \"\n",
    "      f\"95% CI = [{ci_diff_lower:.4f}, {ci_diff_upper:.4f}]\")\n",
    "\n",
    "if ci_diff_lower <= 0 <= ci_diff_upper:\n",
    "    print(f\"✅ {tuning_fold_id}와 나머지 평균의 차이는 유의하지 않음 (p≈0.05 기준)\")\n",
    "else:\n",
    "    print(f\"⚠️ {tuning_fold_id}와 나머지 평균이 유의하게 다를 수 있음\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50abd3ae",
   "metadata": {},
   "source": [
    "🎯 튜닝 기준 Fold 설정\n",
    "\n",
    "이후의 하이퍼파라미터 탐색 및 튜닝은 특정 Fold를 대표 Fold로 삼아 진행합니다.\n",
    "여기서는 **fold_7**을 기준으로 고정하여, 해당 Fold의 성능을 튜닝 지표로 활용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f93ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "선택 Fold: fold_7 (F1: 0.7395)\n",
      "이후 모든 튜닝은 fold_7를 기준으로 진행됩니다.\n"
     ]
    }
   ],
   "source": [
    "# 4. 튜닝 기준 Fold를 fold_7로 고정\n",
    "tuning_fold_id = \"fold_7\"\n",
    "\n",
    "print(f\"\\n선택 Fold: {tuning_fold_id} \"\n",
    "      f\"(F1: {macro_f1_scores_df.loc[tuning_fold_id, 'f1-score']:.4f})\")\n",
    "print(f\"이후 모든 튜닝은 {tuning_fold_id}를 기준으로 진행됩니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1bf0ad",
   "metadata": {},
   "source": [
    "# # 모델 아키텍처(`variant`) 성능 비교\n",
    "\n",
    "🏗️ 아키텍처 후보 비교 (단일 Fold 튜닝 기준)\n",
    "\n",
    "선택한 **튜닝 기준 Fold({tuning_fold_id})**에서, 지정한 아키텍처 후보들의 Macro F1을 비교합니다.\n",
    "기존 결과가 있으면 재사용, 없으면 단일 폴드 평가를 새로 수행합니다.\n",
    "\n",
    "🔧 단계 요약\n",
    "\n",
    "결과 캐시 확인\n",
    "\n",
    "경로: outputs/step2_arch_test/<variant>/models/{tuning_fold_id}/classification_report.csv\n",
    "\n",
    "전부 있으면 로드, 하나라도 없으면 해당 변형(variant)을 재실행\n",
    "\n",
    "단일 폴드 평가\n",
    "\n",
    "run_single_fold(test_cfg, fold_id=<int>) 호출\n",
    "\n",
    "test_cfg['model_arch']['variant']로 모델 변형 지정\n",
    "\n",
    "시각화\n",
    "\n",
    "barplot으로 아키텍처별 Macro F1 비교\n",
    "\n",
    "최적 후보 출력\n",
    "\n",
    "최고 성능 아키텍처와 점수 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c648343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 기존 아키텍처 테스트 결과('outputs/step2_arch_test')를 찾았습니다. 결과를 재사용합니다.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA14AAAIqCAYAAAA0HJuRAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjUsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvWftoOwAAAAlwSFlzAAAPYQAAD2EBqD+naQAARRBJREFUeJzt3QuYVWW9P/CXmxJXUcPyxkVS85IpgmRHTREV0Y4pYZYpBaEFqWEJJ0nFlBDUtLzl7Wje4njNzCtpWHqkvOAlLTFIwFRKAUEBQfg/v/c8e/5z2QMDzGIG5vN5nv0Ms/Zea9bes9ewvvv3vr/VbOXKlSsTAAAAhWle3KYBAAAIghcAAEDBBC8AAICCCV4AAAAFE7wAAAAKJngBAAAUTPACAAAomOAFAABQMMELAACgYIIXAKyBlStXNvQuNFoffvhh+ta3vpW/bgz+/ve/pzPPPLOhdwPYSAheAPXoxRdfTD179kw77bRTrbedd945n5xW96UvfanGYz/96U+nefPm5ftvuOGGdNZZZ+V/X3DBBenyyy9f7f4sWrQojRkzJu2zzz5p7733Tj/4wQ/SggULqjzmF7/4RfrhD39YY90VK1ak5cuX17hV9/TTT6eDDz64yrLf/e53aY899qjxfP785z9XPKZXr17p7bffTu+99176zGc+kz766KMq23j22WdrrH///ffX+Pm77757+uc//1ll2eDBg9Ovf/3rGo+tvvy+++5LJ554YloTsf5xxx1X9r433ngj7085Rx99dHrwwQdTfYjf6ZVXXllj+b333pufY0m8LuX259FHH13le7R0O/zww9coRMV78rDDDkubbLJJxbJ4zvvvv3/ac8890yuvvLJGz7Nfv351es2GDBlS9ve9rnbYYYd8HPzhD3+o920DTU/Lht4BgI3J3/72t9StW7d02223pWbNmpV9zNSpU9P3vve9KssidLz88ss5WHTp0qVieWyjRYsW+d9xAlw6CV66dGmVk9vanHLKKTksXX/99ally5bp4osvziepsX+tWrWqsd3K4gQ+9rW69u3bp0mTJuWT0trWf+mll9JBBx2Uxo8fX2X5pptuWvHvJUuWpGXLluX9iOdTvZK011575fBV2cc+9rEa+xM/O7ZT/fWsHuTKLa/tua9KBNd27dqVvS9e69q2V9s+rY3aQnAsq/wz4nUptz/xu/nLX/6yyp8R60ZYj3C83XbbrXafIuQ98MAD+VbZj3/84/TNb34zHXHEEalDhw5pTdT1NYvnuCav7dy5c3MYrK16OWrUqLzP4eSTT04nnXRSPjabN/d5NbD2BC+AehQnchEOSqGmnLZt2+ZP0auvVwomEZDqw5NPPpleeOGFXH3q2LFjXvazn/0sHXroobkycswxx6xy/V/+8pc1lsV+f+5zn0vvvPNORfAqJx4Xz6Vy0Fob8VqtrVLFrvrrfMUVV6Rf/epX+ft33303ffzjH1/j4We1Ba8NyereZ3F/hPtyAa+cqMh+8YtfrPigIMyfPz/9+9//Tscff/w6vxfqU+fOnXP1tXrwivB25JFHVnlvxwch8R6JKmH1yi7AmhC8ABqxCDkRDioPR6yryZMnp/32268idIU4+Y3hY1GVWF3wKmfhwoX5ZLpyVa4u4gQ3KlzlKlbl/PSnP01XXXVV2SAW1bvPfvazq91GzM0pNz8nhnnG6xJiCNkzzzyT6qo07Cx+J//617/yCXkMVXzqqafSxiiqXpWDVG0inN111135Vtn777+fvzam0FW5clvdI488ksNm6f1REmHs9ttvF7yAdSJ4ATRiU6ZMqfhU/rrrrkuzZs2q87ozZsxIffv2rbE85nuVmyu1KlEJWLx4cZ4T1b1797TVVlutdp2Yc/Pb3/62IrDEehMnTkzDhg3Ly1Y1xO+0005L3/3ud6ssmz17dp4/VNeK4E9+8pM8r6qyr3/963n9UhBY0+pinJh/8pOfTJ///OfzvLiYH/ff//3fFRXM+P3079+/7LoRYCJ8rq6CVF8Vz3UVv/MIXnUJyzG3sU2bNmn77bevWLbbbrtVDAGN+WLxvErDG+M9fdNNN+Uhr/F77dSpU67Exu99VdXEeM9ccskluWIbQz533HHHdOqpp6b6cuutt6ZBgwbVGFLYu3fv/H4qDY0FWBuN4687QBNXOtGLZhhxi5ATVZXXX389zZkzJ1dV1vSEPIZ4xQltuWFWUa2JsFDbnJU4wY3QFkEh9iXmmkVYiYpXNOioixh2dv755+efEyersY34d5zchghRtYnHVn++V199dW42Eif0dVF93ldp2bRp03JDj1I4rav4fcR8pQiP0bDia1/7Wp4LNG7cuIqT8VVVh2K+1I9+9KOyjUwqi6YZMQdrdS677LJ8qy5CQn0oNWGpy7yseE2jmUplUQWM9+5//ud/5mF9ld9rEyZMSHfccUc655xz8gcB0ZQkgk2E8ghktb2O8ZiHHnoo/x6iIcvzzz+f34/1EYZmzpyZ9zN+v9V17do1V8KmT5+edtlll3X+WUDTJHgB1KMIDBFU4gS/tuYaMfyqeuCJ7+PEPE5SY704ydtss81Sjx498qf65UJEffrNb36TbyGqYTHHJapVEX6i4hH7FFWGOGH+yle+UqdtlgtP8Ty33XbbivvrKk6w77nnnnTRRRdVVGNW1dY99j9O6ktdIEuiKnPNNdfk1zrE0LgYQrY6EYZHjhyZh5rF8M8QQx6//e1v5+GfEQq33nrrVW4jAtfqQteaGDFiRI2qYDyfu+++u87biAAa891KVchoZhKB68ADD8zdNKP6VJdhgtGsovpcuVi3VL2qHN4i7MZ8sBtvvLEiJG655Zbp2muvzYEzKqXVK5WlfY3f1YUXXlhRyY3fRxwbUSlbV1F9i58f+1JOLI8PLADWluAFUI8iKEXzhVVVZSJwfOELX6ix/JZbbsnD0GqrbMWclAhjdRUnipXnh1U+SY6qV+XwFx3nonJTCn3VT5b/8Y9/5OpV3OrSWCIqEFEdi5AZVa44aY7qx2uvvZarVtGxsK6iq16EjKhwxPy02FafPn1W2cUuQlfcVqcu4S+qMRFMorNftHEv2WKLLfLv7H/+53/yv2urBB177LFpbUQHygEDBqQiRZCIMFsKXk888UQORhG8okvh6sJkSfyua3sNqotmL3GcVK/Mxfsq5lJFRatc8IpOmaF6NTBazpebr7UmorIbgTWaz9QmfkapUgqwNgQvgHoUTR+ee+65KsviJDJauNflJHpVwwkrXzsqKmOrG14VJ7dx4n/CCSfUGAIWVbTKSkMJy4nAFK21o9JV1+YCEYxiyFicXEeQiwYfMf8nKlEx3ye0bt16tc8hQtfQoUNzCIxhlzHn7YADDsit90tK21sbETCqD5GrLgLeIYccksNf9SFwsf8x5HBV74fVtW1vrHO94vcWw0XrIkJJhK+6iDld8d4sJ94f0Y2znOikGR8mVL+MQrxOpSrq2opq7+abb57ft7WJ57em7fABKhO8ABqZmLcTLc9XJUJSVERWNd8kKgERFmLoWKmzYXyyH0MIv//979dpX+I6XjGM66ijjqpx7bFViaF85a4BVlnpYspxDa8IdtVDTQTYaJwQITEaKkTFI76PeT51mQMVInTGBZ5rE1W/mD+3qnlrERgrN3CI1zOuYxaVmagKRVUvhtnFvK+oHMY8rqIDVATlcs1J4rWsLzH3Km51ERXU6h84rG2Vsbb7I6jXNry0+uUZ1maYYTTVWNW+xZzJ2oYhAtSF4AXQyERYituqxHyjuEZXVGxqEyfNUXEZPnx4+q//+q8cAGJ+zDbbbLPa6lt8uh9NBqISEMPr1qb1fKliFReqXZ2oOpVOeiM8RLiKuT7R+j0uXhuhLKoREUjjtYlKTDyn1Sl3LbLKYphg/Jy6itD15S9/OQ87jN9BhMJocR/PM6px0b6+tjlwMYw0ukI++OCDuQoWJ/IhhuhF1S7C5MCBA+s0pyp+bvx+Ym5ZuSC5vsUw0JizVRdxKYLa5qHFMMdoZFFOdJOMqlcEzspVr5hTGVW0tRVV4WiasapLNUS1NX5u9UoxwJoQvADqQXwSv6o5R+Uu5hsibNTlOklrKypFEWKiohT7F0MFo1K2up8Zc8Piel3RUCNCxtqKtvPRanxVjTBi/lgEqdi/2K9SU45o8hEn25X17Nkz3Xnnnemxxx5L9WFNKyURnCIYRYOOynPkunXrlm8xdy2CVwwtrT4sLRpxRHAYPHhwDmhRJYrnGnPuIkRHSIyge/PNN6+2SvaNb3wj39ZG5fdh6fmXlsX38bsq915d1T5F8Ir5TxGAVvd+idAf12mLimjlilqE/Xi/xVzDciKcRpfOmCNWuWV/NBT54IMP0tqKLpsxlDSGGtbmT3/6U67iaiUPrAvBC6AexIl4qeNeOTG0r9zwvjjxfvzxx/NJeEkEjtGjR69y2Fh0GoxQtTpRjTnvvPPybU1EVWJVjQbWRPU5OeX2MZTCWTy+chOL6qLhw6rmVVUWzzuGkdUW/CLoRShdE3HyXVsb/tqqVVERi99zDE+sXtGJ+Ulx23fffXMQiQYkO++8cypCDDONSl11u+66a5Xv4wLb1cXwytouXB2vScxljOrh6qq1EcbjNT/99NPz7zkCTVz/7Oyzz87Pu9y150o/IyqgY8eOzf+OKmnMV4z2+zE3bG1E58aoQEaHylWJQBjXgANYF4IXQD2IrnClznDrKoZbxfC8ctdoYs387W9/y/O3ospUH6JaEyfp0cY9hvR96lOfyiE4hg1GR8BLL700h4Pq1a4I1nHR5diXqFRFhagUtmPdmL8WFa8IE2sbIuq6/0V1SozXOG5R2StVVKNKVq5SFvMF4/lHuI/QFdWm2K94XSvPs4oQXjnMxpy9aB8f3TWjehhzHCN4RRW3LkM0q4ugGEG4dHmB2iqyUZWsa2MZgNo0W7mq8R8ArHdR8YoL85ZrnlBZDG2LT+vX1S9+8Ys8h6W2IV6rE80rokI3efLktVo/uiZGlSPmPa1NI4poahGVpHKtz6PiFS3fVzWksFzVcXVVkhieFq997Hs0LIl5WrEfMcwwui6WE7/PmNv08MMPp1deeSUHhxDD52IYXZzYx/yxaCJRX6L5x6GHHpqHe64PEYCieheNKjYWZ5xxRg6Ftf1eAepK8AKgQUUFIzocxlC7DVGEutqGHq5uvfgvuMg5futbhMuTTz45XXXVVasdYlqEaH4SAXZVpzZxjb24gHNdxDX54sLOMU8SYF0JXgDARiFOaaLKt6pTmxiSGO3/AdY3wQsAAKBgaz42AgAAgDUieAEAABRMO/k19Nxzz+Wx4y6iCAAATduyZctyd9w999xztY8VvNZQhC7T4gAAgJVrkAsErzVUqnTF9VoAAICm68U1uE6iOV4AAAAFE7wAAAAKJngBAAAUTPACAAAomOAFAABQMMELAACgYIIXAABAwQQvAACAggleAAAABRO8AAAACiZ4AQAAFEzwAgAAKJjgBQAAUDDBCwAAoGCCFwAAQMEELwAAgIIJXgAAAAUTvAAAAAomeAEAABRM8AIAACiY4AUAAFAwwQsAAKBgghcAAEBTCl5PP/10GjRoUOrVq1fq169fmjRp0mrXWbFiRbr11lvTMccck/bZZ5/Us2fPNHLkyCqPeeqpp9IJJ5yQ7/+P//iP9L3vfS/NmjWrwGcCAADw/7VMjUQEoeHDh6cJEyakAw44IM2YMSOddNJJqW3btumII44ou87KlStzyFqyZEm64IILUo8ePdJHH32U3n777YrHPPPMM+m73/1uxXbff//9NHHixDR48OD00EMPpVatWq3HZwkAADRFjabidfPNN6djjz02h6PQvXv3NGbMmHT99dfXus7999+fZs+enS677LIcukKLFi3S1ltvXfGY++67Lx111FHpwAMPTM2bN0/t27dP55xzTpo/f3569dVX18MzAwAAmrpGU/F67LHH0oUXXlhl2b777ptOPfXUNHfu3NS5c+ca69x+++1pyJAhqWXL2p/GJz7xifTcc89VWfbGG2/kIYqf/OQn13p/o7IGAACwwQSvCDFRuYoqV2UxDHDbbbdN06dPrxG8YpjhtGnT0mmnnZZvU6dOTR06dMjVraFDh1YMITzuuOPSXXfdlS666KI85LA0pDGGKG6++eZrtb8R2hYuXLgOzxgAANjQRS6IUXUbTPCKYX8hhgFWF8sWLFhQY/m8efPS4sWL0/jx49OwYcPyHK8333wzjRo1Kr311ltp7Nix+XERxm677bZ0+umn54Ydsa3zzjuv1nljdVEasggAADRdzesYuhpN8Fq+fHmuYMWtWbNmVe6LZeUsXbo0f41uhgcddFD+d9euXXMQGzBgQA5aEbo++OCD3Ezj3XffTSNGjEh/+9vf0sUXX5yHJx522GFrvc8xlwwAAGCDCV6l6lEM34uwVFm5ZaF169b5a58+faos79atW97ezJkz0x577JHOP//8XBm78847K+aCHX300bmr4VZbbZX23HPPAp8ZAABAI+lq2KZNmzyHK8JSZcuWLUtz5sxJXbp0qbFOp06d8nqlylf1sZbt2rXL/37kkUfSt771rSoNOHbZZZc81DDuAwAAaBLBq9TBcPLkyVWWPfHEEzmQbbfddmXX6d27d3r00UerLHvppZfy1+233z5/3WSTTfIww+reeecd1/ACAACaVvCKtvCTJk1KU6ZMyd/HBZTHjRuXL6Jc6nwYwwNjeUl0L7zmmmvSU089lb+P63KNHj06dy0shapvfvOb+XpgTz75ZN5GDDu84YYb0uOPP547IAIAADSJOV5hxx13TJdcckluhBGt3jt27JhOPPHENHDgwIoGHBG6Fi1aVLFOr1690rnnnptv0dEwhh8ef/zxOaCVRPD6+Mc/nrf9+uuv50AWc79uueWWPB8MAACgaM1W1tY2kLJefPHF/HX33Xdv6F0BAAA2kGzQaIYaAgAAbKwELwAAgIIJXgAAAAUTvAAAAAomeAEAABRM8AIAACiY4AUAAFAwwQsAAKBgghcAAEDBBC8AAICCCV4AAAAFE7wAAAAKJngBAAAUTPACAAAomOAFAABQMMELAACgYIIXAABAwQQvAACAggleAAAABRO8AAAACiZ4AQAAFEzwAgAAKJjgBQAAUDDBCwAAoGCCFwAAQMEELwAAgIIJXgAAAAUTvAAAAAomeAEAABRM8AIAACiY4AUAAFAwwQsAAKBgghcAAEDBBC8AAICCCV4AAAAFE7wAAAAKJngBAAAUTPACAAAomOAFAABQMMELAACgYIIXAABAwQQvAACAggleAAAABRO8AAAACiZ4AQAAFEzwAgAAKJjgBQAAUDDBCwAAoGCCFwAAQMEELwAAgIIJXgAAAAUTvAAAAAomeAEAABRM8AIAACiY4AUAAFAwwQsAAKBgghcAAEDBBC8AAICCCV4AAAAFE7wAAAAKJngBAAAUTPACAAAomOAFAABQMMELAACgYIIXAABAwQQvAACAphS8nn766TRo0KDUq1ev1K9fvzRp0qTVrrNixYp06623pmOOOSbts88+qWfPnmnkyJE1HvfHP/4xDRkyJH3uc59Le+21V+rfv39avHhxQc8EAADg/2uZGolZs2al4cOHpwkTJqQDDjggzZgxI5100kmpbdu26Ygjjii7zsqVK3PIWrJkSbrgggtSjx490kcffZTefvvtKo+75ZZb0s0335zOOuus1Lt379SiRYv0xhtvpNatW6+nZwcAADRljSZ4RTA69thjc+gK3bt3T2PGjEmXXnpprcHr/vvvT7Nnz86VsZYt/++pRKjaeuutKx4TASu28Zvf/CZttdVWFcu32Wabwp8TAABAoxpq+Nhjj6W+fftWWbbvvvvmytfcuXPLrnP77bfn4YOl0FXOr3/963T44YdXCV0AAABNruIVwwOjchVVrspatWqVtt122zR9+vTUuXPnGsMMp02blk477bR8mzp1aurQoUM66qij0tChQ/O64dlnn02HHnpouvDCC3PVq1mzZjnQnX766WmLLbZYp30GAADYYILX/Pnz89f27dvXuC+WLViwoMbyefPm5eYY48ePT8OGDctzvN588800atSo9NZbb6WxY8fmx8W/r7322jyMMYJXBKaf/vSnafDgwenuu+9eZbVsVQ09Fi5cuFbPFQAA2DhELmjevPmGE7yWL1+eK1hxi4pUZbGsnKVLl+av0c3woIMOyv/u2rVrDmIDBgzIFa2ogEXjjT333DN985vfrFj3Rz/6UTrkkEPS448/XrHumogXt1xIBAAAmo7mdQxdjSZ4lUJMVJEiLFVWblkodSTs06dPleXdunXL25s5c2baY4890sc+9rHcZr6yGIa4++67p9dee22tglepiQcAAMAG01yjTZs2eQ5XhKXKli1blubMmZO6dOlSY51OnTrl9UqVr+olv3bt2lV0Lyz3mBhyqGoFAAA0meAVouHF5MmTqyx74oknciDbbrvtyq4T1+R69NFHqyx76aWX8tftt98+f42LMUfHxMpiblg05thtt93q+VkAAAA04uAVbeHjelxTpkzJ30cb+XHjxuWLKJcqVNEQI5aXRPfCa665Jj311FP5+1dffTWNHj06X4i51NVw4MCB6eWXX0433HBD3sY777yTG3DEMMQYbggAAFC0Zitr617RAJ588sk0ceLENGvWrNSxY8d04okn5luI4YL9+vVLl112WfrMZz5Tsc4DDzyQfv7zn+eOhjH88Pjjj6/SSCP89a9/Teedd1565ZVXciCLxhpnnHFGxXDENfHiiy/mr0IbAAA0bS+uQTZoVMFrQyB4AQAAa5oNGs1QQwAAgI2V4AUAAFAwwQsAAKBgghcAAEDBBC8AAICCCV4AAAAFE7wAAAAKJngBAAAUTPACAAAomOAFAABQMMELAACgYIIXAABAwQQvAACAggleAAAABRO8GoEVK1Y29C7QRHivAQA0jJYN9HOppHnzZuny255Ib8xd0NC7wkZsm84d0/DjPt/QuwEA0CQJXo1EhK5/vDGvoXcDAAAogKGGAAAABRO8AAAACiZ4AQAAFEzwAgAAKJjgBQAAUDDBCwAAoGCCFwAAQMEELwAAgIIJXgAAAAUTvAAAAAomeAEAABRM8AIAACiY4AUAAFAwwQsAAKBgghcAAEDBBC8AAICCCV4AAAAFE7wAAAAKJngBAAAUTPACAAAomOAFAABQMMELAACgYIIXAABAwQQvAACAggleAAAABRO8AAAACiZ4AQAAFEzwAgAAKJjgBQAAUDDBCwAAoGCCFwAAQMEELwAAgIIJXgAAAAUTvAAAAAomeAEAABRM8AIAACiY4AUAAFAwwQsAAKBgghcAAEDBBC8AAICCCV4AAAAFE7wAAAAKJngBAAAUTPACAAAomOAFAABQMMELAACgYIIXAABAwQQvAACAggleAAAABRO8AAAAmkrwevrpp9OgQYNSr169Ur9+/dKkSZNWu86KFSvSrbfemo455pi0zz77pJ49e6aRI0fW+vif/OQnaaeddkrPP/98Pe89AABA7VqmRmDWrFlp+PDhacKECemAAw5IM2bMSCeddFJq27ZtOuKII8qus3LlyhyylixZki644ILUo0eP9NFHH6W333677OOfe+65fPvEJz6Rli1bVvAzAgAAaGQVr5tvvjkde+yxOXSF7t27pzFjxqTrr7++1nXuv//+NHv27HTZZZfl0BVatGiRtt566xqP/fDDD9PZZ5+dzj333PwYAACAJhe8HnvssdS3b98qy/bdd99c+Zo7d27ZdW6//fY0ZMiQ1LLl6ot2l19+efrCF76Qdt5553rbZwAAgA1mqGEMD4zKVVS5KmvVqlXadttt0/Tp01Pnzp1rDDOcNm1aOu200/Jt6tSpqUOHDumoo45KQ4cOzeuWvPLKK2ny5Mnprrvuqvf9ri+qcKxP9fneBQBgAwle8+fPz1/bt29f475YtmDBghrL582blxYvXpzGjx+fhg0blud4vfnmm2nUqFHprbfeSmPHjs2PW758eTrzzDPzMMNNN9203vY5mnosXLiw3kJXuecORfnggw+ELwCAesoFzZs33zCCV4SjqGDFrVmzZlXui2XlLF26NH+NboYHHXRQ/nfXrl1zEBswYEA6/fTTcwXsuuuuS7vuumvq3bt3ve5zvLjCEhuqNm3aNPQuAABsFOoauhpF8CoFmKggRViqrNyy0Lp16/y1T58+VZZ369Ytb2/mzJmpU6dO6X/+53/S3XffXch+Gx7Ihsp7FwBg/WvZGD59jzlcEZb22GOPiuXR8n3OnDmpS5cuNdaJUBXrlSpf1ct97dq1y3PD3nnnnYqKWMmiRYvSt771rbTlllumRx55pKBnBQAA0IiCV6mDYTTAqBy8nnjiiRzItttuu7LrxPDBRx99tKKVfHjppZfy1+233z7tsMMOuQFHdRHE4nphe++9dyHPBQAAoFG2k4+28JMmTUpTpkzJ30cb+XHjxuWLKIdoBDB48OC8vCS6F15zzTXpqaeeyt+/+uqrafTo0flCzJW7GgIAADS0RlHx2nHHHdMll1ySJk6cmEaOHJk6duyYTjzxxDRw4MCKBhwRumKYYEmvXr3yBZHjFh0NY/jh8ccfnwPaqmyyySb5BgAA0KSCV2m4YW2NMKIV/OOPP15jef/+/fNtTTz44INrvY8AAAAb7FBDAACAjZngBQAAUDDBCwAAoLHO8YqGF/fdd196/vnn09y5c3MXwmiKUbovLtLarFmz+txXAACAplPxmj17djrssMPS9ddfnwPWH//4x/T+++9X3H/11VenUaNG1ed+AgAANK3gdf7556f9998/3XvvvWnMmDE12rMffPDBFdfXAgAAaOrWaqjh1KlT069//eta799yyy3TvHnz1mW/AAAAmnbFK66rtXDhwlrvnz59esV8LwAAgKZurYJXzO+aMGFCWrp0aY373nnnnXTuueemAw88sD72DwAAoGkONfzBD36QvvOd76S+ffvm24cffpgbbcyfPz/97ne/S9tuu20aOXJk/e8tAABAUwlebdu2TTfeeGMOWVOmTEl77713mjlzZtpqq63SOeeck/r371+j4QYAAEBTtVbB6w9/+EMOW6WKFwAAAPU8x+uUU06pct0uAAAA6jl47bTTTumVV15Zm1UBAACanLUKXhdffHH61a9+lRtqzJgxIy1ZsqT+9wwAAKApz/E65phj0vLly3NzjYkTJ9a4f+XKlally5bppZdeqo99BAAAaHrB65577snBa5UbbrlWmwYAANjorFU6irbxAAAA1M06laVmz56dJk+enGbNmpWaNWuWunbtmgYMGJC22GKL+ttDAACApthco9Rg47DDDkt33XVXmj9/fpo3b16644470sEHH5yuuOKK+t1LAACAplbx+uUvf5nnecXXnj17Vrnvz3/+cxoxYkTacsst06BBg+prPwEAAJpWxStayZ999tk1Qlfo1atXvu+mm26qj/0DAABomsEr5nbtueeetd6/zz77pNdff31d9gsAAKBpB6/NN988N9SoTdy32Wabrct+AQAANO3gdcghh6SzzjorLVq0qMZ9sWz8+PG5uyEAAABr2Vzj1FNPTUOHDk19+/ZNX/ziF1OPHj3y8tdeey3de++9accdd0ynnHJKfe8rAABA0wle7dq1S7fccku6884783W8/vjHP+breG2//fbp3HPPTYceemj97ykAAEBTu4ByixYtcrt4LeMBAAAKmON1zjnnpL/85S+13v/yyy+nUaNGrc2mAQAANjprFbx++9vfpk6dOtV6f9z38MMPr8t+AQAANO3gtXjx4tS6deta7990003TsmXL1mW/AAAAmnbwiiYaL774Yq33P//886lz587rsl8AAABNO3gNHDgwXXjhhWnevHk17pszZ046++yz05FHHlkf+wcAANA0uxoOHjw4V7X69euXjj766Hwdr48++ig31YjrePXs2TMNHz68/vcWAACgqQSv5s2bp0svvTQ98MADudFGXMcrdO/ePY0bNy4dfvjh+bpeAAAArMN1vEL//v3zDQAAgHqe41XO/Pnz0wsvvFB23hcAAEBTVufgdffdd9d60eTbb789HXDAAenYY49N++23X7r66qvrcx8BAACaRvC68sory16ba/LkyXle18UXX5wrXtdee2269dZb89wvAAAA1iB4vfnmm6lr165Vli1atCiNHTs2nXHGGalv376pVatWqU+fPmn06NHpxhtvLGJ/AQAANt7gtdlmm6UFCxZUWXb55ZfnCyUfd9xxVZbvueee6e9//3v97SUAAEBTCF5xba7f/OY3Fd//+c9/TjfddFOueFW3dOnS+ttDAACAptJOfsSIEemrX/1q+sc//pGrXL/61a/Saaedlnbbbbcaj33mmWdSly5d6ntfAQAANu6KV48ePdJtt92WWrZsmYcRjhkzJg0dOrTsY+Mxtd0HAADQ1KzRBZR32GGHNH78+NU+7sgjj1yXfQIAANio1NsFlAEAAChP8AIAACiY4AUAAFAwwQsAAKBgghcAAEDBBC8AAICCCV4AAAAFE7wAAAAaywWUzzvvvLR8+fK6b7hlyzRmzJi13S8AAICmF7wefvjh1K5du7THHnvU6fFrEtIAAAA2ZnUOXldddVUaOnRo+vKXv5z22muvYvcKAABgI1LnOV677LJLGj9+fPrud7+b3nvvvWL3CgAAoKk219h///3THXfckTp06FDcHgEAADT1roaf/OQni9kTAACAjZR28kCDW7FiZUPvAk1EY36vNeZ9Y+PivQaNvLnGkCFD0pVXXpk22WSTKstvuOGG9JWvfCW1bt26iP0DmoDmzZuly297Ir0xd0FD7wobsW06d0zDj/t8aqwcB6wPjf04gI1ZnYPXk08+mT788MMaweuKK65IhxxySNp6662L2D+giYiTzX+8Ma+hdwMalOMAYONV56GGK1euXKPlAAAA/B9zvAAAABpL8GrWrFmty2u7DwAAgDWY4xVDCk855ZTUokWLKss/+OCDdMYZZ9RorhFzwS6//PL621MAAICNPXiddtppafny5TWW77HHHmUfX70JBwAArK7VfXT4hI3xvVbn4HXyyScXuycppaeffjpNmDAhzZw5M2222WZp6NCh6dhjj13lOitWrEi/+tWv0p133pnmzJmTw+EBBxyQLr744nz/3//+93Tdddflrozvv/9+2m677dKIESPSQQcdVPjzAQCg7lxWgY35sgp1Dl5FmzVrVho+fHgOXhGcZsyYkU466aTUtm3bdMQRR9Q6/HHkyJFpyZIl6YILLkg9evRIH330UXr77bcrHvPGG2+kXr16pVGjRqUOHTqkRx99NK9z2223pU9/+tPr8RkCALA6LqvAxqrRBK+bb745V7cidIXu3bunMWPGpEsvvbTW4HX//fen2bNnp0mTJqWWLf/vqcQctMrXFNt///2rrNO3b9906KGHpsmTJwteAABA02on/9hjj+VQVNm+++6bK19z584tu87tt9+ehgwZUhG66qp9+/Zp0aJF67S/AAAAG1TFK4YHRuUqqlyVtWrVKm277bZp+vTpqXPnzjWGGU6bNi03/Yjb1KlT81DCo446Ks8Ni3Vr+1kR8n784x+v8z7Xl+qdIqFI9fnerS+OAZr6MRAcB6xPjgNI6/04aBTBa/78+RWVqOpi2YIFNSdYzps3Ly1evDiNHz8+DRs2LM/xevPNN/NcrrfeeiuNHTu27M+KuV0f//jHczVtbUVDj4ULF6b6+gNT7nlDUeISEI3pP1zHAE39GAiOA9Y3xwGkejkOIhc0b958wwle0YkwKlhxq34x5lhWztKlS/PXY445pqJDYdeuXXMQGzBgQDr99NNzBayy6HB42WWXpRtvvHGd9jdeXH8Y2FC1adOmoXcBGpRjABwHUF/HQV1DV6MJXqUQE1Wk6mGp3LJQumBznz59qizv1q1b3l60pK98jbH33nsvfec730mjR49OO+200zrvs1I4GyrvXZo6xwA4DqAhjoPmjSVtxhyuCEuVLVu2LF+bq0uXLjXW6dSpU16vVPmqXvJr165dle1897vfzc07Yg4YAABAkwteIeZcRYv3yp544okcyOKix+X07t07X5erspdeeil/3X777SuWnXnmmbkK9v3vf7+QfQcAANgggle0hY/rcU2ZMiV/H23kx40bly+iHGLi2+DBg/PykuheeM0116Snnnoqf//qq6/moYRxIeZSV8O4DljM7Zo4ceIajcEEAACoL41ijlfYcccd0yWXXJID0siRI1PHjh3TiSeemAYOHFjRgCNCV+Xrb/Xq1Sude+65+RYdDWP44fHHH58DWsktt9yShyPut99+VX5eVMTuuuuu9fgMAQCApqrRBK/ScMO777677H2bbrppevzxx2ss79+/f77V5k9/+lO97iMAAMCaMvYOAACgYIIXAABAwQQvAACAggleAAAABRO8AAAACiZ4AQAAFEzwAgAAKJjgBQAAUDDBCwAAoGCCFwAAQMEELwAAgIIJXgAAAAUTvAAAAAomeAEAABRM8AIAACiY4AUAAFAwwQsAAKBgghcAAEDBBC8AAICCCV4AAAAFE7wAAAAKJngBAAAUTPACAAAomOAFAABQMMELAACgYIIXAABAwQQvAACAggleAAAABRO8AAAACiZ4AQAAFEzwAgAAKJjgBQAAUDDBCwAAoGCCFwAAQMEELwAAgIIJXgAAAAUTvAAAAAomeAEAABRM8AIAACiY4AUAAFAwwQsAAKBgghcAAEDBBC8AAICCCV4AAAAFE7wAAAAKJngBAAAUTPACAAAomOAFAABQMMELAACgYIIXAABAwQQvAACAggleAAAABRO8AAAACiZ4AQAAFEzwAgAAKJjgBQAAUDDBCwAAoGCCFwAAQMEELwAAgIIJXgAAAAUTvAAAAAomeAEAABRM8AIAACiY4AUAAFAwwQsAAKBgghcAAEDBBC8AAICmFLyefvrpNGjQoNSrV6/Ur1+/NGnSpNWus2LFinTrrbemY445Ju2zzz6pZ8+eaeTIkVUe89prr6UTTzwx9e7dOx1wwAHpiiuuSCtXrizwmQAAAPx/LVMjMWvWrDR8+PA0YcKEHI5mzJiRTjrppNS2bdt0xBFHlF0nwlOErCVLlqQLLrgg9ejRI3300Ufp7bffrnjMggUL0uDBg/PjbrjhhjR37tw0YsSI1KJFi7x9AACAJlPxuvnmm9Oxxx6bQ1fo3r17GjNmTLr++utrXef+++9Ps2fPTpdddlkOXSEC1dZbb13xmHvuuSdXwo4++ujUrFmztNVWW6Vx48blEBbVMgAAgCYTvB577LHUt2/fKsv23XffXPmKKlU5t99+exoyZEhq2bLlGm33U5/6VGrfvn164YUX6mnvAQAAGvlQwxgeGJWrqHJV1qpVq7Ttttum6dOnp86dO9cYZjht2rR02mmn5dvUqVNThw4d0lFHHZWGDh2a1w2vv/56je2Gbt26pVdffTV99rOfXet9ri9RpYP1pT7fu/XFMUBTPwaC44D1yXEAab0fB40ieM2fPz9/jSpUdbEs5mlVN2/evLR48eI0fvz4NGzYsDzH680330yjRo1Kb731Vho7dmx+3Lvvvlvrdks/d03FEMWFCxem+voDU27/oCgffPBBo/oP1zFAUz8GguOA9c1xAKlejoPIBc2bN99wgtfy5ctzBStuMQ+rstq6Dy5dujR/jW6GBx10UP53165dcxAbMGBAOv3003MFrLTt6sr9rLqKF9cfBjZUbdq0aehdgAblGADHAdTXcVDX0NVoglcpxEQVKcJSZeWWhdatW+evffr0qTGEMLY3c+bMtMcee+R/l6tO1bbdulIKZ0PlvUtT5xgAxwE0xHHQvLGkzZjDFWGpsmXLlqU5c+akLl261FinU6dOeb1S5at6ya9du3YVVbDq2w2xrNx2AQAANsrgVepgOHny5CrLnnjiiRzItttuu7LrxAWRH3300SrLXnrppfx1++23r3W70azj3//+91o31gAAANggg1e0hZ80aVKaMmVK/j7ayMf1tkoXOY6Jb3Eh5FheEt0Lr7nmmvTUU0/l76NL4ejRo/OFmEtdDb/61a+mJ598Mt199915XldcXPnMM8/M2yoNVwQAAChSo5jjFXbcccd0ySWXpIkTJ6aRI0emjh07phNPPDENHDgw3x9NMiJ0LVq0qGKdXr16pXPPPTffoqNhDD88/vjjc6gq2XLLLdN1112Xzj///HyLsBUNOUaMGNEgzxMAAGh6Gk3wKg0LjMpUOZtuuml6/PHHayzv379/vq3Krrvumm699dZ6208AAIANcqghAADAxkrwAgAAKJjgBQAAUDDBCwAAoGCCFwAAQMEELwAAgIIJXgAAAAUTvAAAAAomeAEAABRM8AIAACiY4AUAAFAwwQsAAKBgghcAAEDBBC8AAICCCV4AAAAFE7wAAAAKJngBAAAUTPACAAAomOAFAABQMMELAACgYIIXAABAwQQvAACAggleAAAABRO8AAAACiZ4AQAAFEzwAgAAKJjgBQAAUDDBCwAAoGCCFwAAQMEELwAAgIIJXgAAAAUTvAAAAAomeAEAABRM8AIAACiY4AUAAFAwwQsAAKBgghcAAEDBBC8AAICCCV4AAAAFE7wAAAAKJngBAAAUTPACAAAomOAFAABQMMELAACgYIIXAABAwQQvAACAggleAAAABRO8AAAACiZ4AQAAFEzwAgAAKJjgBQAAUDDBCwAAoGCCFwAAQMEELwAAgIIJXgAAAAUTvAAAAAomeAEAABRM8AIAACiY4AUAAFAwwQsAAKBgghcAAEDBBC8AAICCCV4AAAAFE7wAAAAKJngBAAAUTPACAAAomOAFAADQVILX008/nQYNGpR69eqV+vXrlyZNmrTKx997771p1113TXvvvXeV21VXXVXlcS+//HI6+eST0+c+97l8GzZsWF4GAACwvrRMjcCsWbPS8OHD04QJE9IBBxyQZsyYkU466aTUtm3bdMQRR5RdZ/ny5Tmk3XDDDavc7oknnphGjRqVfv7zn6ePPvooXXvttemEE05IDz/8cNp8880LfFYAAACNqOJ18803p2OPPTaHrtC9e/c0ZsyYdP3116/TdiNcRTgbOHBgatWqVWrdunUaMWJE2mqrrXKFDQAAoMlUvB577LF04YUXVlm27777plNPPTXNnTs3de7cea22+4lPfCL985//TCtWrEjNm/9fxnzvvffSv//979SlS5d12ueontWXFi1a1Nu2YH2+d+uLY4CmfgwExwHrk+MA0no/Dlo2hic8e/bsXOWqLCpU2267bZo+ffpaB69DDz00V9N++MMfph/96Edp4cKFOcx96UtfSjvttNNa73MEudhWff2Bad++fb1sC+rigw8+aFT/4ToGaOrHQHAcsL45DiDVy3FQucDT6IPX/Pnz89dyB1osW7BgQdn1mjVrll555ZUcrt59991c3friF7+YBg8enENbiK8xXPGss85KBx98cFq6dGn6zne+k4YOHbpO+xwvrj8MbKjatGnT0LsADcoxAI4DqK/joK6hq1EEr2iSsXLlynyLMFVZLKvNgQcemPbcc8+0/fbb52385S9/Seecc07617/+lStcpW1fdtll6cUXX8yBKwJaVMBirtfxxx+/TvutFM6GynuXps4xAI4DaIjjoMGDV6lyFEP3OnToUOW+cstKNttss3wLm2yySQ5h559/fg5Uo0ePzunz6quvTtOmTUv33HNP+tjHPpYfG402vva1r+WOhocffnjhzw8AAKB5YyjxxRyumTNnVlm+bNmyNGfOnDVqgtGtW7e0ePHiiuGLjzzySB56WApdYZtttklf//rX00MPPVSPzwIAAKARB69SB8PJkydXWfbEE0/kQLbddtvVeTsvvPBC2mKLLVKnTp3y95tuumkeXljdO++8UzEPDAAAoEkEryFDhqRJkyalKVOm5O/jAsrjxo3LF1EO0W0kKlexvHLIim6I4cMPP0y///3v84WS40LMpblicfHkiy66KD3wwAO5ghaPu/fee9Ott96aBg0a1CDPFQAAaHoafI5X2HHHHdMll1ySJk6cmEaOHJk6duyYQ1PMxyo1yYjQtWjRoop1/vGPf6TTTz89V6+iWUYMMzz77LNT3759Kx7Tv3//1K5du3TNNdeksWPH5kD26U9/Ol111VWpd+/eDfJcAQCApqdRBK/ScMO777677H0xZPDxxx+vsixax8dtdfbbb798AwAAaNJDDQEAADZmghcAAEDBBC8AAICCCV4AAAAFE7wAAAAKJngBAAAUTPACAAAomOAFAABQMMELAACgYIIXAABAwQQvAACAggleAAAABRO8AAAACiZ4AQAAFEzwAgAAKJjgBQAAUDDBCwAAoGCCFwAAQMEELwAAgIIJXgAAAAUTvAAAAAomeAEAABRM8AIAACiY4AUAAFCwZitXrlxZ9A/ZmDz77LMpXrJNNtmkXrc7770P0rLlK+p1m1BZq5bNU6cObVJj5RigqR8DwXFA0RwHUL/HwYcffpiaNWuW9tprr9U+tmW9/MQmJF7YIjT2P4JQNMcAOA4gOA7Y0LJBXfOBihcAAEDBzPECAAAomOAFAABQMMELAACgYIIXAABAwQQvAACAggleAAAABRO8AAAACiZ4AQAAFEzwAgAAKJjgBQAAUDDBCwAAoGCCFwAAQMEEL9a7s846K91333213v/Xv/41ffnLX07Lli1br/sFAABFaVnYlqEWH374Yb7VZuedd0633377et0nAAAokooXAAA0Ec8880z6xje+UWN5LIv7KI7gBQAATcTy5cvzra7LqT+CFw1iyZIl6bzzzkv/8R//kfbcc880bNiwNGfOnHzfCy+8kA499NAqj7/ttttSv3790mc/+9l05JFHpilTpuTHvP322/n+adOmpa985St5+UEHHZT69+9fMZ/s2muvrbKtWCd+JjRWK1euTJdddlnaf//983t10KBB6fnnn6943/72t79Np59+errrrrvyMTRkyJC8PL4+8MADVbYVx8aAAQMa5HnQdD311FPpq1/9atp7773z7ZRTTkl33nln+uEPf5guvvji9PnPfz716tUrnXDCCWnmzJkV682ePTsdcsgh+T0eX/faa6+Kv/n1eQwFxxEb49/wpUuXpv/6r//KPzuOvTh3mjRpUsX9AwcOTCeffHKubMX9cS728MMP53/Hsrgvjs04V/roo4/Sbrvtlv73f/83H4dxPMZxGfvN2jHHiwbxs5/9LB1//PHp97//fVq8eHH6wQ9+kA/2e++9N8//qtxY46GHHkqXXHJJ+ulPf5r22Wef9Le//S2deuqpae7cuRWPiz8077zzTv6PPbbRpk2bWueTxTrxeGis/vu//zv/x3bFFVekXXbZJU2dOjW/5+MDixDv39deey1tsskm6dFHH00tW7as9f0ej13VnEqob3GS9r3vfS9/8BUfkDVv3jzNmzcv/72Pk8pYFieebdu2zf8XjBw5Mr/fmzVrlm9vvfVWuvHGG9MvfvGL1LVr17xevP/j5LBz5871cgwFxxEb49/weGx8sHHmmWemdu3a5Q82vv71r6fPfOYz6dOf/nS644478v5EMLzpppsq1otAFY8bMWJEPteqfM70k5/8JI0bNy6HsPhwfPDgwXlbcWPNqHjRILp06ZIP7vhj0759+zRx4sQ0a9as9Oyzz9Z4bFS7vv3tb6d99903tWjRIv8RGzVqVJX/QEOsH+Et/tDEf/SwoYr3fFQG4j+5eC9/7nOfy//RrVixouIx06dPzx9YxH/c3u80JvEJerx/Dz/88Pw3O8LU5ptvnu+L9+vYsWPTZpttllq1apWGDx+eXn311fTmm29WOdEcM2ZM6tatW173wAMPzCd4Tz75ZL0eQ8FxxMb2NzzOqY444oh8LhTiOOrTp886zd2Kc7Ddd989H4977LFH6tu3b/rd735Xb/vclPgrQ4OIEnj1PxRRgo9qVnWvvPJKLntX9oUvfKHGH6qockUogw3Z+++/nz9EiGEflcV/dJXtsMMOFSez0FjEp+v/+Mc/cugqp3v37mnTTTet+L5169b5fVwaNh7i5K76J+lbb711lcfUxzEUHEdsjH/Do4oWUzhi6kUMD3zwwQfT/Pnz13p71c+tttlmmzzqiDUneNEgyv2x2WKLLdIHH3xQY/nChQtTx44dqyyLSln1bXTq1KnOY6+hsYr3e6j+nq8+xCoqBnXh/c769O677+a/zaWhU9VVDl0lUfmKuSQl8aFaLFvVY+rjGAqOIza2v+ExjDdGBcUH1DGU8E9/+lOen7Uu7+Hqx20c33U9HqlK8KJBxBj+6mKO1pZbblljecwDiP/MK4uuOwsWLKiyLIa0VBf/gVfv0POvf/1rHfYcilWan1j9PR/HR2Xe7zRGEbrivdqQ86HqegwFxxEb29/wW265JTfuiOY2UZmKkFTunIuGIXjRIKLsXXms86JFi3Lnnp122qnGY3v27Jn+8Ic/VFkWHa5iwufqxkVHkHvjjTeqLFuTeQKwvnXo0CHtuOOONd7zkydPzkOwVsX7nYYWzTBiDm9Ddj1bl2MoOI7YkP+GRyObCFyVK3DRUbGyCGPlKmC1Laf+CF40iChRxwTr+FQ0QtcZZ5yRevfuXXaO1mmnnZZ++ctf5rAV6z333HPpoosuyp8WVS/lV7fffvvlTlgxTyxEJx//gdLYxaeV8R6PDyPiPR/v/RtuuGG1Q1Pi/R6dPUuXZrj//vvLzpuEosSJZQxzioZJMeSp9AFbnAxuCMdQcByxIf8Nj+YXsY1oUhNVt2gtXzmIhY9//ONpxowZ6b333qsYGlkKfrHPcdzGuRn1T/BivYsOPtF9ML5Gt6q4RYD6+c9/XnF/5fH9O++8c7r88svzH7GofkWL4mh52qNHjzwMsTT+uNzcgWjKES1co3NWNPSI1q7RcavcY6GxiLH58T6PDyRigvaVV16Zh42UrgFT2/s9xvHH7bjjjsvbiBPf2E4cU7C+xHsv/tZed911+f0bf7cjjMV7ttx7MZaVlsff/nLv7cqPqY9jKDiO2Bj/hkfQiqpVrP+lL30pH3+xzcpzsrbffvvceCMafsQ1xkrDG+PcKtrNRxfE+Nmlfa0+Z7O2Y5nVa7ZSTZFG7p///Ge+xR+sGFoY176IVsPf+MY30mGHHdbQuwf1LtoMRzU4OrvFn+gXX3wxv+fjQ4O4iDiwao4hGpL3H7VxAWU2iNasEyZMyGXxKH/HJzVRMRO62FjF8JC4YOXs2bPz95/61Kfyf9r+w6api2MjLsBc22fGMdTxj3/8o2OIBlXU+y9G8DzxxBOrrHYdc8wx6/QzKJaKFwAAQMHM8QIAACiY4AUAAFAwwQsAAKBgghcAAEDBBC8AmoRf//rXaciQIat93E033ZSOOuqo9bJPADQdghcAjU4EpM985jPp7bffrrdtxgVE49o6q7PFFlukLl26VFn22GOPpaeffjoVIbYb2wdg4yZ4AdCozJo1Kz3//PNpl112yVWq9e3www9Pl156aZVlDz30UPrf//3fQn5ebDe2D8DGTfACoFG588470xFHHJG++tWvprvvvruhdwcA6oXgBUCjEcMB77rrrvSVr3wlHXrooenf//53rn5VFlWwU045JT355JPpoIMOSnvvvXdasGBBvm/x4sXppz/9aV6+6667pt69e6fLL7+8Yt2VK1emq666KvXt2zfttttu6Qtf+EK65ZZbamw/gl/4zW9+k3baaaccAC+77LL87x/96EcVj3399dfTsGHD0mc/+9nUp0+fNHbs2PTBBx/UeE7XX3996t+/f/6ZPXv2TGeddVZ69tln8/Ziu7H9+Pc3v/nNvM60adPSYYcdVuP1efDBB1O/fv2qDFM8+uij0yuvvJKOPPLItOeee6ZXX3013xev3emnn5722muv/BrFv9999911+v0AsPZarsO6AFCvpkyZkj7xiU+knXfeOX8fASiC2B577FElyESAuPLKK9MVV1yROnbsmNq3b5/nb0VwiXA1bty49KlPfSotXLiwyryuZ555Ji1fvjz95Cc/yfO4pk6dmsaMGZMfGyGttP14TGnY4QEHHJAD1Sc/+ckcslq3bp3ve+edd9LXvva1HJBGjx6dQ9/555+fg9lFF11U8TNHjhyZXnvttXTGGWek3XffPS1ZsiTNmzcvh7A///nP6eqrr05vvvlmOvvss9Mmm2yS11m6dGnZ+Wixb3Gr/H08Lp5P7OM222yTOnXqlNc/8cQT8/O69dZbU4sWLdLPfvazNHz48HTbbbcV8JsDYHUELwAajTvuuCMNGjSo4vuBAwemwYMHpzPPPLMilIQILLfffntFQAu//OUvczOOBx54IG266aYVjTIqi9AUYW3zzTfP33/xi1/M24pKUil4VRaBpUOHDqlVq1Z5m/HvkghMMQ8tglvJJZdckg4++OA0e/bstN122+V9+cMf/pAeeeSRKvuy7bbb5q+xvdhubL/yttfE9OnTc5UvKluVX4vmzZvn5c2aNcvLLrzwwhwi//SnP5V9rgAUy1BDABqFf/3rX7kiNWDAgIplMVwwQsrkyZOrPHbLLbfMXQ8ru++++9IJJ5xQEbrK2WGHHSpCV0mEt2josaZ+//vf5+BWWefOnXMl7cUXX6zYpxgKWD0A1qcIWBGoqlcOo1pYCl0hXpcYEhnDGAFY/1S8AGgUYp5TzINq06ZNleVR9YrhhjHsrySGI1Y3Y8aMHKxW5WMf+1jZZTE0b0298cYbeVjhOeecU2V5zPGaO3duxT7tt99+qUgx1LJt27ZVls2ZMyeH2GuuuabK8hjmWKq2AbB+CV4ANJpuhhEY4mtlMWdrxYoVeRjhVlttlZdVD2cl8bj1KUJXNMuornJVrfKcrHUVwam62l6L73znO1XCauWgBsD6J3gB0OBi3lHM4br33nvL3h/NMuK+b33rW7Vuo3v37ukvf/lLjWF39aHykL2SCIGrqyDFPr388strvO2Yi/b+++/XWF7XIZGxb4sWLVLdAmhEzPECoMFFo4yY2xVDBcvdjjrqqHTPPfeschsxTPHmm2+uaC1fnyIUljodluyzzz5p0qRJuSK3qn2KwBjNNla17WXLllVZFkMp58+fn7sdVq6cxZyxuoh9i9erXIUMgIYheAHQoKLl+8MPP1z2ulUlcd2tmFP1wgsv1PqYaKwRTSziGmDRJj5azkeFKLr+rautt946N6yIoZDROCPCVlTfZs6cmb797W/na2fFdbPi2lw33XRTxXox1C+uoXXcccelRx99NLeg/+c//5krc5W3HZ0V//73v6e//vWvOSxFxSrWixbxMcQy1vv+97+funbtWqf9jYtPx7DL6AgZr1msHz/zF7/4xTq/FgCsHcELgAYVLdejqrWqUBHzmCJ8RfUoWrzHrVyTjAg9Ue2JiwV//vOfT//5n/+ZL4Jc6upXuSV9SfXlse2WLauOxP/yl7+cG1hEkPrhD3+YK1TdunXLF1+OSlQEqwMPPLDiel6VOw7G9ca+9KUvpR//+Me50UZUwW644YaKx8Tziu6NxxxzTBoxYkSudJXav0fAi0pgdCiMKljM26r83Gt7LeJaXnG9rghwERBj+GVsOwIYAA2j2cpVjZEAAABgnal4AQAAFEzwAgAAKJjgBQAAUDDBCwAAoGCCFwAAQMEELwAAgIIJXgAAAAUTvAAAAAomeAEAABRM8AIAACiY4AUAAJCK9f8Ak3JXB0WSBecAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "최고 F1 점수 아키텍처: bigru (F1: 0.6546)\n"
     ]
    }
   ],
   "source": [
    "# === 아키텍처 후보 & 결과 컨테이너 ===\n",
    "architectures_to_test = ['gru', 'bigru', 'gru_attn', 'cnn_gru']\n",
    "results_arch = {}\n",
    "arch_test_output_dir = 'outputs/step2_arch_test'\n",
    "\n",
    "\n",
    "# === Fold ID 정규화 ===\n",
    "def norm_fold_id(fid):\n",
    "    \"\"\"정수/문자 어떤 입력이 와도 'fold_X' 형식으로 통일.\"\"\"\n",
    "    if isinstance(fid, int) or (isinstance(fid, str) and fid.isdigit()):\n",
    "        return f\"fold_{int(fid)}\"\n",
    "    if isinstance(fid, str) and fid.startswith(\"fold_\"):\n",
    "        return fid\n",
    "    raise ValueError(f\"지원하지 않는 fold 형식: {fid!r}\")\n",
    "\n",
    "\n",
    "tuning_fold_id = norm_fold_id(tuning_fold_id)  # 예: 7 -> 'fold_7'\n",
    "\n",
    "\n",
    "# === 1) 모든 아키텍처의 기존 결과 확인 ===\n",
    "all_results_exist = True\n",
    "missing = []\n",
    "\n",
    "def _report_path(base_dir, variant, fold_id):\n",
    "    return os.path.join(base_dir, variant, 'models', fold_id, 'classification_report.csv')\n",
    "\n",
    "for variant in architectures_to_test:\n",
    "    rp = _report_path(arch_test_output_dir, variant, tuning_fold_id)\n",
    "    if not os.path.exists(rp):\n",
    "        all_results_exist = False\n",
    "        missing.append(rp)\n",
    "\n",
    "# === 2) 결과 불러오기 또는 새로 실행 ===\n",
    "if all_results_exist:\n",
    "    print(f\"✅ 기존 아키텍처 테스트 결과('{arch_test_output_dir}')를 찾았습니다. 결과를 재사용합니다.\")\n",
    "    for variant in architectures_to_test:\n",
    "        rp = _report_path(arch_test_output_dir, variant, tuning_fold_id)\n",
    "        report_df = pd.read_csv(rp, index_col=0)\n",
    "        results_arch[variant] = float(report_df.loc['macro avg', 'f1-score'])\n",
    "else:\n",
    "    if missing:\n",
    "        print(\"기존 아키텍처 테스트 결과가 없거나 불완전합니다. 새로 테스트를 시작합니다.\")\n",
    "        print(\"누락 파일 예시:\", missing[0])\n",
    "\n",
    "    for variant in architectures_to_test:\n",
    "        print(f\"\\n===== 아키텍처 테스트: {variant} =====\")\n",
    "        test_cfg = copy.deepcopy(config.MANUAL_CONFIG)\n",
    "        test_cfg['paths']['OUTPUT_DIR'] = os.path.join(arch_test_output_dir, variant)\n",
    "        test_cfg['model_arch']['variant'] = variant\n",
    "\n",
    "        # run_single_fold가 fold id를 정수로 받으므로 int로 변환\n",
    "        fold_int = int(tuning_fold_id.split('_')[1])\n",
    "        f1_score = run_single_fold(test_cfg, fold_id=fold_int)\n",
    "        results_arch[variant] = f1_score\n",
    "\n",
    "# === 3) 결과 시각화 ===\n",
    "if not results_arch:\n",
    "    raise RuntimeError(\"아키텍처 테스트 결과가 비어 있습니다. 상위 단계 실행을 확인하세요.\")\n",
    "\n",
    "results_df = (\n",
    "    pd.DataFrame(list(results_arch.items()), columns=['Architecture', 'F1 Score'])\n",
    "    .sort_values('F1 Score', ascending=False)\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Architecture', y='F1 Score', data=results_df, order=results_df['Architecture'])\n",
    "plt.title(f'모델 아키텍처별 성능 비교 ({tuning_fold_id})')\n",
    "plt.ylim(bottom=max(0.0, results_df['F1 Score'].min() - 0.05))\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# === 4) 최적 아키텍처 출력 ===\n",
    "best_architecture = results_df.iloc[0]['Architecture']\n",
    "best_score = results_df.iloc[0]['F1 Score']\n",
    "print(f\"\\n최고 F1 점수 아키텍처: {best_architecture} (F1: {best_score:.4f})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce74cbd",
   "metadata": {},
   "source": [
    "# # 모델 아키텍처별 추론 속도 비교\n",
    "\n",
    "⚡ 아키텍처별 추론 속도 비교 (inference throughput)\n",
    "\n",
    "동일한 입력(N_BENCH=2000 샘플)에 대해 각 아키텍처 모델의 **초당 처리 샘플 수(Samples/Sec)**를 측정하여\n",
    "실행 속도 관점에서의 우열을 비교합니다.\n",
    "\n",
    "🔧 절차\n",
    "\n",
    "데이터 로드: 학습 때 생성한 특징 파일(.npz)에서 X만 로드 → 상위 2,000개 사용\n",
    "\n",
    "모델 로드 & 워밍업: 각 아키텍처 모델을 로드한 뒤, 첫 추론은 워밍업으로 분리\n",
    "\n",
    "속도 측정: batch_size=64로 전체 test_sample에 대해 predict 수행 시간 측정\n",
    "\n",
    "정리/시각화: Samples/Sec 기준 내림차순 정렬 막대 그래프 출력 + 표 요약\n",
    "\n",
    "📂 모델 경로 규칙\n",
    "\n",
    "기본: outputs/step2_arch_test/<variant>/models/{tuning_fold_id}/best_model.keras\n",
    "\n",
    ".keras가 없으면 자동으로 .h5 경로도 시도합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64439f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "추론 속도 측정을 위해 2000개의 샘플을 사용합니다.\n",
      "\n",
      "===== 추론 속도 측정: gru =====\n",
      "'gru' 모델 로드 성공.\n",
      "처리 시간: 0.5167초, 초당 처리 샘플 수: 3870.87개\n",
      "\n",
      "===== 추론 속도 측정: bigru =====\n",
      "'bigru' 모델 로드 성공.\n",
      "처리 시간: 0.8660초, 초당 처리 샘플 수: 2309.46개\n",
      "\n",
      "===== 추론 속도 측정: gru_attn =====\n",
      "'gru_attn' 모델 로드 성공.\n",
      "처리 시간: 0.6032초, 초당 처리 샘플 수: 3315.76개\n",
      "\n",
      "===== 추론 속도 측정: cnn_gru =====\n",
      "'cnn_gru' 모델 로드 성공.\n",
      "처리 시간: 0.4999초, 초당 처리 샘플 수: 4000.44개\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[추론 속도 비교 결과 (Samples/Sec)]\n",
      "  Architecture  Samples/Sec\n",
      "3      cnn_gru  4000.440646\n",
      "0          gru  3870.871404\n",
      "2     gru_attn  3315.758872\n",
      "1        bigru  2309.457545\n"
     ]
    }
   ],
   "source": [
    "# === 1) 테스트 데이터 로드 ===\n",
    "# 학습에 사용했던 특징 데이터(.npz)에서 X만 로드\n",
    "features_path = runner.get_or_create_features(config.MANUAL_CONFIG)\n",
    "with np.load(features_path, allow_pickle=True) as data:\n",
    "    X_data = data['X']\n",
    "\n",
    "# 전체 중 앞부분 일부(예: 2,000개)로 속도 측정\n",
    "N_BENCH = 2000\n",
    "test_sample = X_data[:N_BENCH]\n",
    "print(f\"추론 속도 측정을 위해 {len(test_sample)}개의 샘플을 사용합니다.\")\n",
    "\n",
    "\n",
    "# === 2) 각 아키텍처 모델 추론 속도 측정 ===\n",
    "results_speed = {}\n",
    "\n",
    "def _resolve_model_path(variant: str, fold_id: str) -> str:\n",
    "    \"\"\"아키텍처/폴드별 모델 경로 결정 (.keras 우선, 없으면 .h5 시도).\"\"\"\n",
    "    base = os.path.join('outputs', 'step2_arch_test', variant, 'models', fold_id, 'best_model')\n",
    "    keras_path = base + '.keras'\n",
    "    h5_path = base + '.h5'\n",
    "    if os.path.exists(keras_path):\n",
    "        return keras_path\n",
    "    if os.path.exists(h5_path):\n",
    "        return h5_path\n",
    "    # 기본 경로(.keras) 반환해 로드 시 에러 메시지에 경로 노출\n",
    "    return keras_path\n",
    "\n",
    "for variant in architectures_to_test:\n",
    "    print(f\"\\n===== 추론 속도 측정: {variant} =====\")\n",
    "\n",
    "    model_path = _resolve_model_path(variant, tuning_fold_id)\n",
    "    try:\n",
    "        # 모델 로드\n",
    "        model = tf.keras.models.load_model(model_path)\n",
    "        print(f\"'{variant}' 모델 로드 성공: {model_path}\")\n",
    "\n",
    "        # 워밍업 (초기 그래프/커널 준비로 인한 첫 호출 지연 제외)\n",
    "        _ = model.predict(test_sample[:1], verbose=0)\n",
    "\n",
    "        # 실제 추론 시간 측정\n",
    "        start_time = time.time()\n",
    "        _ = model.predict(test_sample, batch_size=64, verbose=0)  # 로그 숨김\n",
    "        duration = time.time() - start_time\n",
    "\n",
    "        samples_per_second = len(test_sample) / max(duration, 1e-9)\n",
    "        results_speed[variant] = samples_per_second\n",
    "\n",
    "        print(f\"처리 시간: {duration:.4f}초, 초당 처리 샘플 수: {samples_per_second:.2f}개\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ '{variant}' 모델 처리 중 오류: {e}\")\n",
    "        results_speed[variant] = 0.0\n",
    "\n",
    "\n",
    "# === 3) 결과 시각화 ===\n",
    "if not results_speed:\n",
    "    raise RuntimeError(\"속도 측정 결과가 비어 있습니다. 상위 단계 실행을 확인하세요.\")\n",
    "\n",
    "speed_df = (\n",
    "    pd.DataFrame(list(results_speed.items()), columns=['Architecture', 'Samples/Sec'])\n",
    "    .sort_values('Samples/Sec', ascending=False)\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Architecture', y='Samples/Sec', data=speed_df, order=speed_df['Architecture'])\n",
    "plt.title('모델 아키텍처별 추론 속도 비교 (Samples/Sec)')\n",
    "plt.ylabel('Samples per Second (높을수록 빠름)')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# === 4) 결과 요약 ===\n",
    "print(\"\\n[추론 속도 비교 결과 (Samples/Sec)]\")\n",
    "print(speed_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87cb83c",
   "metadata": {},
   "source": [
    "# # 랜드마크 특징(`FEATURE_CONFIG`) 조합 탐색\n",
    "\n",
    "🧱 랜드마크 특징 조합 탐색 (단일 Fold, 고속 테스트)\n",
    "\n",
    "고정 아키텍처({best_architecture})와 기준 폴드({tuning_fold_id})에서\n",
    "얼굴 관련 특징 4개({', '.join(face_features)})의 사용/미사용 조합을 전수 탐색하여 Macro F1을 비교합니다.\n",
    "모든 특징이 False면 Hand Only로 간주합니다.\n",
    "\n",
    "🔧 절차\n",
    "\n",
    "조합 생성: itertools.product로 2⁴=16개 플래그 조합 생성\n",
    "\n",
    "캐시 확인: 각 조합별 classification_report.csv 존재 시 재사용, 없으면 재학습\n",
    "\n",
    "학습 설정(고속): epochs=30, early_stopping_patience=5, data_subset_fraction=0.3\n",
    "\n",
    "평가/정리: 각 조합의 Macro F1 수집 → 수평 막대그래프로 비교 → 최고 조합/설정 출력\n",
    "\n",
    "📂 결과 경로 규칙\n",
    "outputs/step3_1_feature_test/<Feature_Combo_Dir>/models/{tuning_fold_id}/classification_report.csv\n",
    "\n",
    "\n",
    "<Feature_Combo_Dir>: 라벨의 “, ”를 “_”로 치환 (예: face_pitch_face_inner_eyes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbcf8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아키텍처를 'cnn_gru'로 고정하여 테스트를 진행합니다.\n",
      "기존 특징 조합 테스트 결과가 없거나 불완전합니다. 새로 테스트를 시작합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== 특징 조합 테스트: face_roll_scale, face_pitch, face_mouth_corners, face_inner_eyes =====\n",
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "특징 추출(병렬): 100%|██████████| 1111/1111 [00:35<00:00, 31.49it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "청크 병합: 100%|██████████| 1111/1111 [00:15<00:00, 72.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터가 11356개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 1.46433, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch_face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 7s - 38ms/step - accuracy: 0.2738 - loss: 2.2114 - val_accuracy: 0.5357 - val_loss: 1.4643 - learning_rate: 3.0000e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 1.46433 to 0.92075, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch_face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 24ms/step - accuracy: 0.4959 - loss: 1.5486 - val_accuracy: 0.7449 - val_loss: 0.9207 - learning_rate: 3.0000e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 0.92075 to 0.65391, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch_face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.6295 - loss: 1.1584 - val_accuracy: 0.8098 - val_loss: 0.6539 - learning_rate: 3.0000e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 0.65391 to 0.51450, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch_face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 24ms/step - accuracy: 0.7159 - loss: 0.9169 - val_accuracy: 0.8502 - val_loss: 0.5145 - learning_rate: 3.0000e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 0.51450 to 0.39194, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch_face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 24ms/step - accuracy: 0.7681 - loss: 0.7725 - val_accuracy: 0.8904 - val_loss: 0.3919 - learning_rate: 3.0000e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 0.39194 to 0.32726, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch_face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 24ms/step - accuracy: 0.8064 - loss: 0.6718 - val_accuracy: 0.9070 - val_loss: 0.3273 - learning_rate: 3.0000e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.32726 to 0.27614, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch_face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 24ms/step - accuracy: 0.8357 - loss: 0.5911 - val_accuracy: 0.9273 - val_loss: 0.2761 - learning_rate: 3.0000e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 0.27614 to 0.26654, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch_face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 24ms/step - accuracy: 0.8456 - loss: 0.5450 - val_accuracy: 0.9300 - val_loss: 0.2665 - learning_rate: 3.0000e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.26654 to 0.24829, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch_face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 24ms/step - accuracy: 0.8646 - loss: 0.4930 - val_accuracy: 0.9331 - val_loss: 0.2483 - learning_rate: 3.0000e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 0.24829 to 0.22909, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch_face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 24ms/step - accuracy: 0.8815 - loss: 0.4350 - val_accuracy: 0.9427 - val_loss: 0.2291 - learning_rate: 3.0000e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 0.22909 to 0.20022, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch_face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.8953 - loss: 0.3967 - val_accuracy: 0.9517 - val_loss: 0.2002 - learning_rate: 3.0000e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 0.20022 to 0.19094, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch_face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.9031 - loss: 0.3732 - val_accuracy: 0.9555 - val_loss: 0.1909 - learning_rate: 3.0000e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 0.19094 to 0.17410, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch_face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 24ms/step - accuracy: 0.9077 - loss: 0.3652 - val_accuracy: 0.9578 - val_loss: 0.1741 - learning_rate: 3.0000e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.17410\n",
      "178/178 - 4s - 24ms/step - accuracy: 0.9135 - loss: 0.3375 - val_accuracy: 0.9563 - val_loss: 0.1837 - learning_rate: 3.0000e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.17410\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.9167 - loss: 0.3279 - val_accuracy: 0.9445 - val_loss: 0.2023 - learning_rate: 3.0000e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.17410\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.9185 - loss: 0.3237 - val_accuracy: 0.9557 - val_loss: 0.1900 - learning_rate: 3.0000e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 0.17410 to 0.15885, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch_face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.9256 - loss: 0.2901 - val_accuracy: 0.9599 - val_loss: 0.1588 - learning_rate: 3.0000e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss improved from 0.15885 to 0.12304, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch_face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.9313 - loss: 0.2764 - val_accuracy: 0.9750 - val_loss: 0.1230 - learning_rate: 3.0000e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.12304\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.9294 - loss: 0.2710 - val_accuracy: 0.9654 - val_loss: 0.1446 - learning_rate: 3.0000e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss improved from 0.12304 to 0.12236, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch_face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.9360 - loss: 0.2546 - val_accuracy: 0.9714 - val_loss: 0.1224 - learning_rate: 3.0000e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.12236\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.9391 - loss: 0.2544 - val_accuracy: 0.9710 - val_loss: 0.1264 - learning_rate: 3.0000e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.12236\n",
      "178/178 - 5s - 26ms/step - accuracy: 0.9420 - loss: 0.2416 - val_accuracy: 0.9665 - val_loss: 0.1421 - learning_rate: 3.0000e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.12236\n",
      "178/178 - 5s - 27ms/step - accuracy: 0.9443 - loss: 0.2387 - val_accuracy: 0.9675 - val_loss: 0.1517 - learning_rate: 3.0000e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss improved from 0.12236 to 0.10132, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch_face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 27ms/step - accuracy: 0.9489 - loss: 0.2223 - val_accuracy: 0.9795 - val_loss: 0.1013 - learning_rate: 3.0000e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.10132\n",
      "178/178 - 5s - 27ms/step - accuracy: 0.9471 - loss: 0.2209 - val_accuracy: 0.9729 - val_loss: 0.1191 - learning_rate: 3.0000e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.10132\n",
      "178/178 - 5s - 28ms/step - accuracy: 0.9472 - loss: 0.2220 - val_accuracy: 0.9705 - val_loss: 0.1412 - learning_rate: 3.0000e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss improved from 0.10132 to 0.09505, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch_face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 28ms/step - accuracy: 0.9531 - loss: 0.2030 - val_accuracy: 0.9828 - val_loss: 0.0951 - learning_rate: 3.0000e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.09505\n",
      "178/178 - 5s - 27ms/step - accuracy: 0.9575 - loss: 0.1866 - val_accuracy: 0.9750 - val_loss: 0.1127 - learning_rate: 3.0000e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.09505\n",
      "178/178 - 5s - 27ms/step - accuracy: 0.9528 - loss: 0.2078 - val_accuracy: 0.9811 - val_loss: 0.0972 - learning_rate: 3.0000e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss improved from 0.09505 to 0.09360, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch_face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 26ms/step - accuracy: 0.9564 - loss: 0.1926 - val_accuracy: 0.9831 - val_loss: 0.0936 - learning_rate: 3.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 30.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.6618 ---\n",
      "\n",
      "===== 특징 조합 테스트: face_roll_scale, face_pitch, face_mouth_corners =====\n",
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "특징 추출(병렬): 100%|██████████| 1111/1111 [00:51<00:00, 21.65it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "청크 병합: 100%|██████████| 1111/1111 [00:24<00:00, 44.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터가 11356개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 1.43163, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch_face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 8s - 43ms/step - accuracy: 0.2891 - loss: 2.1980 - val_accuracy: 0.5412 - val_loss: 1.4316 - learning_rate: 3.0000e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 1.43163 to 0.82791, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch_face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 25ms/step - accuracy: 0.5185 - loss: 1.4920 - val_accuracy: 0.7524 - val_loss: 0.8279 - learning_rate: 3.0000e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 0.82791 to 0.61240, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch_face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 25ms/step - accuracy: 0.6632 - loss: 1.0863 - val_accuracy: 0.8132 - val_loss: 0.6124 - learning_rate: 3.0000e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 0.61240 to 0.46441, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch_face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 26ms/step - accuracy: 0.7461 - loss: 0.8421 - val_accuracy: 0.8734 - val_loss: 0.4644 - learning_rate: 3.0000e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 0.46441 to 0.44447, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch_face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 24ms/step - accuracy: 0.7891 - loss: 0.7288 - val_accuracy: 0.8626 - val_loss: 0.4445 - learning_rate: 3.0000e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 0.44447 to 0.32145, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch_face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 24ms/step - accuracy: 0.8152 - loss: 0.6325 - val_accuracy: 0.9072 - val_loss: 0.3215 - learning_rate: 3.0000e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.32145 to 0.29778, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch_face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 29ms/step - accuracy: 0.8460 - loss: 0.5503 - val_accuracy: 0.9108 - val_loss: 0.2978 - learning_rate: 3.0000e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 0.29778 to 0.21543, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch_face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 28ms/step - accuracy: 0.8595 - loss: 0.5070 - val_accuracy: 0.9446 - val_loss: 0.2154 - learning_rate: 3.0000e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.21543\n",
      "178/178 - 5s - 27ms/step - accuracy: 0.8723 - loss: 0.4631 - val_accuracy: 0.9298 - val_loss: 0.2490 - learning_rate: 3.0000e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 0.21543 to 0.20324, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch_face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 27ms/step - accuracy: 0.8825 - loss: 0.4331 - val_accuracy: 0.9455 - val_loss: 0.2032 - learning_rate: 3.0000e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.20324\n",
      "178/178 - 4s - 24ms/step - accuracy: 0.9016 - loss: 0.3780 - val_accuracy: 0.9316 - val_loss: 0.2494 - learning_rate: 3.0000e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 0.20324 to 0.16097, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch_face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 24ms/step - accuracy: 0.8993 - loss: 0.3750 - val_accuracy: 0.9630 - val_loss: 0.1610 - learning_rate: 3.0000e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 0.16097 to 0.15921, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch_face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 24ms/step - accuracy: 0.9074 - loss: 0.3545 - val_accuracy: 0.9612 - val_loss: 0.1592 - learning_rate: 3.0000e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 0.15921 to 0.14890, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch_face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 24ms/step - accuracy: 0.9134 - loss: 0.3343 - val_accuracy: 0.9663 - val_loss: 0.1489 - learning_rate: 3.0000e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.14890\n",
      "178/178 - 5s - 25ms/step - accuracy: 0.9155 - loss: 0.3282 - val_accuracy: 0.9552 - val_loss: 0.1826 - learning_rate: 3.0000e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss improved from 0.14890 to 0.13734, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch_face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 25ms/step - accuracy: 0.9255 - loss: 0.3028 - val_accuracy: 0.9710 - val_loss: 0.1373 - learning_rate: 3.0000e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 0.13734 to 0.13029, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch_face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 25ms/step - accuracy: 0.9279 - loss: 0.2860 - val_accuracy: 0.9689 - val_loss: 0.1303 - learning_rate: 3.0000e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss improved from 0.13029 to 0.12525, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch_face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 26ms/step - accuracy: 0.9289 - loss: 0.2777 - val_accuracy: 0.9722 - val_loss: 0.1253 - learning_rate: 3.0000e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss improved from 0.12525 to 0.12163, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch_face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 25ms/step - accuracy: 0.9279 - loss: 0.2900 - val_accuracy: 0.9741 - val_loss: 0.1216 - learning_rate: 3.0000e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.12163\n",
      "178/178 - 4s - 25ms/step - accuracy: 0.9340 - loss: 0.2647 - val_accuracy: 0.9726 - val_loss: 0.1287 - learning_rate: 3.0000e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.12163\n",
      "178/178 - 4s - 24ms/step - accuracy: 0.9406 - loss: 0.2416 - val_accuracy: 0.9644 - val_loss: 0.1427 - learning_rate: 3.0000e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss improved from 0.12163 to 0.11718, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch_face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 26ms/step - accuracy: 0.9414 - loss: 0.2454 - val_accuracy: 0.9749 - val_loss: 0.1172 - learning_rate: 3.0000e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.11718\n",
      "178/178 - 5s - 26ms/step - accuracy: 0.9451 - loss: 0.2315 - val_accuracy: 0.9695 - val_loss: 0.1365 - learning_rate: 3.0000e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss improved from 0.11718 to 0.09834, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch_face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 25ms/step - accuracy: 0.9473 - loss: 0.2175 - val_accuracy: 0.9802 - val_loss: 0.0983 - learning_rate: 3.0000e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.09834\n",
      "178/178 - 4s - 24ms/step - accuracy: 0.9466 - loss: 0.2268 - val_accuracy: 0.9780 - val_loss: 0.1084 - learning_rate: 3.0000e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.09834\n",
      "178/178 - 4s - 24ms/step - accuracy: 0.9465 - loss: 0.2275 - val_accuracy: 0.9787 - val_loss: 0.1047 - learning_rate: 3.0000e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.09834\n",
      "178/178 - 4s - 25ms/step - accuracy: 0.9482 - loss: 0.2125 - val_accuracy: 0.9732 - val_loss: 0.1240 - learning_rate: 3.0000e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.09834\n",
      "178/178 - 4s - 24ms/step - accuracy: 0.9523 - loss: 0.2102 - val_accuracy: 0.9734 - val_loss: 0.1191 - learning_rate: 3.0000e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss improved from 0.09834 to 0.08677, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch_face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 26ms/step - accuracy: 0.9517 - loss: 0.2123 - val_accuracy: 0.9846 - val_loss: 0.0868 - learning_rate: 3.0000e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.08677\n",
      "178/178 - 5s - 26ms/step - accuracy: 0.9563 - loss: 0.1947 - val_accuracy: 0.9807 - val_loss: 0.1013 - learning_rate: 3.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 29.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.7086 ---\n",
      "\n",
      "===== 특징 조합 테스트: face_roll_scale, face_pitch, face_inner_eyes =====\n",
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "특징 추출(병렬): 100%|██████████| 1111/1111 [00:42<00:00, 25.85it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "청크 병합: 100%|██████████| 1111/1111 [00:14<00:00, 78.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터가 11356개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 1.47056, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 7s - 39ms/step - accuracy: 0.2735 - loss: 2.2172 - val_accuracy: 0.5881 - val_loss: 1.4706 - learning_rate: 3.0000e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 1.47056 to 0.90311, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 25ms/step - accuracy: 0.5067 - loss: 1.5211 - val_accuracy: 0.7087 - val_loss: 0.9031 - learning_rate: 3.0000e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 0.90311 to 0.58065, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 25ms/step - accuracy: 0.6513 - loss: 1.0961 - val_accuracy: 0.8376 - val_loss: 0.5807 - learning_rate: 3.0000e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 0.58065 to 0.45970, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 26ms/step - accuracy: 0.7282 - loss: 0.8700 - val_accuracy: 0.8773 - val_loss: 0.4597 - learning_rate: 3.0000e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 0.45970 to 0.37336, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 26ms/step - accuracy: 0.7807 - loss: 0.7429 - val_accuracy: 0.8883 - val_loss: 0.3734 - learning_rate: 3.0000e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 0.37336 to 0.32995, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 24ms/step - accuracy: 0.8153 - loss: 0.6416 - val_accuracy: 0.9069 - val_loss: 0.3300 - learning_rate: 3.0000e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.32995 to 0.30268, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 25ms/step - accuracy: 0.8396 - loss: 0.5655 - val_accuracy: 0.9177 - val_loss: 0.3027 - learning_rate: 3.0000e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 0.30268 to 0.26322, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 25ms/step - accuracy: 0.8595 - loss: 0.5158 - val_accuracy: 0.9355 - val_loss: 0.2632 - learning_rate: 3.0000e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.26322 to 0.26213, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 25ms/step - accuracy: 0.8678 - loss: 0.4766 - val_accuracy: 0.9320 - val_loss: 0.2621 - learning_rate: 3.0000e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 0.26213 to 0.22702, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 27ms/step - accuracy: 0.8797 - loss: 0.4432 - val_accuracy: 0.9371 - val_loss: 0.2270 - learning_rate: 3.0000e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 0.22702 to 0.18983, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 26ms/step - accuracy: 0.8914 - loss: 0.4027 - val_accuracy: 0.9524 - val_loss: 0.1898 - learning_rate: 3.0000e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.18983\n",
      "178/178 - 4s - 24ms/step - accuracy: 0.9039 - loss: 0.3712 - val_accuracy: 0.9355 - val_loss: 0.2425 - learning_rate: 3.0000e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.18983\n",
      "178/178 - 4s - 25ms/step - accuracy: 0.9006 - loss: 0.3716 - val_accuracy: 0.9431 - val_loss: 0.2109 - learning_rate: 3.0000e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 0.18983 to 0.16182, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 25ms/step - accuracy: 0.9082 - loss: 0.3489 - val_accuracy: 0.9606 - val_loss: 0.1618 - learning_rate: 3.0000e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.16182\n",
      "178/178 - 4s - 24ms/step - accuracy: 0.9146 - loss: 0.3315 - val_accuracy: 0.9567 - val_loss: 0.1684 - learning_rate: 3.0000e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss improved from 0.16182 to 0.14396, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 25ms/step - accuracy: 0.9126 - loss: 0.3279 - val_accuracy: 0.9684 - val_loss: 0.1440 - learning_rate: 3.0000e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 0.14396 to 0.14116, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 31ms/step - accuracy: 0.9262 - loss: 0.3014 - val_accuracy: 0.9687 - val_loss: 0.1412 - learning_rate: 3.0000e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.14116\n",
      "178/178 - 5s - 26ms/step - accuracy: 0.9256 - loss: 0.2998 - val_accuracy: 0.9526 - val_loss: 0.1907 - learning_rate: 3.0000e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss improved from 0.14116 to 0.12617, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 22ms/step - accuracy: 0.9314 - loss: 0.2791 - val_accuracy: 0.9692 - val_loss: 0.1262 - learning_rate: 3.0000e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.12617\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.9323 - loss: 0.2721 - val_accuracy: 0.9626 - val_loss: 0.1437 - learning_rate: 3.0000e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.12617\n",
      "178/178 - 3s - 20ms/step - accuracy: 0.9340 - loss: 0.2671 - val_accuracy: 0.9692 - val_loss: 0.1303 - learning_rate: 3.0000e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss improved from 0.12617 to 0.11698, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.9390 - loss: 0.2541 - val_accuracy: 0.9729 - val_loss: 0.1170 - learning_rate: 3.0000e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.11698\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.9404 - loss: 0.2451 - val_accuracy: 0.9508 - val_loss: 0.1939 - learning_rate: 3.0000e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.11698\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.9405 - loss: 0.2526 - val_accuracy: 0.9732 - val_loss: 0.1259 - learning_rate: 3.0000e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.11698\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.9416 - loss: 0.2340 - val_accuracy: 0.9671 - val_loss: 0.1385 - learning_rate: 3.0000e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.11698\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.9442 - loss: 0.2296 - val_accuracy: 0.9704 - val_loss: 0.1246 - learning_rate: 3.0000e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss improved from 0.11698 to 0.10003, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.9465 - loss: 0.2204 - val_accuracy: 0.9825 - val_loss: 0.1000 - learning_rate: 3.0000e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.10003\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.9428 - loss: 0.2287 - val_accuracy: 0.9710 - val_loss: 0.1252 - learning_rate: 3.0000e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss improved from 0.10003 to 0.09907, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.9455 - loss: 0.2276 - val_accuracy: 0.9820 - val_loss: 0.0991 - learning_rate: 3.0000e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss improved from 0.09907 to 0.09401, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.9532 - loss: 0.2029 - val_accuracy: 0.9831 - val_loss: 0.0940 - learning_rate: 3.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 30.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.6310 ---\n",
      "\n",
      "===== 특징 조합 테스트: face_roll_scale, face_pitch =====\n",
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n",
      "학습 데이터가 11356개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 1.41904, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch\\models\\fold_7\\best_model.keras\n",
      "178/178 - 6s - 32ms/step - accuracy: 0.2855 - loss: 2.1853 - val_accuracy: 0.5469 - val_loss: 1.4190 - learning_rate: 3.0000e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 1.41904 to 0.93885, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 18ms/step - accuracy: 0.5284 - loss: 1.4874 - val_accuracy: 0.6924 - val_loss: 0.9388 - learning_rate: 3.0000e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 0.93885 to 0.58077, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 18ms/step - accuracy: 0.6612 - loss: 1.0912 - val_accuracy: 0.8461 - val_loss: 0.5808 - learning_rate: 3.0000e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 0.58077 to 0.45549, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 18ms/step - accuracy: 0.7385 - loss: 0.8591 - val_accuracy: 0.8755 - val_loss: 0.4555 - learning_rate: 3.0000e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 0.45549 to 0.37001, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.7868 - loss: 0.7317 - val_accuracy: 0.9003 - val_loss: 0.3700 - learning_rate: 3.0000e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 0.37001 to 0.33367, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.8199 - loss: 0.6486 - val_accuracy: 0.9109 - val_loss: 0.3337 - learning_rate: 3.0000e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.33367 to 0.28725, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.8426 - loss: 0.5709 - val_accuracy: 0.9253 - val_loss: 0.2872 - learning_rate: 3.0000e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 0.28725 to 0.26861, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.8595 - loss: 0.5204 - val_accuracy: 0.9283 - val_loss: 0.2686 - learning_rate: 3.0000e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.26861\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.8705 - loss: 0.4730 - val_accuracy: 0.9187 - val_loss: 0.2945 - learning_rate: 3.0000e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 0.26861 to 0.22082, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.8855 - loss: 0.4333 - val_accuracy: 0.9446 - val_loss: 0.2208 - learning_rate: 3.0000e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.22082\n",
      "178/178 - 3s - 18ms/step - accuracy: 0.8916 - loss: 0.4176 - val_accuracy: 0.9406 - val_loss: 0.2246 - learning_rate: 3.0000e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.22082\n",
      "178/178 - 3s - 18ms/step - accuracy: 0.8982 - loss: 0.3896 - val_accuracy: 0.9329 - val_loss: 0.2418 - learning_rate: 3.0000e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 0.22082 to 0.20427, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.8971 - loss: 0.3858 - val_accuracy: 0.9427 - val_loss: 0.2043 - learning_rate: 3.0000e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 0.20427 to 0.16440, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.9111 - loss: 0.3443 - val_accuracy: 0.9596 - val_loss: 0.1644 - learning_rate: 3.0000e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.16440\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.9108 - loss: 0.3447 - val_accuracy: 0.9485 - val_loss: 0.2019 - learning_rate: 3.0000e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.16440\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.9199 - loss: 0.3226 - val_accuracy: 0.9434 - val_loss: 0.2231 - learning_rate: 3.0000e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.16440\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.9221 - loss: 0.3052 - val_accuracy: 0.9567 - val_loss: 0.1718 - learning_rate: 3.0000e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.16440\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.9236 - loss: 0.3033 - val_accuracy: 0.9563 - val_loss: 0.1697 - learning_rate: 3.0000e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss improved from 0.16440 to 0.14645, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.9296 - loss: 0.2838 - val_accuracy: 0.9683 - val_loss: 0.1464 - learning_rate: 3.0000e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss improved from 0.14645 to 0.13562, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 20ms/step - accuracy: 0.9345 - loss: 0.2658 - val_accuracy: 0.9683 - val_loss: 0.1356 - learning_rate: 3.0000e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss improved from 0.13562 to 0.13270, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_pitch\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.9380 - loss: 0.2548 - val_accuracy: 0.9711 - val_loss: 0.1327 - learning_rate: 3.0000e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.13270\n",
      "178/178 - 3s - 18ms/step - accuracy: 0.9428 - loss: 0.2355 - val_accuracy: 0.9695 - val_loss: 0.1424 - learning_rate: 3.0000e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.13270\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.9369 - loss: 0.2523 - val_accuracy: 0.9546 - val_loss: 0.1756 - learning_rate: 3.0000e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.13270\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.9436 - loss: 0.2386 - val_accuracy: 0.9740 - val_loss: 0.1351 - learning_rate: 3.0000e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.13270\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.9451 - loss: 0.2318 - val_accuracy: 0.9669 - val_loss: 0.1384 - learning_rate: 3.0000e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.13270\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.9496 - loss: 0.2112 - val_accuracy: 0.9707 - val_loss: 0.1355 - learning_rate: 3.0000e-04\n",
      "Epoch 26: early stopping\n",
      "Restoring model weights from the end of the best epoch: 21.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.6162 ---\n",
      "\n",
      "===== 특징 조합 테스트: face_roll_scale, face_mouth_corners, face_inner_eyes =====\n",
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "특징 추출(병렬): 100%|██████████| 1111/1111 [00:37<00:00, 29.96it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "청크 병합: 100%|██████████| 1111/1111 [00:14<00:00, 77.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터가 11356개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 2.37386, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 7s - 37ms/step - accuracy: 0.1040 - loss: 2.6374 - val_accuracy: 0.2066 - val_loss: 2.3739 - learning_rate: 3.0000e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 2.37386 to 2.17643, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 22ms/step - accuracy: 0.1701 - loss: 2.4060 - val_accuracy: 0.2506 - val_loss: 2.1764 - learning_rate: 3.0000e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 2.17643 to 1.91691, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 22ms/step - accuracy: 0.2334 - loss: 2.2078 - val_accuracy: 0.3591 - val_loss: 1.9169 - learning_rate: 3.0000e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 1.91691 to 1.61949, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.3190 - loss: 1.9835 - val_accuracy: 0.4607 - val_loss: 1.6195 - learning_rate: 3.0000e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 1.61949 to 1.43749, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 24ms/step - accuracy: 0.3583 - loss: 1.8724 - val_accuracy: 0.5336 - val_loss: 1.4375 - learning_rate: 3.0000e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss did not improve from 1.43749\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.4224 - loss: 1.6790 - val_accuracy: 0.4709 - val_loss: 1.4759 - learning_rate: 3.0000e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 1.43749 to 1.13826, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.4618 - loss: 1.5398 - val_accuracy: 0.5906 - val_loss: 1.1383 - learning_rate: 3.0000e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 1.13826 to 1.12692, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.4970 - loss: 1.4401 - val_accuracy: 0.5767 - val_loss: 1.1269 - learning_rate: 3.0000e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 1.12692 to 1.02674, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.5253 - loss: 1.3570 - val_accuracy: 0.6174 - val_loss: 1.0267 - learning_rate: 3.0000e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 1.02674 to 1.00775, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.5444 - loss: 1.2948 - val_accuracy: 0.6390 - val_loss: 1.0077 - learning_rate: 3.0000e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 1.00775 to 0.88227, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.5643 - loss: 1.2377 - val_accuracy: 0.6890 - val_loss: 0.8823 - learning_rate: 3.0000e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.88227\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.5842 - loss: 1.1925 - val_accuracy: 0.6692 - val_loss: 0.9495 - learning_rate: 3.0000e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 0.88227 to 0.84026, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.6083 - loss: 1.1259 - val_accuracy: 0.6993 - val_loss: 0.8403 - learning_rate: 3.0000e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 0.84026 to 0.76176, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.6287 - loss: 1.0784 - val_accuracy: 0.7364 - val_loss: 0.7618 - learning_rate: 3.0000e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.76176\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.6368 - loss: 1.0617 - val_accuracy: 0.7168 - val_loss: 0.8063 - learning_rate: 3.0000e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss improved from 0.76176 to 0.70749, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 22ms/step - accuracy: 0.6574 - loss: 0.9929 - val_accuracy: 0.7626 - val_loss: 0.7075 - learning_rate: 3.0000e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 0.70749 to 0.70345, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.6795 - loss: 0.9585 - val_accuracy: 0.7592 - val_loss: 0.7035 - learning_rate: 3.0000e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.70345\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.6707 - loss: 0.9669 - val_accuracy: 0.7553 - val_loss: 0.7143 - learning_rate: 3.0000e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss improved from 0.70345 to 0.69956, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.6989 - loss: 0.9091 - val_accuracy: 0.7577 - val_loss: 0.6996 - learning_rate: 3.0000e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss improved from 0.69956 to 0.67010, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.7069 - loss: 0.8754 - val_accuracy: 0.7783 - val_loss: 0.6701 - learning_rate: 3.0000e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.67010\n",
      "178/178 - 4s - 22ms/step - accuracy: 0.7176 - loss: 0.8570 - val_accuracy: 0.7782 - val_loss: 0.6720 - learning_rate: 3.0000e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss improved from 0.67010 to 0.63090, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.7128 - loss: 0.8560 - val_accuracy: 0.7798 - val_loss: 0.6309 - learning_rate: 3.0000e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.63090\n",
      "178/178 - 4s - 22ms/step - accuracy: 0.7262 - loss: 0.8214 - val_accuracy: 0.7913 - val_loss: 0.6505 - learning_rate: 3.0000e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss improved from 0.63090 to 0.52183, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.7405 - loss: 0.7960 - val_accuracy: 0.8276 - val_loss: 0.5218 - learning_rate: 3.0000e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.52183\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.7484 - loss: 0.7679 - val_accuracy: 0.8265 - val_loss: 0.5277 - learning_rate: 3.0000e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.52183\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.7526 - loss: 0.7472 - val_accuracy: 0.8057 - val_loss: 0.5513 - learning_rate: 3.0000e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.52183\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.7528 - loss: 0.7586 - val_accuracy: 0.8189 - val_loss: 0.5599 - learning_rate: 3.0000e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.52183\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.7688 - loss: 0.7165 - val_accuracy: 0.8193 - val_loss: 0.5311 - learning_rate: 3.0000e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.52183\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.7612 - loss: 0.7204 - val_accuracy: 0.8138 - val_loss: 0.5845 - learning_rate: 3.0000e-04\n",
      "Epoch 29: early stopping\n",
      "Restoring model weights from the end of the best epoch: 24.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.5462 ---\n",
      "\n",
      "===== 특징 조합 테스트: face_roll_scale, face_mouth_corners =====\n",
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "특징 추출(병렬): 100%|██████████| 1111/1111 [00:36<00:00, 30.17it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "청크 병합: 100%|██████████| 1111/1111 [00:13<00:00, 81.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터가 11356개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 2.45079, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 6s - 33ms/step - accuracy: 0.0864 - loss: 2.6442 - val_accuracy: 0.1717 - val_loss: 2.4508 - learning_rate: 3.0000e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 2.45079 to 2.16858, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 18ms/step - accuracy: 0.1610 - loss: 2.4093 - val_accuracy: 0.2506 - val_loss: 2.1686 - learning_rate: 3.0000e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 2.16858 to 1.93188, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.2236 - loss: 2.2365 - val_accuracy: 0.3419 - val_loss: 1.9319 - learning_rate: 3.0000e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 1.93188 to 1.72662, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.3123 - loss: 2.0203 - val_accuracy: 0.4607 - val_loss: 1.7266 - learning_rate: 3.0000e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 1.72662 to 1.40598, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 20ms/step - accuracy: 0.3909 - loss: 1.7884 - val_accuracy: 0.5266 - val_loss: 1.4060 - learning_rate: 3.0000e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 1.40598 to 1.19728, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.4498 - loss: 1.6207 - val_accuracy: 0.5951 - val_loss: 1.1973 - learning_rate: 3.0000e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 1.19728 to 1.08341, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 20ms/step - accuracy: 0.4855 - loss: 1.4789 - val_accuracy: 0.6423 - val_loss: 1.0834 - learning_rate: 3.0000e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss did not improve from 1.08341\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.5362 - loss: 1.3211 - val_accuracy: 0.5978 - val_loss: 1.1287 - learning_rate: 3.0000e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 1.08341 to 1.03978, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.5665 - loss: 1.2402 - val_accuracy: 0.6251 - val_loss: 1.0398 - learning_rate: 3.0000e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 1.03978 to 0.90953, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 20ms/step - accuracy: 0.5832 - loss: 1.1768 - val_accuracy: 0.6912 - val_loss: 0.9095 - learning_rate: 3.0000e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 0.90953 to 0.85377, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 20ms/step - accuracy: 0.6090 - loss: 1.1273 - val_accuracy: 0.6785 - val_loss: 0.8538 - learning_rate: 3.0000e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 0.85377 to 0.74099, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.6291 - loss: 1.0595 - val_accuracy: 0.7454 - val_loss: 0.7410 - learning_rate: 3.0000e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 0.74099 to 0.69139, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.6596 - loss: 1.0020 - val_accuracy: 0.7593 - val_loss: 0.6914 - learning_rate: 3.0000e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 0.69139 to 0.63851, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.6777 - loss: 0.9481 - val_accuracy: 0.7806 - val_loss: 0.6385 - learning_rate: 3.0000e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.63851\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.6969 - loss: 0.9001 - val_accuracy: 0.7848 - val_loss: 0.6428 - learning_rate: 3.0000e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss improved from 0.63851 to 0.55885, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.7109 - loss: 0.8610 - val_accuracy: 0.8128 - val_loss: 0.5588 - learning_rate: 3.0000e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 0.55885 to 0.51973, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.7278 - loss: 0.8088 - val_accuracy: 0.8270 - val_loss: 0.5197 - learning_rate: 3.0000e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.51973\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.7404 - loss: 0.7914 - val_accuracy: 0.8181 - val_loss: 0.5373 - learning_rate: 3.0000e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss improved from 0.51973 to 0.50559, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.7559 - loss: 0.7436 - val_accuracy: 0.8304 - val_loss: 0.5056 - learning_rate: 3.0000e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss improved from 0.50559 to 0.50373, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.7648 - loss: 0.7266 - val_accuracy: 0.8367 - val_loss: 0.5037 - learning_rate: 3.0000e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss improved from 0.50373 to 0.48811, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 20ms/step - accuracy: 0.7645 - loss: 0.7141 - val_accuracy: 0.8372 - val_loss: 0.4881 - learning_rate: 3.0000e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss improved from 0.48811 to 0.42191, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.7866 - loss: 0.6692 - val_accuracy: 0.8618 - val_loss: 0.4219 - learning_rate: 3.0000e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.42191\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.7933 - loss: 0.6436 - val_accuracy: 0.8383 - val_loss: 0.4797 - learning_rate: 3.0000e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.42191\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.7968 - loss: 0.6439 - val_accuracy: 0.8568 - val_loss: 0.4420 - learning_rate: 3.0000e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss improved from 0.42191 to 0.39735, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.8073 - loss: 0.6119 - val_accuracy: 0.8716 - val_loss: 0.3973 - learning_rate: 3.0000e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss improved from 0.39735 to 0.39011, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.8136 - loss: 0.5915 - val_accuracy: 0.8629 - val_loss: 0.3901 - learning_rate: 3.0000e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss improved from 0.39011 to 0.37448, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.8150 - loss: 0.5943 - val_accuracy: 0.8827 - val_loss: 0.3745 - learning_rate: 3.0000e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.37448\n",
      "178/178 - 3s - 20ms/step - accuracy: 0.8203 - loss: 0.5710 - val_accuracy: 0.8690 - val_loss: 0.3866 - learning_rate: 3.0000e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss improved from 0.37448 to 0.32531, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.8358 - loss: 0.5424 - val_accuracy: 0.8997 - val_loss: 0.3253 - learning_rate: 3.0000e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.32531\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.8322 - loss: 0.5375 - val_accuracy: 0.8865 - val_loss: 0.3503 - learning_rate: 3.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 29.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.6091 ---\n",
      "\n",
      "===== 특징 조합 테스트: face_roll_scale, face_inner_eyes =====\n",
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "특징 추출(병렬): 100%|██████████| 1111/1111 [00:37<00:00, 29.74it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "청크 병합: 100%|██████████| 1111/1111 [00:13<00:00, 79.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터가 11356개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 2.46421, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 6s - 32ms/step - accuracy: 0.0865 - loss: 2.6416 - val_accuracy: 0.1738 - val_loss: 2.4642 - learning_rate: 3.0000e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 2.46421 to 2.14933, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 18ms/step - accuracy: 0.1627 - loss: 2.4056 - val_accuracy: 0.2504 - val_loss: 2.1493 - learning_rate: 3.0000e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 2.14933 to 1.94494, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 18ms/step - accuracy: 0.2190 - loss: 2.2091 - val_accuracy: 0.3438 - val_loss: 1.9449 - learning_rate: 3.0000e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 1.94494 to 1.61388, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.3248 - loss: 1.9488 - val_accuracy: 0.4513 - val_loss: 1.6139 - learning_rate: 3.0000e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 1.61388 to 1.35884, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.3849 - loss: 1.7765 - val_accuracy: 0.5387 - val_loss: 1.3588 - learning_rate: 3.0000e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 1.35884 to 1.21402, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.4414 - loss: 1.5995 - val_accuracy: 0.5761 - val_loss: 1.2140 - learning_rate: 3.0000e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 1.21402 to 1.06995, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.4896 - loss: 1.4447 - val_accuracy: 0.6149 - val_loss: 1.0699 - learning_rate: 3.0000e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 1.06995 to 0.97653, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.5328 - loss: 1.3312 - val_accuracy: 0.6538 - val_loss: 0.9765 - learning_rate: 3.0000e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.97653 to 0.95151, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.5584 - loss: 1.2517 - val_accuracy: 0.6530 - val_loss: 0.9515 - learning_rate: 3.0000e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 0.95151 to 0.88241, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.5803 - loss: 1.1924 - val_accuracy: 0.6801 - val_loss: 0.8824 - learning_rate: 3.0000e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 0.88241 to 0.84341, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.6125 - loss: 1.1123 - val_accuracy: 0.6984 - val_loss: 0.8434 - learning_rate: 3.0000e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 0.84341 to 0.77955, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.6398 - loss: 1.0561 - val_accuracy: 0.7152 - val_loss: 0.7796 - learning_rate: 3.0000e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 0.77955 to 0.68614, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.6508 - loss: 1.0113 - val_accuracy: 0.7827 - val_loss: 0.6861 - learning_rate: 3.0000e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 0.68614 to 0.64941, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.6773 - loss: 0.9598 - val_accuracy: 0.7827 - val_loss: 0.6494 - learning_rate: 3.0000e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss improved from 0.64941 to 0.61251, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.6925 - loss: 0.9264 - val_accuracy: 0.7870 - val_loss: 0.6125 - learning_rate: 3.0000e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss improved from 0.61251 to 0.58143, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.7068 - loss: 0.8749 - val_accuracy: 0.8014 - val_loss: 0.5814 - learning_rate: 3.0000e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 0.58143 to 0.57117, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.7156 - loss: 0.8531 - val_accuracy: 0.8048 - val_loss: 0.5712 - learning_rate: 3.0000e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.57117\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.7326 - loss: 0.8100 - val_accuracy: 0.8027 - val_loss: 0.5738 - learning_rate: 3.0000e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.57117\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.7479 - loss: 0.7712 - val_accuracy: 0.7785 - val_loss: 0.6247 - learning_rate: 3.0000e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss improved from 0.57117 to 0.50591, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.7413 - loss: 0.7772 - val_accuracy: 0.8445 - val_loss: 0.5059 - learning_rate: 3.0000e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss improved from 0.50591 to 0.45534, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.7621 - loss: 0.7471 - val_accuracy: 0.8524 - val_loss: 0.4553 - learning_rate: 3.0000e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss improved from 0.45534 to 0.45533, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.7681 - loss: 0.7219 - val_accuracy: 0.8467 - val_loss: 0.4553 - learning_rate: 3.0000e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.45533\n",
      "178/178 - 3s - 18ms/step - accuracy: 0.7787 - loss: 0.6870 - val_accuracy: 0.8470 - val_loss: 0.4629 - learning_rate: 3.0000e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss improved from 0.45533 to 0.42428, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.7852 - loss: 0.6628 - val_accuracy: 0.8548 - val_loss: 0.4243 - learning_rate: 3.0000e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss improved from 0.42428 to 0.39695, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.7955 - loss: 0.6446 - val_accuracy: 0.8692 - val_loss: 0.3969 - learning_rate: 3.0000e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss improved from 0.39695 to 0.39012, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.7973 - loss: 0.6323 - val_accuracy: 0.8758 - val_loss: 0.3901 - learning_rate: 3.0000e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.39012\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.8109 - loss: 0.5924 - val_accuracy: 0.8714 - val_loss: 0.3990 - learning_rate: 3.0000e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss improved from 0.39012 to 0.37759, saving model to outputs/step3_1_feature_test\\face_roll_scale_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.8099 - loss: 0.6079 - val_accuracy: 0.8750 - val_loss: 0.3776 - learning_rate: 3.0000e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.37759\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.8222 - loss: 0.5811 - val_accuracy: 0.8749 - val_loss: 0.3798 - learning_rate: 3.0000e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.37759\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.8175 - loss: 0.5738 - val_accuracy: 0.8678 - val_loss: 0.3827 - learning_rate: 3.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 28.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.5248 ---\n",
      "\n",
      "===== 특징 조합 테스트: face_roll_scale =====\n",
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "특징 추출(병렬): 100%|██████████| 1111/1111 [00:36<00:00, 30.37it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "청크 병합: 100%|██████████| 1111/1111 [00:12<00:00, 88.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터가 11356개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 2.51143, saving model to outputs/step3_1_feature_test\\face_roll_scale\\models\\fold_7\\best_model.keras\n",
      "178/178 - 6s - 31ms/step - accuracy: 0.0902 - loss: 2.6490 - val_accuracy: 0.1691 - val_loss: 2.5114 - learning_rate: 3.0000e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 2.51143 to 2.13183, saving model to outputs/step3_1_feature_test\\face_roll_scale\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 18ms/step - accuracy: 0.1618 - loss: 2.4261 - val_accuracy: 0.2112 - val_loss: 2.1318 - learning_rate: 3.0000e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 2.13183 to 1.78510, saving model to outputs/step3_1_feature_test\\face_roll_scale\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 18ms/step - accuracy: 0.2498 - loss: 2.1495 - val_accuracy: 0.4091 - val_loss: 1.7851 - learning_rate: 3.0000e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 1.78510 to 1.77826, saving model to outputs/step3_1_feature_test\\face_roll_scale\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 18ms/step - accuracy: 0.3289 - loss: 1.9349 - val_accuracy: 0.3992 - val_loss: 1.7783 - learning_rate: 3.0000e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 1.77826 to 1.45446, saving model to outputs/step3_1_feature_test\\face_roll_scale\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.3975 - loss: 1.7520 - val_accuracy: 0.5074 - val_loss: 1.4545 - learning_rate: 3.0000e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 1.45446 to 1.24254, saving model to outputs/step3_1_feature_test\\face_roll_scale\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 18ms/step - accuracy: 0.4520 - loss: 1.5998 - val_accuracy: 0.5983 - val_loss: 1.2425 - learning_rate: 3.0000e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 1.24254 to 1.17155, saving model to outputs/step3_1_feature_test\\face_roll_scale\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 18ms/step - accuracy: 0.4884 - loss: 1.4882 - val_accuracy: 0.5698 - val_loss: 1.1716 - learning_rate: 3.0000e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 1.17155 to 0.95332, saving model to outputs/step3_1_feature_test\\face_roll_scale\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.5382 - loss: 1.3407 - val_accuracy: 0.6842 - val_loss: 0.9533 - learning_rate: 3.0000e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.95332 to 0.88360, saving model to outputs/step3_1_feature_test\\face_roll_scale\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.5708 - loss: 1.2396 - val_accuracy: 0.6987 - val_loss: 0.8836 - learning_rate: 3.0000e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 0.88360 to 0.78526, saving model to outputs/step3_1_feature_test\\face_roll_scale\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 18ms/step - accuracy: 0.6015 - loss: 1.1591 - val_accuracy: 0.7433 - val_loss: 0.7853 - learning_rate: 3.0000e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.78526\n",
      "178/178 - 3s - 18ms/step - accuracy: 0.6259 - loss: 1.1012 - val_accuracy: 0.7270 - val_loss: 0.7959 - learning_rate: 3.0000e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.78526\n",
      "178/178 - 3s - 18ms/step - accuracy: 0.6428 - loss: 1.0507 - val_accuracy: 0.7300 - val_loss: 0.7903 - learning_rate: 3.0000e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 0.78526 to 0.69496, saving model to outputs/step3_1_feature_test\\face_roll_scale\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 18ms/step - accuracy: 0.6594 - loss: 1.0157 - val_accuracy: 0.7596 - val_loss: 0.6950 - learning_rate: 3.0000e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 0.69496 to 0.61425, saving model to outputs/step3_1_feature_test\\face_roll_scale\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 18ms/step - accuracy: 0.6823 - loss: 0.9462 - val_accuracy: 0.7967 - val_loss: 0.6142 - learning_rate: 3.0000e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.61425\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.7064 - loss: 0.8809 - val_accuracy: 0.7765 - val_loss: 0.6346 - learning_rate: 3.0000e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss improved from 0.61425 to 0.56162, saving model to outputs/step3_1_feature_test\\face_roll_scale\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.7108 - loss: 0.8589 - val_accuracy: 0.8074 - val_loss: 0.5616 - learning_rate: 3.0000e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.56162\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.7282 - loss: 0.8210 - val_accuracy: 0.8021 - val_loss: 0.5766 - learning_rate: 3.0000e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss improved from 0.56162 to 0.49779, saving model to outputs/step3_1_feature_test\\face_roll_scale\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.7376 - loss: 0.7927 - val_accuracy: 0.8276 - val_loss: 0.4978 - learning_rate: 3.0000e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.49779\n",
      "178/178 - 3s - 18ms/step - accuracy: 0.7492 - loss: 0.7699 - val_accuracy: 0.8210 - val_loss: 0.5241 - learning_rate: 3.0000e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss improved from 0.49779 to 0.47047, saving model to outputs/step3_1_feature_test\\face_roll_scale\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 18ms/step - accuracy: 0.7552 - loss: 0.7513 - val_accuracy: 0.8366 - val_loss: 0.4705 - learning_rate: 3.0000e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss improved from 0.47047 to 0.46363, saving model to outputs/step3_1_feature_test\\face_roll_scale\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 18ms/step - accuracy: 0.7684 - loss: 0.7152 - val_accuracy: 0.8298 - val_loss: 0.4636 - learning_rate: 3.0000e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss improved from 0.46363 to 0.43278, saving model to outputs/step3_1_feature_test\\face_roll_scale\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 18ms/step - accuracy: 0.7741 - loss: 0.7029 - val_accuracy: 0.8578 - val_loss: 0.4328 - learning_rate: 3.0000e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss improved from 0.43278 to 0.42187, saving model to outputs/step3_1_feature_test\\face_roll_scale\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.7895 - loss: 0.6647 - val_accuracy: 0.8550 - val_loss: 0.4219 - learning_rate: 3.0000e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.42187\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.7958 - loss: 0.6421 - val_accuracy: 0.8589 - val_loss: 0.4314 - learning_rate: 3.0000e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss improved from 0.42187 to 0.40196, saving model to outputs/step3_1_feature_test\\face_roll_scale\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.8021 - loss: 0.6283 - val_accuracy: 0.8690 - val_loss: 0.4020 - learning_rate: 3.0000e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.40196\n",
      "178/178 - 3s - 18ms/step - accuracy: 0.8054 - loss: 0.6201 - val_accuracy: 0.8267 - val_loss: 0.4721 - learning_rate: 3.0000e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss improved from 0.40196 to 0.36909, saving model to outputs/step3_1_feature_test\\face_roll_scale\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 20ms/step - accuracy: 0.8139 - loss: 0.5965 - val_accuracy: 0.8708 - val_loss: 0.3691 - learning_rate: 3.0000e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.36909\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.8168 - loss: 0.5851 - val_accuracy: 0.8810 - val_loss: 0.3748 - learning_rate: 3.0000e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.36909\n",
      "178/178 - 3s - 18ms/step - accuracy: 0.8241 - loss: 0.5670 - val_accuracy: 0.8702 - val_loss: 0.3761 - learning_rate: 3.0000e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss improved from 0.36909 to 0.36416, saving model to outputs/step3_1_feature_test\\face_roll_scale\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.8248 - loss: 0.5632 - val_accuracy: 0.8804 - val_loss: 0.3642 - learning_rate: 3.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 30.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.5333 ---\n",
      "\n",
      "===== 특징 조합 테스트: face_pitch, face_mouth_corners, face_inner_eyes =====\n",
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "특징 추출(병렬): 100%|██████████| 1111/1111 [00:35<00:00, 31.38it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "청크 병합: 100%|██████████| 1111/1111 [00:13<00:00, 80.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터가 11356개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 1.39020, saving model to outputs/step3_1_feature_test\\face_pitch_face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 6s - 32ms/step - accuracy: 0.3020 - loss: 2.1421 - val_accuracy: 0.5727 - val_loss: 1.3902 - learning_rate: 3.0000e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 1.39020 to 0.92708, saving model to outputs/step3_1_feature_test\\face_pitch_face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 18ms/step - accuracy: 0.5088 - loss: 1.5120 - val_accuracy: 0.7105 - val_loss: 0.9271 - learning_rate: 3.0000e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 0.92708 to 0.74924, saving model to outputs/step3_1_feature_test\\face_pitch_face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 18ms/step - accuracy: 0.6278 - loss: 1.1812 - val_accuracy: 0.7683 - val_loss: 0.7492 - learning_rate: 3.0000e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 0.74924 to 0.61601, saving model to outputs/step3_1_feature_test\\face_pitch_face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.6911 - loss: 1.0062 - val_accuracy: 0.8131 - val_loss: 0.6160 - learning_rate: 3.0000e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 0.61601 to 0.55807, saving model to outputs/step3_1_feature_test\\face_pitch_face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.7212 - loss: 0.9144 - val_accuracy: 0.8258 - val_loss: 0.5581 - learning_rate: 3.0000e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 0.55807 to 0.50376, saving model to outputs/step3_1_feature_test\\face_pitch_face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.7540 - loss: 0.8203 - val_accuracy: 0.8424 - val_loss: 0.5038 - learning_rate: 3.0000e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.50376 to 0.43887, saving model to outputs/step3_1_feature_test\\face_pitch_face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.7731 - loss: 0.7648 - val_accuracy: 0.8774 - val_loss: 0.4389 - learning_rate: 3.0000e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 0.43887 to 0.41642, saving model to outputs/step3_1_feature_test\\face_pitch_face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.7890 - loss: 0.7196 - val_accuracy: 0.8744 - val_loss: 0.4164 - learning_rate: 3.0000e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.41642 to 0.35230, saving model to outputs/step3_1_feature_test\\face_pitch_face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.8067 - loss: 0.6602 - val_accuracy: 0.9011 - val_loss: 0.3523 - learning_rate: 3.0000e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.35230\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.8199 - loss: 0.6357 - val_accuracy: 0.8856 - val_loss: 0.3698 - learning_rate: 3.0000e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 0.35230 to 0.31370, saving model to outputs/step3_1_feature_test\\face_pitch_face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.8244 - loss: 0.6070 - val_accuracy: 0.9093 - val_loss: 0.3137 - learning_rate: 3.0000e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.31370\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.8450 - loss: 0.5451 - val_accuracy: 0.9044 - val_loss: 0.3410 - learning_rate: 3.0000e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 0.31370 to 0.28786, saving model to outputs/step3_1_feature_test\\face_pitch_face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.8419 - loss: 0.5451 - val_accuracy: 0.9213 - val_loss: 0.2879 - learning_rate: 3.0000e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.28786\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.8507 - loss: 0.5227 - val_accuracy: 0.9129 - val_loss: 0.2913 - learning_rate: 3.0000e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss improved from 0.28786 to 0.25519, saving model to outputs/step3_1_feature_test\\face_pitch_face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.8609 - loss: 0.5060 - val_accuracy: 0.9316 - val_loss: 0.2552 - learning_rate: 3.0000e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.25519\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.8619 - loss: 0.4930 - val_accuracy: 0.9223 - val_loss: 0.2670 - learning_rate: 3.0000e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.25519\n",
      "178/178 - 5s - 31ms/step - accuracy: 0.8744 - loss: 0.4667 - val_accuracy: 0.9302 - val_loss: 0.2598 - learning_rate: 3.0000e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss improved from 0.25519 to 0.22539, saving model to outputs/step3_1_feature_test\\face_pitch_face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 10s - 58ms/step - accuracy: 0.8747 - loss: 0.4468 - val_accuracy: 0.9413 - val_loss: 0.2254 - learning_rate: 3.0000e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.22539\n",
      "178/178 - 11s - 60ms/step - accuracy: 0.8847 - loss: 0.4284 - val_accuracy: 0.9371 - val_loss: 0.2356 - learning_rate: 3.0000e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.22539\n",
      "178/178 - 11s - 59ms/step - accuracy: 0.8846 - loss: 0.4226 - val_accuracy: 0.9412 - val_loss: 0.2272 - learning_rate: 3.0000e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss improved from 0.22539 to 0.20009, saving model to outputs/step3_1_feature_test\\face_pitch_face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 6s - 31ms/step - accuracy: 0.8900 - loss: 0.4153 - val_accuracy: 0.9496 - val_loss: 0.2001 - learning_rate: 3.0000e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.20009\n",
      "178/178 - 6s - 31ms/step - accuracy: 0.8952 - loss: 0.3981 - val_accuracy: 0.9397 - val_loss: 0.2215 - learning_rate: 3.0000e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.20009\n",
      "178/178 - 6s - 31ms/step - accuracy: 0.8942 - loss: 0.3944 - val_accuracy: 0.9216 - val_loss: 0.2587 - learning_rate: 3.0000e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.20009\n",
      "178/178 - 5s - 31ms/step - accuracy: 0.8977 - loss: 0.3813 - val_accuracy: 0.9434 - val_loss: 0.2168 - learning_rate: 3.0000e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss improved from 0.20009 to 0.17715, saving model to outputs/step3_1_feature_test\\face_pitch_face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 31ms/step - accuracy: 0.9023 - loss: 0.3750 - val_accuracy: 0.9549 - val_loss: 0.1772 - learning_rate: 3.0000e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.17715\n",
      "178/178 - 6s - 31ms/step - accuracy: 0.9038 - loss: 0.3576 - val_accuracy: 0.9529 - val_loss: 0.1831 - learning_rate: 3.0000e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.17715\n",
      "178/178 - 6s - 31ms/step - accuracy: 0.9076 - loss: 0.3590 - val_accuracy: 0.9493 - val_loss: 0.1843 - learning_rate: 3.0000e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss improved from 0.17715 to 0.15899, saving model to outputs/step3_1_feature_test\\face_pitch_face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 6s - 32ms/step - accuracy: 0.9085 - loss: 0.3487 - val_accuracy: 0.9603 - val_loss: 0.1590 - learning_rate: 3.0000e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.15899\n",
      "178/178 - 6s - 32ms/step - accuracy: 0.9057 - loss: 0.3539 - val_accuracy: 0.9594 - val_loss: 0.1596 - learning_rate: 3.0000e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.15899\n",
      "178/178 - 10s - 56ms/step - accuracy: 0.9060 - loss: 0.3520 - val_accuracy: 0.9573 - val_loss: 0.1690 - learning_rate: 3.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 28.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.5248 ---\n",
      "\n",
      "===== 특징 조합 테스트: face_pitch, face_mouth_corners =====\n",
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "특징 추출(병렬): 100%|██████████| 1111/1111 [00:53<00:00, 20.78it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "청크 병합: 100%|██████████| 1111/1111 [00:15<00:00, 70.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터가 11356개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 1.42052, saving model to outputs/step3_1_feature_test\\face_pitch_face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 10s - 57ms/step - accuracy: 0.3035 - loss: 2.1397 - val_accuracy: 0.5423 - val_loss: 1.4205 - learning_rate: 3.0000e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 1.42052 to 0.99745, saving model to outputs/step3_1_feature_test\\face_pitch_face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 10s - 55ms/step - accuracy: 0.5058 - loss: 1.5399 - val_accuracy: 0.6941 - val_loss: 0.9975 - learning_rate: 3.0000e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 0.99745 to 0.73432, saving model to outputs/step3_1_feature_test\\face_pitch_face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 6s - 33ms/step - accuracy: 0.6087 - loss: 1.2226 - val_accuracy: 0.7867 - val_loss: 0.7343 - learning_rate: 3.0000e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 0.73432 to 0.65004, saving model to outputs/step3_1_feature_test\\face_pitch_face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 6s - 34ms/step - accuracy: 0.6855 - loss: 1.0066 - val_accuracy: 0.7979 - val_loss: 0.6500 - learning_rate: 3.0000e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 0.65004 to 0.54903, saving model to outputs/step3_1_feature_test\\face_pitch_face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 6s - 34ms/step - accuracy: 0.7161 - loss: 0.9158 - val_accuracy: 0.8303 - val_loss: 0.5490 - learning_rate: 3.0000e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 0.54903 to 0.52828, saving model to outputs/step3_1_feature_test\\face_pitch_face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 10s - 56ms/step - accuracy: 0.7499 - loss: 0.8266 - val_accuracy: 0.8357 - val_loss: 0.5283 - learning_rate: 3.0000e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.52828 to 0.49382, saving model to outputs/step3_1_feature_test\\face_pitch_face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 6s - 33ms/step - accuracy: 0.7680 - loss: 0.7767 - val_accuracy: 0.8524 - val_loss: 0.4938 - learning_rate: 3.0000e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 0.49382 to 0.45644, saving model to outputs/step3_1_feature_test\\face_pitch_face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 6s - 34ms/step - accuracy: 0.7866 - loss: 0.7262 - val_accuracy: 0.8584 - val_loss: 0.4564 - learning_rate: 3.0000e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.45644 to 0.39528, saving model to outputs/step3_1_feature_test\\face_pitch_face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 6s - 34ms/step - accuracy: 0.8043 - loss: 0.6663 - val_accuracy: 0.8843 - val_loss: 0.3953 - learning_rate: 3.0000e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 0.39528 to 0.37119, saving model to outputs/step3_1_feature_test\\face_pitch_face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 6s - 34ms/step - accuracy: 0.8130 - loss: 0.6420 - val_accuracy: 0.8945 - val_loss: 0.3712 - learning_rate: 3.0000e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.37119\n",
      "178/178 - 10s - 58ms/step - accuracy: 0.8161 - loss: 0.6371 - val_accuracy: 0.8807 - val_loss: 0.3955 - learning_rate: 3.0000e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 0.37119 to 0.35291, saving model to outputs/step3_1_feature_test\\face_pitch_face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 6s - 34ms/step - accuracy: 0.8332 - loss: 0.5787 - val_accuracy: 0.8942 - val_loss: 0.3529 - learning_rate: 3.0000e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 0.35291 to 0.32814, saving model to outputs/step3_1_feature_test\\face_pitch_face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 11s - 62ms/step - accuracy: 0.8360 - loss: 0.5796 - val_accuracy: 0.9091 - val_loss: 0.3281 - learning_rate: 3.0000e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 0.32814 to 0.28938, saving model to outputs/step3_1_feature_test\\face_pitch_face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 7s - 38ms/step - accuracy: 0.8494 - loss: 0.5335 - val_accuracy: 0.9225 - val_loss: 0.2894 - learning_rate: 3.0000e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.28938\n",
      "178/178 - 6s - 35ms/step - accuracy: 0.8521 - loss: 0.5272 - val_accuracy: 0.9169 - val_loss: 0.3065 - learning_rate: 3.0000e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.28938\n",
      "178/178 - 6s - 36ms/step - accuracy: 0.8558 - loss: 0.5187 - val_accuracy: 0.9079 - val_loss: 0.3199 - learning_rate: 3.0000e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 0.28938 to 0.25965, saving model to outputs/step3_1_feature_test\\face_pitch_face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 7s - 38ms/step - accuracy: 0.8636 - loss: 0.4964 - val_accuracy: 0.9313 - val_loss: 0.2597 - learning_rate: 3.0000e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.25965\n",
      "178/178 - 7s - 38ms/step - accuracy: 0.8639 - loss: 0.4854 - val_accuracy: 0.9102 - val_loss: 0.3014 - learning_rate: 3.0000e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss improved from 0.25965 to 0.25878, saving model to outputs/step3_1_feature_test\\face_pitch_face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 7s - 37ms/step - accuracy: 0.8770 - loss: 0.4586 - val_accuracy: 0.9301 - val_loss: 0.2588 - learning_rate: 3.0000e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss improved from 0.25878 to 0.24459, saving model to outputs/step3_1_feature_test\\face_pitch_face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 10s - 56ms/step - accuracy: 0.8804 - loss: 0.4443 - val_accuracy: 0.9328 - val_loss: 0.2446 - learning_rate: 3.0000e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.24459\n",
      "178/178 - 6s - 35ms/step - accuracy: 0.8813 - loss: 0.4291 - val_accuracy: 0.9313 - val_loss: 0.2486 - learning_rate: 3.0000e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.24459\n",
      "178/178 - 6s - 35ms/step - accuracy: 0.8787 - loss: 0.4366 - val_accuracy: 0.9316 - val_loss: 0.2449 - learning_rate: 3.0000e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.24459\n",
      "178/178 - 10s - 58ms/step - accuracy: 0.8794 - loss: 0.4305 - val_accuracy: 0.9319 - val_loss: 0.2457 - learning_rate: 3.0000e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss improved from 0.24459 to 0.21877, saving model to outputs/step3_1_feature_test\\face_pitch_face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 6s - 35ms/step - accuracy: 0.8871 - loss: 0.4096 - val_accuracy: 0.9419 - val_loss: 0.2188 - learning_rate: 3.0000e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.21877\n",
      "178/178 - 6s - 35ms/step - accuracy: 0.8928 - loss: 0.4040 - val_accuracy: 0.9322 - val_loss: 0.2395 - learning_rate: 3.0000e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss improved from 0.21877 to 0.20147, saving model to outputs/step3_1_feature_test\\face_pitch_face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 6s - 35ms/step - accuracy: 0.8919 - loss: 0.3935 - val_accuracy: 0.9490 - val_loss: 0.2015 - learning_rate: 3.0000e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.20147\n",
      "178/178 - 6s - 35ms/step - accuracy: 0.8920 - loss: 0.3867 - val_accuracy: 0.9389 - val_loss: 0.2129 - learning_rate: 3.0000e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss improved from 0.20147 to 0.19957, saving model to outputs/step3_1_feature_test\\face_pitch_face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 6s - 35ms/step - accuracy: 0.9015 - loss: 0.3637 - val_accuracy: 0.9493 - val_loss: 0.1996 - learning_rate: 3.0000e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss improved from 0.19957 to 0.18302, saving model to outputs/step3_1_feature_test\\face_pitch_face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 6s - 35ms/step - accuracy: 0.9029 - loss: 0.3616 - val_accuracy: 0.9530 - val_loss: 0.1830 - learning_rate: 3.0000e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.18302\n",
      "178/178 - 7s - 38ms/step - accuracy: 0.9031 - loss: 0.3646 - val_accuracy: 0.9398 - val_loss: 0.2056 - learning_rate: 3.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 29.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.6109 ---\n",
      "\n",
      "===== 특징 조합 테스트: face_pitch, face_inner_eyes =====\n",
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "특징 추출(병렬): 100%|██████████| 1111/1111 [01:00<00:00, 18.47it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "청크 병합: 100%|██████████| 1111/1111 [00:14<00:00, 77.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터가 11356개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 1.43762, saving model to outputs/step3_1_feature_test\\face_pitch_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 6s - 35ms/step - accuracy: 0.2934 - loss: 2.1551 - val_accuracy: 0.5511 - val_loss: 1.4376 - learning_rate: 3.0000e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 1.43762 to 0.99899, saving model to outputs/step3_1_feature_test\\face_pitch_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.4970 - loss: 1.5542 - val_accuracy: 0.7162 - val_loss: 0.9990 - learning_rate: 3.0000e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 0.99899 to 0.75677, saving model to outputs/step3_1_feature_test\\face_pitch_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.6038 - loss: 1.2392 - val_accuracy: 0.7686 - val_loss: 0.7568 - learning_rate: 3.0000e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 0.75677 to 0.62291, saving model to outputs/step3_1_feature_test\\face_pitch_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.6849 - loss: 1.0087 - val_accuracy: 0.8123 - val_loss: 0.6229 - learning_rate: 3.0000e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 0.62291 to 0.54454, saving model to outputs/step3_1_feature_test\\face_pitch_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.7195 - loss: 0.9164 - val_accuracy: 0.8325 - val_loss: 0.5445 - learning_rate: 3.0000e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 0.54454 to 0.48053, saving model to outputs/step3_1_feature_test\\face_pitch_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.7522 - loss: 0.8194 - val_accuracy: 0.8553 - val_loss: 0.4805 - learning_rate: 3.0000e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.48053\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.7644 - loss: 0.7808 - val_accuracy: 0.8400 - val_loss: 0.5198 - learning_rate: 3.0000e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.48053\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.7944 - loss: 0.7164 - val_accuracy: 0.8404 - val_loss: 0.4920 - learning_rate: 3.0000e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.48053 to 0.42621, saving model to outputs/step3_1_feature_test\\face_pitch_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.8001 - loss: 0.6814 - val_accuracy: 0.8728 - val_loss: 0.4262 - learning_rate: 3.0000e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 0.42621 to 0.36699, saving model to outputs/step3_1_feature_test\\face_pitch_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.8145 - loss: 0.6348 - val_accuracy: 0.8952 - val_loss: 0.3670 - learning_rate: 3.0000e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.36699\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.8223 - loss: 0.6178 - val_accuracy: 0.8812 - val_loss: 0.4086 - learning_rate: 3.0000e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 0.36699 to 0.35998, saving model to outputs/step3_1_feature_test\\face_pitch_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.8344 - loss: 0.5745 - val_accuracy: 0.9008 - val_loss: 0.3600 - learning_rate: 3.0000e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 0.35998 to 0.32160, saving model to outputs/step3_1_feature_test\\face_pitch_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.8355 - loss: 0.5817 - val_accuracy: 0.9133 - val_loss: 0.3216 - learning_rate: 3.0000e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 0.32160 to 0.29140, saving model to outputs/step3_1_feature_test\\face_pitch_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.8501 - loss: 0.5330 - val_accuracy: 0.9213 - val_loss: 0.2914 - learning_rate: 3.0000e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss improved from 0.29140 to 0.29032, saving model to outputs/step3_1_feature_test\\face_pitch_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.8519 - loss: 0.5232 - val_accuracy: 0.9214 - val_loss: 0.2903 - learning_rate: 3.0000e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss improved from 0.29032 to 0.27928, saving model to outputs/step3_1_feature_test\\face_pitch_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.8564 - loss: 0.5136 - val_accuracy: 0.9273 - val_loss: 0.2793 - learning_rate: 3.0000e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 0.27928 to 0.25819, saving model to outputs/step3_1_feature_test\\face_pitch_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.8684 - loss: 0.4856 - val_accuracy: 0.9335 - val_loss: 0.2582 - learning_rate: 3.0000e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss improved from 0.25819 to 0.24525, saving model to outputs/step3_1_feature_test\\face_pitch_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.8655 - loss: 0.4861 - val_accuracy: 0.9364 - val_loss: 0.2453 - learning_rate: 3.0000e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss improved from 0.24525 to 0.23285, saving model to outputs/step3_1_feature_test\\face_pitch_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.8751 - loss: 0.4507 - val_accuracy: 0.9416 - val_loss: 0.2329 - learning_rate: 3.0000e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.23285\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.8749 - loss: 0.4460 - val_accuracy: 0.9365 - val_loss: 0.2460 - learning_rate: 3.0000e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.23285\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.8814 - loss: 0.4367 - val_accuracy: 0.9377 - val_loss: 0.2400 - learning_rate: 3.0000e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.23285\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.8789 - loss: 0.4370 - val_accuracy: 0.9314 - val_loss: 0.2509 - learning_rate: 3.0000e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss improved from 0.23285 to 0.22674, saving model to outputs/step3_1_feature_test\\face_pitch_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.8867 - loss: 0.4180 - val_accuracy: 0.9419 - val_loss: 0.2267 - learning_rate: 3.0000e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss improved from 0.22674 to 0.21642, saving model to outputs/step3_1_feature_test\\face_pitch_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.8884 - loss: 0.4024 - val_accuracy: 0.9442 - val_loss: 0.2164 - learning_rate: 3.0000e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss improved from 0.21642 to 0.20173, saving model to outputs/step3_1_feature_test\\face_pitch_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.8907 - loss: 0.4002 - val_accuracy: 0.9497 - val_loss: 0.2017 - learning_rate: 3.0000e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss improved from 0.20173 to 0.19754, saving model to outputs/step3_1_feature_test\\face_pitch_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.8968 - loss: 0.3831 - val_accuracy: 0.9518 - val_loss: 0.1975 - learning_rate: 3.0000e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.19754\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.8969 - loss: 0.3771 - val_accuracy: 0.9476 - val_loss: 0.1979 - learning_rate: 3.0000e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss improved from 0.19754 to 0.18527, saving model to outputs/step3_1_feature_test\\face_pitch_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.8982 - loss: 0.3676 - val_accuracy: 0.9555 - val_loss: 0.1853 - learning_rate: 3.0000e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.18527\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.9016 - loss: 0.3637 - val_accuracy: 0.9514 - val_loss: 0.2037 - learning_rate: 3.0000e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss improved from 0.18527 to 0.18492, saving model to outputs/step3_1_feature_test\\face_pitch_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.9037 - loss: 0.3594 - val_accuracy: 0.9584 - val_loss: 0.1849 - learning_rate: 3.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 30.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.5774 ---\n",
      "\n",
      "===== 특징 조합 테스트: face_pitch =====\n",
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "특징 추출(병렬): 100%|██████████| 1111/1111 [00:37<00:00, 29.26it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "청크 병합: 100%|██████████| 1111/1111 [00:12<00:00, 88.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터가 11356개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 1.45459, saving model to outputs/step3_1_feature_test\\face_pitch\\models\\fold_7\\best_model.keras\n",
      "178/178 - 6s - 34ms/step - accuracy: 0.2739 - loss: 2.1839 - val_accuracy: 0.5403 - val_loss: 1.4546 - learning_rate: 3.0000e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 1.45459 to 1.04527, saving model to outputs/step3_1_feature_test\\face_pitch\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.4959 - loss: 1.5624 - val_accuracy: 0.6944 - val_loss: 1.0453 - learning_rate: 3.0000e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 1.04527 to 0.82988, saving model to outputs/step3_1_feature_test\\face_pitch\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.5949 - loss: 1.2790 - val_accuracy: 0.7595 - val_loss: 0.8299 - learning_rate: 3.0000e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 0.82988 to 0.69534, saving model to outputs/step3_1_feature_test\\face_pitch\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.6580 - loss: 1.0853 - val_accuracy: 0.7876 - val_loss: 0.6953 - learning_rate: 3.0000e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 0.69534 to 0.65455, saving model to outputs/step3_1_feature_test\\face_pitch\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.7018 - loss: 0.9673 - val_accuracy: 0.7942 - val_loss: 0.6545 - learning_rate: 3.0000e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 0.65455 to 0.58589, saving model to outputs/step3_1_feature_test\\face_pitch\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.7297 - loss: 0.8904 - val_accuracy: 0.8086 - val_loss: 0.5859 - learning_rate: 3.0000e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.58589 to 0.53626, saving model to outputs/step3_1_feature_test\\face_pitch\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.7523 - loss: 0.8379 - val_accuracy: 0.8457 - val_loss: 0.5363 - learning_rate: 3.0000e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 0.53626 to 0.47517, saving model to outputs/step3_1_feature_test\\face_pitch\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.7740 - loss: 0.7499 - val_accuracy: 0.8562 - val_loss: 0.4752 - learning_rate: 3.0000e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.47517\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.7867 - loss: 0.7093 - val_accuracy: 0.8436 - val_loss: 0.4791 - learning_rate: 3.0000e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 0.47517 to 0.42116, saving model to outputs/step3_1_feature_test\\face_pitch\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 20ms/step - accuracy: 0.8004 - loss: 0.6788 - val_accuracy: 0.8728 - val_loss: 0.4212 - learning_rate: 3.0000e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.42116\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.8164 - loss: 0.6340 - val_accuracy: 0.8713 - val_loss: 0.4258 - learning_rate: 3.0000e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 0.42116 to 0.36670, saving model to outputs/step3_1_feature_test\\face_pitch\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.8204 - loss: 0.6042 - val_accuracy: 0.8910 - val_loss: 0.3667 - learning_rate: 3.0000e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 0.36670 to 0.34258, saving model to outputs/step3_1_feature_test\\face_pitch\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.8337 - loss: 0.5725 - val_accuracy: 0.8982 - val_loss: 0.3426 - learning_rate: 3.0000e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 0.34258 to 0.34096, saving model to outputs/step3_1_feature_test\\face_pitch\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.8396 - loss: 0.5525 - val_accuracy: 0.9015 - val_loss: 0.3410 - learning_rate: 3.0000e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss improved from 0.34096 to 0.30135, saving model to outputs/step3_1_feature_test\\face_pitch\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.8488 - loss: 0.5258 - val_accuracy: 0.9135 - val_loss: 0.3014 - learning_rate: 3.0000e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.30135\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.8502 - loss: 0.5287 - val_accuracy: 0.8909 - val_loss: 0.3669 - learning_rate: 3.0000e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 0.30135 to 0.27953, saving model to outputs/step3_1_feature_test\\face_pitch\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.8551 - loss: 0.5043 - val_accuracy: 0.9243 - val_loss: 0.2795 - learning_rate: 3.0000e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss improved from 0.27953 to 0.26764, saving model to outputs/step3_1_feature_test\\face_pitch\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.8606 - loss: 0.4974 - val_accuracy: 0.9250 - val_loss: 0.2676 - learning_rate: 3.0000e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss improved from 0.26764 to 0.26546, saving model to outputs/step3_1_feature_test\\face_pitch\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 20ms/step - accuracy: 0.8672 - loss: 0.4727 - val_accuracy: 0.9267 - val_loss: 0.2655 - learning_rate: 3.0000e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss improved from 0.26546 to 0.26299, saving model to outputs/step3_1_feature_test\\face_pitch\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.8725 - loss: 0.4529 - val_accuracy: 0.9292 - val_loss: 0.2630 - learning_rate: 3.0000e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss improved from 0.26299 to 0.22796, saving model to outputs/step3_1_feature_test\\face_pitch\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.8752 - loss: 0.4439 - val_accuracy: 0.9410 - val_loss: 0.2280 - learning_rate: 3.0000e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.22796\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.8821 - loss: 0.4333 - val_accuracy: 0.9335 - val_loss: 0.2488 - learning_rate: 3.0000e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.22796\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.8858 - loss: 0.4230 - val_accuracy: 0.9328 - val_loss: 0.2302 - learning_rate: 3.0000e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.22796\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.8789 - loss: 0.4188 - val_accuracy: 0.9353 - val_loss: 0.2398 - learning_rate: 3.0000e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss improved from 0.22796 to 0.21394, saving model to outputs/step3_1_feature_test\\face_pitch\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.8888 - loss: 0.4015 - val_accuracy: 0.9439 - val_loss: 0.2139 - learning_rate: 3.0000e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.21394\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.8912 - loss: 0.3946 - val_accuracy: 0.9383 - val_loss: 0.2260 - learning_rate: 3.0000e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss improved from 0.21394 to 0.20992, saving model to outputs/step3_1_feature_test\\face_pitch\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.8926 - loss: 0.3931 - val_accuracy: 0.9476 - val_loss: 0.2099 - learning_rate: 3.0000e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.20992\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.9004 - loss: 0.3743 - val_accuracy: 0.9457 - val_loss: 0.2104 - learning_rate: 3.0000e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss improved from 0.20992 to 0.20945, saving model to outputs/step3_1_feature_test\\face_pitch\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.9001 - loss: 0.3683 - val_accuracy: 0.9454 - val_loss: 0.2094 - learning_rate: 3.0000e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss improved from 0.20945 to 0.18946, saving model to outputs/step3_1_feature_test\\face_pitch\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.9005 - loss: 0.3638 - val_accuracy: 0.9526 - val_loss: 0.1895 - learning_rate: 3.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 30.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.5975 ---\n",
      "\n",
      "===== 특징 조합 테스트: face_mouth_corners, face_inner_eyes =====\n",
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "특징 추출(병렬): 100%|██████████| 1111/1111 [00:40<00:00, 27.52it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "청크 병합: 100%|██████████| 1111/1111 [00:14<00:00, 78.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터가 11356개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 2.49724, saving model to outputs/step3_1_feature_test\\face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 7s - 38ms/step - accuracy: 0.0829 - loss: 2.6491 - val_accuracy: 0.1265 - val_loss: 2.4972 - learning_rate: 3.0000e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 2.49724 to 2.14861, saving model to outputs/step3_1_feature_test\\face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.1603 - loss: 2.4030 - val_accuracy: 0.2274 - val_loss: 2.1486 - learning_rate: 3.0000e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 2.14861 to 1.91378, saving model to outputs/step3_1_feature_test\\face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.2338 - loss: 2.2006 - val_accuracy: 0.3191 - val_loss: 1.9138 - learning_rate: 3.0000e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 1.91378 to 1.57129, saving model to outputs/step3_1_feature_test\\face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.3236 - loss: 1.9405 - val_accuracy: 0.4630 - val_loss: 1.5713 - learning_rate: 3.0000e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 1.57129 to 1.40166, saving model to outputs/step3_1_feature_test\\face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.3878 - loss: 1.7557 - val_accuracy: 0.5044 - val_loss: 1.4017 - learning_rate: 3.0000e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 1.40166 to 1.36195, saving model to outputs/step3_1_feature_test\\face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.4289 - loss: 1.6407 - val_accuracy: 0.4850 - val_loss: 1.3619 - learning_rate: 3.0000e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 1.36195 to 1.22967, saving model to outputs/step3_1_feature_test\\face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 20ms/step - accuracy: 0.4671 - loss: 1.5360 - val_accuracy: 0.5358 - val_loss: 1.2297 - learning_rate: 3.0000e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 1.22967 to 1.17426, saving model to outputs/step3_1_feature_test\\face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.4960 - loss: 1.4524 - val_accuracy: 0.5583 - val_loss: 1.1743 - learning_rate: 3.0000e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 1.17426 to 1.12009, saving model to outputs/step3_1_feature_test\\face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.5167 - loss: 1.3652 - val_accuracy: 0.5730 - val_loss: 1.1201 - learning_rate: 3.0000e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss did not improve from 1.12009\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.5321 - loss: 1.3274 - val_accuracy: 0.5897 - val_loss: 1.1211 - learning_rate: 3.0000e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 1.12009 to 1.03728, saving model to outputs/step3_1_feature_test\\face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 20ms/step - accuracy: 0.5511 - loss: 1.2854 - val_accuracy: 0.6099 - val_loss: 1.0373 - learning_rate: 3.0000e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 1.03728 to 0.96069, saving model to outputs/step3_1_feature_test\\face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 20ms/step - accuracy: 0.5763 - loss: 1.2192 - val_accuracy: 0.6502 - val_loss: 0.9607 - learning_rate: 3.0000e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 0.96069 to 0.95981, saving model to outputs/step3_1_feature_test\\face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.5881 - loss: 1.1832 - val_accuracy: 0.6454 - val_loss: 0.9598 - learning_rate: 3.0000e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 0.95981 to 0.82227, saving model to outputs/step3_1_feature_test\\face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.6066 - loss: 1.1379 - val_accuracy: 0.7059 - val_loss: 0.8223 - learning_rate: 3.0000e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss improved from 0.82227 to 0.80899, saving model to outputs/step3_1_feature_test\\face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.6170 - loss: 1.1158 - val_accuracy: 0.7050 - val_loss: 0.8090 - learning_rate: 3.0000e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss improved from 0.80899 to 0.78229, saving model to outputs/step3_1_feature_test\\face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.6324 - loss: 1.0701 - val_accuracy: 0.7288 - val_loss: 0.7823 - learning_rate: 3.0000e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 0.78229 to 0.77778, saving model to outputs/step3_1_feature_test\\face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 20ms/step - accuracy: 0.6478 - loss: 1.0339 - val_accuracy: 0.7279 - val_loss: 0.7778 - learning_rate: 3.0000e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss improved from 0.77778 to 0.74055, saving model to outputs/step3_1_feature_test\\face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.6607 - loss: 1.0155 - val_accuracy: 0.7518 - val_loss: 0.7406 - learning_rate: 3.0000e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss improved from 0.74055 to 0.74045, saving model to outputs/step3_1_feature_test\\face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 20ms/step - accuracy: 0.6721 - loss: 0.9827 - val_accuracy: 0.7617 - val_loss: 0.7405 - learning_rate: 3.0000e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss improved from 0.74045 to 0.64934, saving model to outputs/step3_1_feature_test\\face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.6777 - loss: 0.9666 - val_accuracy: 0.7890 - val_loss: 0.6493 - learning_rate: 3.0000e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss improved from 0.64934 to 0.62165, saving model to outputs/step3_1_feature_test\\face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 20ms/step - accuracy: 0.6899 - loss: 0.9282 - val_accuracy: 0.7913 - val_loss: 0.6217 - learning_rate: 3.0000e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss improved from 0.62165 to 0.60249, saving model to outputs/step3_1_feature_test\\face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.6964 - loss: 0.9163 - val_accuracy: 0.7967 - val_loss: 0.6025 - learning_rate: 3.0000e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.60249\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.7095 - loss: 0.8958 - val_accuracy: 0.7987 - val_loss: 0.6032 - learning_rate: 3.0000e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss improved from 0.60249 to 0.57012, saving model to outputs/step3_1_feature_test\\face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.7142 - loss: 0.8779 - val_accuracy: 0.8075 - val_loss: 0.5701 - learning_rate: 3.0000e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss improved from 0.57012 to 0.53479, saving model to outputs/step3_1_feature_test\\face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.7176 - loss: 0.8672 - val_accuracy: 0.8322 - val_loss: 0.5348 - learning_rate: 3.0000e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.53479\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.7227 - loss: 0.8387 - val_accuracy: 0.8020 - val_loss: 0.5969 - learning_rate: 3.0000e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.53479\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.7363 - loss: 0.8180 - val_accuracy: 0.8211 - val_loss: 0.5395 - learning_rate: 3.0000e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss improved from 0.53479 to 0.53435, saving model to outputs/step3_1_feature_test\\face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 20ms/step - accuracy: 0.7405 - loss: 0.8024 - val_accuracy: 0.8255 - val_loss: 0.5344 - learning_rate: 3.0000e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.53435\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.7364 - loss: 0.8070 - val_accuracy: 0.8163 - val_loss: 0.5470 - learning_rate: 3.0000e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss improved from 0.53435 to 0.50804, saving model to outputs/step3_1_feature_test\\face_mouth_corners_face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 20ms/step - accuracy: 0.7455 - loss: 0.7833 - val_accuracy: 0.8354 - val_loss: 0.5080 - learning_rate: 3.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 30.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.5564 ---\n",
      "\n",
      "===== 특징 조합 테스트: face_mouth_corners =====\n",
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "특징 추출(병렬): 100%|██████████| 1111/1111 [00:37<00:00, 29.43it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "청크 병합: 100%|██████████| 1111/1111 [00:12<00:00, 86.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터가 11356개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 2.42509, saving model to outputs/step3_1_feature_test\\face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 6s - 36ms/step - accuracy: 0.0923 - loss: 2.6411 - val_accuracy: 0.2166 - val_loss: 2.4251 - learning_rate: 3.0000e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 2.42509 to 2.00288, saving model to outputs/step3_1_feature_test\\face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.1854 - loss: 2.3614 - val_accuracy: 0.2803 - val_loss: 2.0029 - learning_rate: 3.0000e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 2.00288 to 1.66939, saving model to outputs/step3_1_feature_test\\face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.2874 - loss: 2.0427 - val_accuracy: 0.4198 - val_loss: 1.6694 - learning_rate: 3.0000e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 1.66939 to 1.50083, saving model to outputs/step3_1_feature_test\\face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.3720 - loss: 1.8039 - val_accuracy: 0.4996 - val_loss: 1.5008 - learning_rate: 3.0000e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 1.50083 to 1.21837, saving model to outputs/step3_1_feature_test\\face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.4229 - loss: 1.6479 - val_accuracy: 0.5825 - val_loss: 1.2184 - learning_rate: 3.0000e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 1.21837 to 1.11537, saving model to outputs/step3_1_feature_test\\face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.4788 - loss: 1.5092 - val_accuracy: 0.6373 - val_loss: 1.1154 - learning_rate: 3.0000e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 1.11537 to 1.11126, saving model to outputs/step3_1_feature_test\\face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.5078 - loss: 1.4148 - val_accuracy: 0.6161 - val_loss: 1.1113 - learning_rate: 3.0000e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 1.11126 to 0.97008, saving model to outputs/step3_1_feature_test\\face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.5332 - loss: 1.3347 - val_accuracy: 0.6706 - val_loss: 0.9701 - learning_rate: 3.0000e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.97008 to 0.93805, saving model to outputs/step3_1_feature_test\\face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.5535 - loss: 1.2838 - val_accuracy: 0.6713 - val_loss: 0.9381 - learning_rate: 3.0000e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 0.93805 to 0.91487, saving model to outputs/step3_1_feature_test\\face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.5836 - loss: 1.2202 - val_accuracy: 0.6948 - val_loss: 0.9149 - learning_rate: 3.0000e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 0.91487 to 0.81477, saving model to outputs/step3_1_feature_test\\face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.5968 - loss: 1.1644 - val_accuracy: 0.7286 - val_loss: 0.8148 - learning_rate: 3.0000e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 0.81477 to 0.77932, saving model to outputs/step3_1_feature_test\\face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 20ms/step - accuracy: 0.6169 - loss: 1.1338 - val_accuracy: 0.7550 - val_loss: 0.7793 - learning_rate: 3.0000e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 0.77932 to 0.71296, saving model to outputs/step3_1_feature_test\\face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 20ms/step - accuracy: 0.6331 - loss: 1.0872 - val_accuracy: 0.7686 - val_loss: 0.7130 - learning_rate: 3.0000e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.71296\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.6426 - loss: 1.0538 - val_accuracy: 0.7589 - val_loss: 0.7742 - learning_rate: 3.0000e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss improved from 0.71296 to 0.67656, saving model to outputs/step3_1_feature_test\\face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.6510 - loss: 1.0302 - val_accuracy: 0.7710 - val_loss: 0.6766 - learning_rate: 3.0000e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss improved from 0.67656 to 0.65778, saving model to outputs/step3_1_feature_test\\face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 20ms/step - accuracy: 0.6616 - loss: 0.9955 - val_accuracy: 0.7798 - val_loss: 0.6578 - learning_rate: 3.0000e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.65778\n",
      "178/178 - 3s - 20ms/step - accuracy: 0.6801 - loss: 0.9543 - val_accuracy: 0.7584 - val_loss: 0.7061 - learning_rate: 3.0000e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss improved from 0.65778 to 0.65260, saving model to outputs/step3_1_feature_test\\face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.6866 - loss: 0.9494 - val_accuracy: 0.7921 - val_loss: 0.6526 - learning_rate: 3.0000e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.65260\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.6890 - loss: 0.9454 - val_accuracy: 0.7625 - val_loss: 0.6760 - learning_rate: 3.0000e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss improved from 0.65260 to 0.58260, saving model to outputs/step3_1_feature_test\\face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.7039 - loss: 0.8952 - val_accuracy: 0.8099 - val_loss: 0.5826 - learning_rate: 3.0000e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.58260\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.7076 - loss: 0.8877 - val_accuracy: 0.8074 - val_loss: 0.5915 - learning_rate: 3.0000e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss improved from 0.58260 to 0.54203, saving model to outputs/step3_1_feature_test\\face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.7203 - loss: 0.8554 - val_accuracy: 0.8220 - val_loss: 0.5420 - learning_rate: 3.0000e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.54203\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.7214 - loss: 0.8496 - val_accuracy: 0.8304 - val_loss: 0.5435 - learning_rate: 3.0000e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.54203\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.7226 - loss: 0.8539 - val_accuracy: 0.8116 - val_loss: 0.5641 - learning_rate: 3.0000e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.54203\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.7343 - loss: 0.8245 - val_accuracy: 0.8187 - val_loss: 0.5507 - learning_rate: 3.0000e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.54203\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.7408 - loss: 0.8070 - val_accuracy: 0.8002 - val_loss: 0.5824 - learning_rate: 3.0000e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss improved from 0.54203 to 0.53652, saving model to outputs/step3_1_feature_test\\face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 20ms/step - accuracy: 0.7371 - loss: 0.7917 - val_accuracy: 0.8208 - val_loss: 0.5365 - learning_rate: 3.0000e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss improved from 0.53652 to 0.50194, saving model to outputs/step3_1_feature_test\\face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.7469 - loss: 0.7832 - val_accuracy: 0.8292 - val_loss: 0.5019 - learning_rate: 3.0000e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss improved from 0.50194 to 0.49393, saving model to outputs/step3_1_feature_test\\face_mouth_corners\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.7545 - loss: 0.7672 - val_accuracy: 0.8422 - val_loss: 0.4939 - learning_rate: 3.0000e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.49393\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.7575 - loss: 0.7569 - val_accuracy: 0.8337 - val_loss: 0.5032 - learning_rate: 3.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 29.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.5272 ---\n",
      "\n",
      "===== 특징 조합 테스트: face_inner_eyes =====\n",
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "특징 추출(병렬): 100%|██████████| 1111/1111 [00:40<00:00, 27.64it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "청크 병합: 100%|██████████| 1111/1111 [00:13<00:00, 80.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터가 11356개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 2.54547, saving model to outputs/step3_1_feature_test\\face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 8s - 44ms/step - accuracy: 0.0854 - loss: 2.6488 - val_accuracy: 0.1545 - val_loss: 2.5455 - learning_rate: 3.0000e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 2.54547 to 2.08249, saving model to outputs/step3_1_feature_test\\face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 22ms/step - accuracy: 0.1678 - loss: 2.4055 - val_accuracy: 0.3202 - val_loss: 2.0825 - learning_rate: 3.0000e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 2.08249 to 1.80444, saving model to outputs/step3_1_feature_test\\face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.2692 - loss: 2.0953 - val_accuracy: 0.3437 - val_loss: 1.8044 - learning_rate: 3.0000e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 1.80444 to 1.47516, saving model to outputs/step3_1_feature_test\\face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.3451 - loss: 1.8511 - val_accuracy: 0.4987 - val_loss: 1.4752 - learning_rate: 3.0000e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 1.47516 to 1.32683, saving model to outputs/step3_1_feature_test\\face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 20ms/step - accuracy: 0.4023 - loss: 1.6960 - val_accuracy: 0.5420 - val_loss: 1.3268 - learning_rate: 3.0000e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 1.32683 to 1.23965, saving model to outputs/step3_1_feature_test\\face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.4598 - loss: 1.5550 - val_accuracy: 0.5721 - val_loss: 1.2396 - learning_rate: 3.0000e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 1.23965 to 1.11407, saving model to outputs/step3_1_feature_test\\face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.4861 - loss: 1.4881 - val_accuracy: 0.6295 - val_loss: 1.1141 - learning_rate: 3.0000e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 1.11407 to 1.05496, saving model to outputs/step3_1_feature_test\\face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.5287 - loss: 1.3555 - val_accuracy: 0.6328 - val_loss: 1.0550 - learning_rate: 3.0000e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 1.05496 to 0.98629, saving model to outputs/step3_1_feature_test\\face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.5531 - loss: 1.2945 - val_accuracy: 0.6511 - val_loss: 0.9863 - learning_rate: 3.0000e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 0.98629 to 0.92763, saving model to outputs/step3_1_feature_test\\face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 20ms/step - accuracy: 0.5712 - loss: 1.2484 - val_accuracy: 0.6779 - val_loss: 0.9276 - learning_rate: 3.0000e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 0.92763 to 0.86605, saving model to outputs/step3_1_feature_test\\face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 20ms/step - accuracy: 0.5915 - loss: 1.1699 - val_accuracy: 0.6966 - val_loss: 0.8661 - learning_rate: 3.0000e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 0.86605 to 0.81714, saving model to outputs/step3_1_feature_test\\face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.6016 - loss: 1.1586 - val_accuracy: 0.7327 - val_loss: 0.8171 - learning_rate: 3.0000e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.81714\n",
      "178/178 - 3s - 20ms/step - accuracy: 0.6152 - loss: 1.1197 - val_accuracy: 0.6825 - val_loss: 0.8390 - learning_rate: 3.0000e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 0.81714 to 0.79081, saving model to outputs/step3_1_feature_test\\face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.6272 - loss: 1.0825 - val_accuracy: 0.7337 - val_loss: 0.7908 - learning_rate: 3.0000e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss improved from 0.79081 to 0.75829, saving model to outputs/step3_1_feature_test\\face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.6441 - loss: 1.0498 - val_accuracy: 0.7379 - val_loss: 0.7583 - learning_rate: 3.0000e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss improved from 0.75829 to 0.70844, saving model to outputs/step3_1_feature_test\\face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.6540 - loss: 1.0238 - val_accuracy: 0.7711 - val_loss: 0.7084 - learning_rate: 3.0000e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.70844\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.6656 - loss: 0.9997 - val_accuracy: 0.7503 - val_loss: 0.7272 - learning_rate: 3.0000e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss improved from 0.70844 to 0.70163, saving model to outputs/step3_1_feature_test\\face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.6765 - loss: 0.9662 - val_accuracy: 0.7662 - val_loss: 0.7016 - learning_rate: 3.0000e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss improved from 0.70163 to 0.65937, saving model to outputs/step3_1_feature_test\\face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.6820 - loss: 0.9499 - val_accuracy: 0.7881 - val_loss: 0.6594 - learning_rate: 3.0000e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss improved from 0.65937 to 0.62489, saving model to outputs/step3_1_feature_test\\face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.6823 - loss: 0.9446 - val_accuracy: 0.7839 - val_loss: 0.6249 - learning_rate: 3.0000e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.62489\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.6878 - loss: 0.9284 - val_accuracy: 0.7707 - val_loss: 0.6737 - learning_rate: 3.0000e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss improved from 0.62489 to 0.61108, saving model to outputs/step3_1_feature_test\\face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.6948 - loss: 0.8978 - val_accuracy: 0.7939 - val_loss: 0.6111 - learning_rate: 3.0000e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.61108\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.7006 - loss: 0.8866 - val_accuracy: 0.7943 - val_loss: 0.6245 - learning_rate: 3.0000e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss improved from 0.61108 to 0.57772, saving model to outputs/step3_1_feature_test\\face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.7093 - loss: 0.8630 - val_accuracy: 0.8006 - val_loss: 0.5777 - learning_rate: 3.0000e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss improved from 0.57772 to 0.57191, saving model to outputs/step3_1_feature_test\\face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.7184 - loss: 0.8621 - val_accuracy: 0.8023 - val_loss: 0.5719 - learning_rate: 3.0000e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.57191\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.7237 - loss: 0.8331 - val_accuracy: 0.7698 - val_loss: 0.6308 - learning_rate: 3.0000e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss improved from 0.57191 to 0.54309, saving model to outputs/step3_1_feature_test\\face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.7351 - loss: 0.8116 - val_accuracy: 0.8110 - val_loss: 0.5431 - learning_rate: 3.0000e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.54309\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.7283 - loss: 0.8280 - val_accuracy: 0.8071 - val_loss: 0.5664 - learning_rate: 3.0000e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss improved from 0.54309 to 0.52768, saving model to outputs/step3_1_feature_test\\face_inner_eyes\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.7341 - loss: 0.8151 - val_accuracy: 0.8151 - val_loss: 0.5277 - learning_rate: 3.0000e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.52768\n",
      "178/178 - 3s - 20ms/step - accuracy: 0.7347 - loss: 0.7986 - val_accuracy: 0.8211 - val_loss: 0.5282 - learning_rate: 3.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 29.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.4759 ---\n",
      "\n",
      "===== 특징 조합 테스트: Hand Only =====\n",
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "특징 추출(병렬): 100%|██████████| 1111/1111 [00:37<00:00, 29.49it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "청크 병합: 100%|██████████| 1111/1111 [00:13<00:00, 80.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터가 11356개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 2.59708, saving model to outputs/step3_1_feature_test\\Hand Only\\models\\fold_7\\best_model.keras\n",
      "178/178 - 6s - 32ms/step - accuracy: 0.0805 - loss: 2.6682 - val_accuracy: 0.1506 - val_loss: 2.5971 - learning_rate: 3.0000e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 2.59708 to 2.31051, saving model to outputs/step3_1_feature_test\\Hand Only\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 18ms/step - accuracy: 0.1403 - loss: 2.5035 - val_accuracy: 0.1940 - val_loss: 2.3105 - learning_rate: 3.0000e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 2.31051 to 1.76612, saving model to outputs/step3_1_feature_test\\Hand Only\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.2560 - loss: 2.1761 - val_accuracy: 0.4052 - val_loss: 1.7661 - learning_rate: 3.0000e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 1.76612 to 1.59174, saving model to outputs/step3_1_feature_test\\Hand Only\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.3207 - loss: 1.9637 - val_accuracy: 0.4447 - val_loss: 1.5917 - learning_rate: 3.0000e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 1.59174 to 1.45195, saving model to outputs/step3_1_feature_test\\Hand Only\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.3746 - loss: 1.8099 - val_accuracy: 0.4845 - val_loss: 1.4519 - learning_rate: 3.0000e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 1.45195 to 1.42651, saving model to outputs/step3_1_feature_test\\Hand Only\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.4197 - loss: 1.6650 - val_accuracy: 0.4767 - val_loss: 1.4265 - learning_rate: 3.0000e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 1.42651 to 1.22837, saving model to outputs/step3_1_feature_test\\Hand Only\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.4464 - loss: 1.5826 - val_accuracy: 0.5890 - val_loss: 1.2284 - learning_rate: 3.0000e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 1.22837 to 1.17835, saving model to outputs/step3_1_feature_test\\Hand Only\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.4616 - loss: 1.5195 - val_accuracy: 0.6019 - val_loss: 1.1784 - learning_rate: 3.0000e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 1.17835 to 1.08281, saving model to outputs/step3_1_feature_test\\Hand Only\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.4954 - loss: 1.4285 - val_accuracy: 0.6159 - val_loss: 1.0828 - learning_rate: 3.0000e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 1.08281 to 1.06613, saving model to outputs/step3_1_feature_test\\Hand Only\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.5055 - loss: 1.3769 - val_accuracy: 0.6191 - val_loss: 1.0661 - learning_rate: 3.0000e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 1.06613 to 0.99427, saving model to outputs/step3_1_feature_test\\Hand Only\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.5244 - loss: 1.3203 - val_accuracy: 0.6487 - val_loss: 0.9943 - learning_rate: 3.0000e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 0.99427 to 0.98663, saving model to outputs/step3_1_feature_test\\Hand Only\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.5436 - loss: 1.2863 - val_accuracy: 0.6448 - val_loss: 0.9866 - learning_rate: 3.0000e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 0.98663 to 0.94596, saving model to outputs/step3_1_feature_test\\Hand Only\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 20ms/step - accuracy: 0.5527 - loss: 1.2591 - val_accuracy: 0.6725 - val_loss: 0.9460 - learning_rate: 3.0000e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 0.94596 to 0.93042, saving model to outputs/step3_1_feature_test\\Hand Only\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.5603 - loss: 1.2424 - val_accuracy: 0.6774 - val_loss: 0.9304 - learning_rate: 3.0000e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss improved from 0.93042 to 0.88091, saving model to outputs/step3_1_feature_test\\Hand Only\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.5727 - loss: 1.2018 - val_accuracy: 0.7044 - val_loss: 0.8809 - learning_rate: 3.0000e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss improved from 0.88091 to 0.86725, saving model to outputs/step3_1_feature_test\\Hand Only\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.5889 - loss: 1.1744 - val_accuracy: 0.6999 - val_loss: 0.8672 - learning_rate: 3.0000e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 0.86725 to 0.84622, saving model to outputs/step3_1_feature_test\\Hand Only\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.5920 - loss: 1.1546 - val_accuracy: 0.7035 - val_loss: 0.8462 - learning_rate: 3.0000e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.84622\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.6021 - loss: 1.1432 - val_accuracy: 0.7153 - val_loss: 0.8512 - learning_rate: 3.0000e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.84622\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.6163 - loss: 1.1039 - val_accuracy: 0.7084 - val_loss: 0.8525 - learning_rate: 3.0000e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss improved from 0.84622 to 0.78552, saving model to outputs/step3_1_feature_test\\Hand Only\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.6280 - loss: 1.0753 - val_accuracy: 0.7390 - val_loss: 0.7855 - learning_rate: 3.0000e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.78552\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.6337 - loss: 1.0643 - val_accuracy: 0.7259 - val_loss: 0.7951 - learning_rate: 3.0000e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss improved from 0.78552 to 0.77302, saving model to outputs/step3_1_feature_test\\Hand Only\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.6450 - loss: 1.0396 - val_accuracy: 0.7375 - val_loss: 0.7730 - learning_rate: 3.0000e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss improved from 0.77302 to 0.73251, saving model to outputs/step3_1_feature_test\\Hand Only\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.6509 - loss: 1.0234 - val_accuracy: 0.7574 - val_loss: 0.7325 - learning_rate: 3.0000e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.73251\n",
      "178/178 - 3s - 18ms/step - accuracy: 0.6570 - loss: 1.0092 - val_accuracy: 0.7541 - val_loss: 0.7331 - learning_rate: 3.0000e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.73251\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.6660 - loss: 0.9811 - val_accuracy: 0.7467 - val_loss: 0.7607 - learning_rate: 3.0000e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss improved from 0.73251 to 0.72100, saving model to outputs/step3_1_feature_test\\Hand Only\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.6663 - loss: 0.9862 - val_accuracy: 0.7692 - val_loss: 0.7210 - learning_rate: 3.0000e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss improved from 0.72100 to 0.67735, saving model to outputs/step3_1_feature_test\\Hand Only\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.6715 - loss: 0.9825 - val_accuracy: 0.7795 - val_loss: 0.6773 - learning_rate: 3.0000e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss improved from 0.67735 to 0.67636, saving model to outputs/step3_1_feature_test\\Hand Only\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.6803 - loss: 0.9503 - val_accuracy: 0.7822 - val_loss: 0.6764 - learning_rate: 3.0000e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.67636\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.6839 - loss: 0.9367 - val_accuracy: 0.7716 - val_loss: 0.6891 - learning_rate: 3.0000e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss improved from 0.67636 to 0.65368, saving model to outputs/step3_1_feature_test\\Hand Only\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.6838 - loss: 0.9373 - val_accuracy: 0.7750 - val_loss: 0.6537 - learning_rate: 3.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 30.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "특징 조합 탐색 진행률: 100%|██████████| 16/16 [58:44<00:00, 220.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.5608 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "최적 특징 조합 (고속 테스트 기준): face_roll_scale, face_pitch, face_mouth_corners (F1: 0.7086)\n",
      "{'hand_all': True, 'face_roll_scale': True, 'face_pitch': True, 'face_mouth_corners': True, 'face_inner_eyes': False}\n"
     ]
    }
   ],
   "source": [
    "# === tqdm: 텍스트 기반으로 고정 (ipywidgets 불필요) ===\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except Exception:\n",
    "    def tqdm(iterable=None, **kwargs):\n",
    "        return iterable\n",
    "\n",
    "# === Fold 유틸 ===\n",
    "def norm_fold_id(fid):\n",
    "    \"\"\"정수/문자 어떤 입력이 와도 'fold_X'로 통일.\"\"\"\n",
    "    s = str(fid)\n",
    "    return s if s.startswith(\"fold_\") else f\"fold_{int(s)}\"\n",
    "\n",
    "def fold_as_int(fid):\n",
    "    \"\"\"'fold_7' -> 7, 7 -> 7\"\"\"\n",
    "    return int(str(fid).split('_')[-1])\n",
    "\n",
    "# (이전 셀에서 tuning_fold_id가 정의되어 있다고 가정)\n",
    "tuning_fold_id = norm_fold_id(tuning_fold_id)   # ex) 'fold_7'\n",
    "tuning_fold_int = fold_as_int(tuning_fold_id)   # ex) 7\n",
    "\n",
    "best_architecture = 'cnn_gru'\n",
    "print(f\"아키텍처를 '{best_architecture}'로 고정하여 테스트를 진행합니다.\")\n",
    "\n",
    "# === 특징 조합 설정 ===\n",
    "face_features = ['face_roll_scale', 'face_pitch', 'face_mouth_corners', 'face_inner_eyes']\n",
    "feature_combinations = list(itertools.product([True, False], repeat=len(face_features)))\n",
    "feature_test_output_dir = 'outputs/step3_1_feature_test'\n",
    "results_feature = {}\n",
    "\n",
    "# === 경로/라벨 유틸 ===\n",
    "def combo_label_from_flags(flags):\n",
    "    \"\"\"True인 특징만 라벨에 표시. 모두 False면 'Hand Only'.\"\"\"\n",
    "    names = [name for name, enabled in zip(face_features, flags) if enabled]\n",
    "    return \", \".join(names) if names else \"Hand Only\"\n",
    "\n",
    "def safe_dirname(label):\n",
    "    \"\"\"라벨을 디렉터리명으로 안전하게 변환.\"\"\"\n",
    "    return label.replace(\", \", \"_\").replace(\"/\", \"-\")\n",
    "\n",
    "def report_path_for_combo(base_dir, label, fold_id):\n",
    "    return os.path.join(base_dir, safe_dirname(label), 'models', fold_id, 'classification_report.csv')\n",
    "\n",
    "# === 1) 모든 특징 조합 결과 존재 여부 확인 ===\n",
    "all_results_exist = True\n",
    "for combo in feature_combinations:\n",
    "    label = combo_label_from_flags(combo)\n",
    "    rp = report_path_for_combo(feature_test_output_dir, label, tuning_fold_id)\n",
    "    if not os.path.exists(rp):\n",
    "        all_results_exist = False\n",
    "        break\n",
    "\n",
    "# === 2) 결과 로드 또는 새로 실행 ===\n",
    "if all_results_exist:\n",
    "    print(f\"✅ 기존 특징 조합 테스트 결과('{feature_test_output_dir}')를 찾았습니다. 결과를 재사용합니다.\")\n",
    "    for combo in tqdm(feature_combinations, desc=\"기존 결과 로드 중\"):\n",
    "        label = combo_label_from_flags(combo)\n",
    "        rp = report_path_for_combo(feature_test_output_dir, label, tuning_fold_id)\n",
    "        report_df = pd.read_csv(rp, index_col=0)\n",
    "        results_feature[label] = float(report_df.loc['macro avg', 'f1-score'])\n",
    "else:\n",
    "    print(\"기존 특징 조합 테스트 결과가 없거나 불완전합니다. 새로 테스트를 시작합니다.\")\n",
    "    for combo in tqdm(feature_combinations, desc=\"특징 조합 탐색 진행률\"):\n",
    "        feature_dict = dict(zip(face_features, combo))\n",
    "        feature_dict['hand_all'] = True\n",
    "\n",
    "        label = combo_label_from_flags(combo)\n",
    "        print(f\"\\n===== 특징 조합 테스트: {label} =====\")\n",
    "\n",
    "        test_cfg = copy.deepcopy(config.MANUAL_CONFIG)\n",
    "        test_cfg['paths']['OUTPUT_DIR'] = os.path.join(feature_test_output_dir, safe_dirname(label))\n",
    "        test_cfg['model_arch']['variant'] = best_architecture\n",
    "        test_cfg['feature_extraction']['FEATURE_CONFIG'] = feature_dict\n",
    "\n",
    "        # 고속 테스트 세팅\n",
    "        test_cfg['training']['epochs'] = 30\n",
    "        test_cfg['training']['early_stopping_patience'] = 5\n",
    "\n",
    "        # run_single_fold는 정수 fold 사용\n",
    "        f1 = run_single_fold(test_cfg, fold_id=tuning_fold_int, data_subset_fraction=0.3)\n",
    "        results_feature[label] = f1\n",
    "\n",
    "# === 3) 시각화 및 최적 조합 ===\n",
    "if not results_feature:\n",
    "    raise RuntimeError(\"특징 조합 결과가 비어 있습니다. 상위 단계 실행을 확인하세요.\")\n",
    "\n",
    "results_df = (\n",
    "    pd.DataFrame(list(results_feature.items()), columns=['Feature Combo', 'F1 Score'])\n",
    "    .sort_values('F1 Score', ascending=False)\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='F1 Score', y='Feature Combo', data=results_df, orient='h',\n",
    "            order=results_df['Feature Combo'])\n",
    "plt.title(f'랜드마크 특징 조합별 성능 비교 ({tuning_fold_id}, 고속 테스트)')\n",
    "plt.xlabel('Macro F1 Score')\n",
    "plt.ylabel('Feature Combination')\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "best_feature_combo_name = results_df.iloc[0]['Feature Combo']\n",
    "selected_features = [] if best_feature_combo_name == \"Hand Only\" else best_feature_combo_name.split(\", \")\n",
    "best_feature_config = {'hand_all': True, **{f: (f in selected_features) for f in face_features}}\n",
    "\n",
    "print(f\"\\n최적 특징 조합 (고속 테스트 기준): {best_feature_combo_name} \"\n",
    "      f\"(F1: {results_df.iloc[0]['F1 Score']:.4f})\")\n",
    "print(best_feature_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87419343",
   "metadata": {},
   "source": [
    "# # 시계열 특징(`TEMPORAL_FEATURES`) 조합\n",
    "\n",
    "⏱️ 시계열 특징 조합 탐색 (단일 Fold, 고속 테스트)\n",
    "\n",
    "기준 폴드({tuning_fold_id})와 고정 아키텍처({best_architecture})에서\n",
    "velocity, acceleration, segment_stats의 사용/미사용 조합을 전수 탐색하여 Macro F1을 비교합니다.\n",
    "모든 시계열 플래그가 False면 Position Only로 간주합니다.\n",
    "\n",
    "🔧 절차\n",
    "\n",
    "조합 생성: itertools.product로 2³=8개 조합 생성\n",
    "\n",
    "캐시 확인: 각 조합의 classification_report.csv가 모두 존재하면 재사용, 아니면 재학습\n",
    "\n",
    "학습(고속): epochs=30, early_stopping_patience=5, data_subset_fraction=0.3\n",
    "\n",
    "정리/시각화: 수평 막대그래프로 성능 비교 → 최고 조합/설정 출력\n",
    "\n",
    "📂 결과 경로 규칙\n",
    "outputs/step3_2_temporal_test/<Temporal_Combo_Dir>/models/{tuning_fold_id}/classification_report.csv\n",
    "\n",
    "\n",
    "<Temporal_Combo_Dir>: 라벨의 “, ”를 “_”로 치환 (예: velocity_segment_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a496591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기존 시계열 특징 테스트 결과가 없거나 불완전합니다. 새로 테스트를 시작합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== 시계열 특징 조합 테스트: velocity, acceleration, segment_stats =====\n",
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n",
      "학습 데이터가 11356개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 1.43163, saving model to outputs/step3_2_temporal_test\\velocity_acceleration_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 6s - 32ms/step - accuracy: 0.2891 - loss: 2.1980 - val_accuracy: 0.5412 - val_loss: 1.4316 - learning_rate: 3.0000e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 1.43163 to 0.82791, saving model to outputs/step3_2_temporal_test\\velocity_acceleration_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 17ms/step - accuracy: 0.5185 - loss: 1.4920 - val_accuracy: 0.7524 - val_loss: 0.8279 - learning_rate: 3.0000e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 0.82791 to 0.61240, saving model to outputs/step3_2_temporal_test\\velocity_acceleration_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 17ms/step - accuracy: 0.6632 - loss: 1.0863 - val_accuracy: 0.8132 - val_loss: 0.6124 - learning_rate: 3.0000e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 0.61240 to 0.46441, saving model to outputs/step3_2_temporal_test\\velocity_acceleration_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 17ms/step - accuracy: 0.7461 - loss: 0.8421 - val_accuracy: 0.8734 - val_loss: 0.4644 - learning_rate: 3.0000e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 0.46441 to 0.44447, saving model to outputs/step3_2_temporal_test\\velocity_acceleration_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 17ms/step - accuracy: 0.7891 - loss: 0.7288 - val_accuracy: 0.8626 - val_loss: 0.4445 - learning_rate: 3.0000e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 0.44447 to 0.32145, saving model to outputs/step3_2_temporal_test\\velocity_acceleration_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 17ms/step - accuracy: 0.8152 - loss: 0.6325 - val_accuracy: 0.9072 - val_loss: 0.3215 - learning_rate: 3.0000e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.32145 to 0.29778, saving model to outputs/step3_2_temporal_test\\velocity_acceleration_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 17ms/step - accuracy: 0.8460 - loss: 0.5503 - val_accuracy: 0.9108 - val_loss: 0.2978 - learning_rate: 3.0000e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 0.29778 to 0.21543, saving model to outputs/step3_2_temporal_test\\velocity_acceleration_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 18ms/step - accuracy: 0.8595 - loss: 0.5070 - val_accuracy: 0.9446 - val_loss: 0.2154 - learning_rate: 3.0000e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.21543\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.8723 - loss: 0.4631 - val_accuracy: 0.9298 - val_loss: 0.2490 - learning_rate: 3.0000e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 0.21543 to 0.20324, saving model to outputs/step3_2_temporal_test\\velocity_acceleration_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.8825 - loss: 0.4331 - val_accuracy: 0.9455 - val_loss: 0.2032 - learning_rate: 3.0000e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.20324\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.9016 - loss: 0.3780 - val_accuracy: 0.9316 - val_loss: 0.2494 - learning_rate: 3.0000e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 0.20324 to 0.16097, saving model to outputs/step3_2_temporal_test\\velocity_acceleration_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.8993 - loss: 0.3750 - val_accuracy: 0.9630 - val_loss: 0.1610 - learning_rate: 3.0000e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 0.16097 to 0.15921, saving model to outputs/step3_2_temporal_test\\velocity_acceleration_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.9074 - loss: 0.3545 - val_accuracy: 0.9612 - val_loss: 0.1592 - learning_rate: 3.0000e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 0.15921 to 0.14890, saving model to outputs/step3_2_temporal_test\\velocity_acceleration_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.9134 - loss: 0.3343 - val_accuracy: 0.9663 - val_loss: 0.1489 - learning_rate: 3.0000e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.14890\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.9155 - loss: 0.3282 - val_accuracy: 0.9552 - val_loss: 0.1826 - learning_rate: 3.0000e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss improved from 0.14890 to 0.13734, saving model to outputs/step3_2_temporal_test\\velocity_acceleration_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.9255 - loss: 0.3028 - val_accuracy: 0.9710 - val_loss: 0.1373 - learning_rate: 3.0000e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 0.13734 to 0.13029, saving model to outputs/step3_2_temporal_test\\velocity_acceleration_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.9279 - loss: 0.2860 - val_accuracy: 0.9689 - val_loss: 0.1303 - learning_rate: 3.0000e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss improved from 0.13029 to 0.12525, saving model to outputs/step3_2_temporal_test\\velocity_acceleration_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.9289 - loss: 0.2777 - val_accuracy: 0.9722 - val_loss: 0.1253 - learning_rate: 3.0000e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss improved from 0.12525 to 0.12163, saving model to outputs/step3_2_temporal_test\\velocity_acceleration_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.9279 - loss: 0.2900 - val_accuracy: 0.9741 - val_loss: 0.1216 - learning_rate: 3.0000e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.12163\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.9340 - loss: 0.2647 - val_accuracy: 0.9726 - val_loss: 0.1287 - learning_rate: 3.0000e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.12163\n",
      "178/178 - 4s - 24ms/step - accuracy: 0.9406 - loss: 0.2416 - val_accuracy: 0.9644 - val_loss: 0.1427 - learning_rate: 3.0000e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss improved from 0.12163 to 0.11718, saving model to outputs/step3_2_temporal_test\\velocity_acceleration_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.9414 - loss: 0.2454 - val_accuracy: 0.9749 - val_loss: 0.1172 - learning_rate: 3.0000e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.11718\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.9451 - loss: 0.2315 - val_accuracy: 0.9695 - val_loss: 0.1365 - learning_rate: 3.0000e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss improved from 0.11718 to 0.09834, saving model to outputs/step3_2_temporal_test\\velocity_acceleration_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.9473 - loss: 0.2175 - val_accuracy: 0.9802 - val_loss: 0.0983 - learning_rate: 3.0000e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.09834\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.9466 - loss: 0.2268 - val_accuracy: 0.9780 - val_loss: 0.1084 - learning_rate: 3.0000e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.09834\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.9465 - loss: 0.2275 - val_accuracy: 0.9787 - val_loss: 0.1047 - learning_rate: 3.0000e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.09834\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.9482 - loss: 0.2125 - val_accuracy: 0.9732 - val_loss: 0.1240 - learning_rate: 3.0000e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.09834\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.9523 - loss: 0.2102 - val_accuracy: 0.9734 - val_loss: 0.1191 - learning_rate: 3.0000e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss improved from 0.09834 to 0.08677, saving model to outputs/step3_2_temporal_test\\velocity_acceleration_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.9517 - loss: 0.2123 - val_accuracy: 0.9846 - val_loss: 0.0868 - learning_rate: 3.0000e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.08677\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.9563 - loss: 0.1947 - val_accuracy: 0.9807 - val_loss: 0.1013 - learning_rate: 3.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 29.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.7086 ---\n",
      "\n",
      "===== 시계열 특징 조합 테스트: velocity, acceleration =====\n",
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "특징 추출(병렬): 100%|██████████| 1111/1111 [00:33<00:00, 32.80it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "청크 병합: 100%|██████████| 1111/1111 [00:09<00:00, 113.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터가 11356개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 1.44878, saving model to outputs/step3_2_temporal_test\\velocity_acceleration\\models\\fold_7\\best_model.keras\n",
      "178/178 - 6s - 36ms/step - accuracy: 0.2887 - loss: 2.1648 - val_accuracy: 0.5703 - val_loss: 1.4488 - learning_rate: 3.0000e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 1.44878 to 1.05470, saving model to outputs/step3_2_temporal_test\\velocity_acceleration\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.4782 - loss: 1.6044 - val_accuracy: 0.6997 - val_loss: 1.0547 - learning_rate: 3.0000e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 1.05470 to 0.78936, saving model to outputs/step3_2_temporal_test\\velocity_acceleration\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.5986 - loss: 1.2623 - val_accuracy: 0.7690 - val_loss: 0.7894 - learning_rate: 3.0000e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 0.78936 to 0.57092, saving model to outputs/step3_2_temporal_test\\velocity_acceleration\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.6837 - loss: 1.0043 - val_accuracy: 0.8348 - val_loss: 0.5709 - learning_rate: 3.0000e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 0.57092 to 0.48663, saving model to outputs/step3_2_temporal_test\\velocity_acceleration\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.7395 - loss: 0.8530 - val_accuracy: 0.8493 - val_loss: 0.4866 - learning_rate: 3.0000e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 0.48663 to 0.43142, saving model to outputs/step3_2_temporal_test\\velocity_acceleration\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.7792 - loss: 0.7316 - val_accuracy: 0.8747 - val_loss: 0.4314 - learning_rate: 3.0000e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.43142 to 0.36315, saving model to outputs/step3_2_temporal_test\\velocity_acceleration\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 22ms/step - accuracy: 0.8001 - loss: 0.6689 - val_accuracy: 0.8993 - val_loss: 0.3631 - learning_rate: 3.0000e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 0.36315 to 0.34397, saving model to outputs/step3_2_temporal_test\\velocity_acceleration\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 22ms/step - accuracy: 0.8227 - loss: 0.6192 - val_accuracy: 0.9063 - val_loss: 0.3440 - learning_rate: 3.0000e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.34397 to 0.30231, saving model to outputs/step3_2_temporal_test\\velocity_acceleration\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 22ms/step - accuracy: 0.8474 - loss: 0.5384 - val_accuracy: 0.9154 - val_loss: 0.3023 - learning_rate: 3.0000e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 0.30231 to 0.26179, saving model to outputs/step3_2_temporal_test\\velocity_acceleration\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 22ms/step - accuracy: 0.8541 - loss: 0.5111 - val_accuracy: 0.9295 - val_loss: 0.2618 - learning_rate: 3.0000e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.26179\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.8669 - loss: 0.4674 - val_accuracy: 0.9223 - val_loss: 0.2693 - learning_rate: 3.0000e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 0.26179 to 0.23655, saving model to outputs/step3_2_temporal_test\\velocity_acceleration\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.8740 - loss: 0.4556 - val_accuracy: 0.9379 - val_loss: 0.2366 - learning_rate: 3.0000e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 0.23655 to 0.23175, saving model to outputs/step3_2_temporal_test\\velocity_acceleration\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 22ms/step - accuracy: 0.8848 - loss: 0.4254 - val_accuracy: 0.9407 - val_loss: 0.2317 - learning_rate: 3.0000e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 0.23175 to 0.18054, saving model to outputs/step3_2_temporal_test\\velocity_acceleration\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.8910 - loss: 0.3952 - val_accuracy: 0.9548 - val_loss: 0.1805 - learning_rate: 3.0000e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss improved from 0.18054 to 0.17790, saving model to outputs/step3_2_temporal_test\\velocity_acceleration\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 22ms/step - accuracy: 0.8973 - loss: 0.3789 - val_accuracy: 0.9551 - val_loss: 0.1779 - learning_rate: 3.0000e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss improved from 0.17790 to 0.17539, saving model to outputs/step3_2_temporal_test\\velocity_acceleration\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 22ms/step - accuracy: 0.9050 - loss: 0.3553 - val_accuracy: 0.9539 - val_loss: 0.1754 - learning_rate: 3.0000e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 0.17539 to 0.15660, saving model to outputs/step3_2_temporal_test\\velocity_acceleration\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 22ms/step - accuracy: 0.9126 - loss: 0.3424 - val_accuracy: 0.9636 - val_loss: 0.1566 - learning_rate: 3.0000e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss improved from 0.15660 to 0.14788, saving model to outputs/step3_2_temporal_test\\velocity_acceleration\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.9126 - loss: 0.3329 - val_accuracy: 0.9633 - val_loss: 0.1479 - learning_rate: 3.0000e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.14788\n",
      "178/178 - 4s - 22ms/step - accuracy: 0.9179 - loss: 0.3137 - val_accuracy: 0.9621 - val_loss: 0.1528 - learning_rate: 3.0000e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss improved from 0.14788 to 0.13153, saving model to outputs/step3_2_temporal_test\\velocity_acceleration\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 22ms/step - accuracy: 0.9208 - loss: 0.3056 - val_accuracy: 0.9725 - val_loss: 0.1315 - learning_rate: 3.0000e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss improved from 0.13153 to 0.11963, saving model to outputs/step3_2_temporal_test\\velocity_acceleration\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 22ms/step - accuracy: 0.9275 - loss: 0.2929 - val_accuracy: 0.9765 - val_loss: 0.1196 - learning_rate: 3.0000e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.11963\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.9314 - loss: 0.2831 - val_accuracy: 0.9698 - val_loss: 0.1375 - learning_rate: 3.0000e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss improved from 0.11963 to 0.10555, saving model to outputs/step3_2_temporal_test\\velocity_acceleration\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.9320 - loss: 0.2677 - val_accuracy: 0.9799 - val_loss: 0.1055 - learning_rate: 3.0000e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.10555\n",
      "178/178 - 4s - 22ms/step - accuracy: 0.9340 - loss: 0.2678 - val_accuracy: 0.9798 - val_loss: 0.1085 - learning_rate: 3.0000e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.10555\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.9362 - loss: 0.2648 - val_accuracy: 0.9680 - val_loss: 0.1415 - learning_rate: 3.0000e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss improved from 0.10555 to 0.10044, saving model to outputs/step3_2_temporal_test\\velocity_acceleration\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 22ms/step - accuracy: 0.9421 - loss: 0.2459 - val_accuracy: 0.9831 - val_loss: 0.1004 - learning_rate: 3.0000e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.10044\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.9449 - loss: 0.2326 - val_accuracy: 0.9810 - val_loss: 0.1019 - learning_rate: 3.0000e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.10044\n",
      "178/178 - 4s - 22ms/step - accuracy: 0.9452 - loss: 0.2338 - val_accuracy: 0.9805 - val_loss: 0.1039 - learning_rate: 3.0000e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss improved from 0.10044 to 0.08512, saving model to outputs/step3_2_temporal_test\\velocity_acceleration\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.9438 - loss: 0.2383 - val_accuracy: 0.9876 - val_loss: 0.0851 - learning_rate: 3.0000e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.08512\n",
      "178/178 - 4s - 22ms/step - accuracy: 0.9468 - loss: 0.2243 - val_accuracy: 0.9801 - val_loss: 0.1020 - learning_rate: 3.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 29.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.6237 ---\n",
      "\n",
      "===== 시계열 특징 조합 테스트: velocity, segment_stats =====\n",
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "특징 추출(병렬): 100%|██████████| 1111/1111 [00:37<00:00, 29.78it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "청크 병합: 100%|██████████| 1111/1111 [00:13<00:00, 84.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터가 11356개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 1.47835, saving model to outputs/step3_2_temporal_test\\velocity_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 7s - 37ms/step - accuracy: 0.2710 - loss: 2.2121 - val_accuracy: 0.5068 - val_loss: 1.4783 - learning_rate: 3.0000e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 1.47835 to 1.09747, saving model to outputs/step3_2_temporal_test\\velocity_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 25ms/step - accuracy: 0.4648 - loss: 1.6287 - val_accuracy: 0.6593 - val_loss: 1.0975 - learning_rate: 3.0000e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 1.09747 to 0.75777, saving model to outputs/step3_2_temporal_test\\velocity_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 25ms/step - accuracy: 0.5839 - loss: 1.2889 - val_accuracy: 0.7804 - val_loss: 0.7578 - learning_rate: 3.0000e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 0.75777 to 0.58929, saving model to outputs/step3_2_temporal_test\\velocity_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 26ms/step - accuracy: 0.6759 - loss: 1.0158 - val_accuracy: 0.8300 - val_loss: 0.5893 - learning_rate: 3.0000e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 0.58929 to 0.49032, saving model to outputs/step3_2_temporal_test\\velocity_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 26ms/step - accuracy: 0.7329 - loss: 0.8492 - val_accuracy: 0.8526 - val_loss: 0.4903 - learning_rate: 3.0000e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 0.49032 to 0.45647, saving model to outputs/step3_2_temporal_test\\velocity_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 26ms/step - accuracy: 0.7766 - loss: 0.7405 - val_accuracy: 0.8595 - val_loss: 0.4565 - learning_rate: 3.0000e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.45647 to 0.34722, saving model to outputs/step3_2_temporal_test\\velocity_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 27ms/step - accuracy: 0.8060 - loss: 0.6637 - val_accuracy: 0.9017 - val_loss: 0.3472 - learning_rate: 3.0000e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 0.34722 to 0.29443, saving model to outputs/step3_2_temporal_test\\velocity_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 26ms/step - accuracy: 0.8274 - loss: 0.5949 - val_accuracy: 0.9198 - val_loss: 0.2944 - learning_rate: 3.0000e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.29443\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.8475 - loss: 0.5382 - val_accuracy: 0.9120 - val_loss: 0.3167 - learning_rate: 3.0000e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 0.29443 to 0.27821, saving model to outputs/step3_2_temporal_test\\velocity_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 24ms/step - accuracy: 0.8643 - loss: 0.4974 - val_accuracy: 0.9240 - val_loss: 0.2782 - learning_rate: 3.0000e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 0.27821 to 0.22106, saving model to outputs/step3_2_temporal_test\\velocity_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.8795 - loss: 0.4466 - val_accuracy: 0.9391 - val_loss: 0.2211 - learning_rate: 3.0000e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 0.22106 to 0.19675, saving model to outputs/step3_2_temporal_test\\velocity_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 24ms/step - accuracy: 0.8822 - loss: 0.4424 - val_accuracy: 0.9472 - val_loss: 0.1967 - learning_rate: 3.0000e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 0.19675 to 0.19230, saving model to outputs/step3_2_temporal_test\\velocity_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.8953 - loss: 0.3871 - val_accuracy: 0.9521 - val_loss: 0.1923 - learning_rate: 3.0000e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 0.19230 to 0.19209, saving model to outputs/step3_2_temporal_test\\velocity_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.8974 - loss: 0.3805 - val_accuracy: 0.9505 - val_loss: 0.1921 - learning_rate: 3.0000e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss improved from 0.19209 to 0.15841, saving model to outputs/step3_2_temporal_test\\velocity_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 24ms/step - accuracy: 0.9052 - loss: 0.3652 - val_accuracy: 0.9614 - val_loss: 0.1584 - learning_rate: 3.0000e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss improved from 0.15841 to 0.15220, saving model to outputs/step3_2_temporal_test\\velocity_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.9106 - loss: 0.3518 - val_accuracy: 0.9656 - val_loss: 0.1522 - learning_rate: 3.0000e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.15220\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.9133 - loss: 0.3326 - val_accuracy: 0.9677 - val_loss: 0.1541 - learning_rate: 3.0000e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.15220\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.9220 - loss: 0.3120 - val_accuracy: 0.9570 - val_loss: 0.1663 - learning_rate: 3.0000e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss improved from 0.15220 to 0.15077, saving model to outputs/step3_2_temporal_test\\velocity_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 24ms/step - accuracy: 0.9268 - loss: 0.2946 - val_accuracy: 0.9648 - val_loss: 0.1508 - learning_rate: 3.0000e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss improved from 0.15077 to 0.14819, saving model to outputs/step3_2_temporal_test\\velocity_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 24ms/step - accuracy: 0.9234 - loss: 0.3009 - val_accuracy: 0.9665 - val_loss: 0.1482 - learning_rate: 3.0000e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss improved from 0.14819 to 0.13457, saving model to outputs/step3_2_temporal_test\\velocity_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 24ms/step - accuracy: 0.9341 - loss: 0.2734 - val_accuracy: 0.9689 - val_loss: 0.1346 - learning_rate: 3.0000e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss improved from 0.13457 to 0.12779, saving model to outputs/step3_2_temporal_test\\velocity_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 24ms/step - accuracy: 0.9299 - loss: 0.2798 - val_accuracy: 0.9726 - val_loss: 0.1278 - learning_rate: 3.0000e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.12779\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.9320 - loss: 0.2754 - val_accuracy: 0.9678 - val_loss: 0.1437 - learning_rate: 3.0000e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.12779\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.9373 - loss: 0.2648 - val_accuracy: 0.9702 - val_loss: 0.1328 - learning_rate: 3.0000e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss improved from 0.12779 to 0.11559, saving model to outputs/step3_2_temporal_test\\velocity_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 22ms/step - accuracy: 0.9408 - loss: 0.2509 - val_accuracy: 0.9761 - val_loss: 0.1156 - learning_rate: 3.0000e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.11559\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.9390 - loss: 0.2532 - val_accuracy: 0.9689 - val_loss: 0.1340 - learning_rate: 3.0000e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss improved from 0.11559 to 0.10356, saving model to outputs/step3_2_temporal_test\\velocity_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 22ms/step - accuracy: 0.9436 - loss: 0.2409 - val_accuracy: 0.9793 - val_loss: 0.1036 - learning_rate: 3.0000e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.10356\n",
      "178/178 - 4s - 22ms/step - accuracy: 0.9500 - loss: 0.2281 - val_accuracy: 0.9746 - val_loss: 0.1161 - learning_rate: 3.0000e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.10356\n",
      "178/178 - 4s - 24ms/step - accuracy: 0.9472 - loss: 0.2329 - val_accuracy: 0.9648 - val_loss: 0.1442 - learning_rate: 3.0000e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.10356\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.9490 - loss: 0.2274 - val_accuracy: 0.9663 - val_loss: 0.1447 - learning_rate: 3.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 27.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.5589 ---\n",
      "\n",
      "===== 시계열 특징 조합 테스트: velocity =====\n",
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "특징 추출(병렬): 100%|██████████| 1111/1111 [00:34<00:00, 32.09it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "청크 병합: 100%|██████████| 1111/1111 [00:09<00:00, 117.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터가 11356개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 1.69004, saving model to outputs/step3_2_temporal_test\\velocity\\models\\fold_7\\best_model.keras\n",
      "178/178 - 6s - 34ms/step - accuracy: 0.2375 - loss: 2.3230 - val_accuracy: 0.4306 - val_loss: 1.6900 - learning_rate: 3.0000e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 1.69004 to 1.30391, saving model to outputs/step3_2_temporal_test\\velocity\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 20ms/step - accuracy: 0.4059 - loss: 1.7845 - val_accuracy: 0.5962 - val_loss: 1.3039 - learning_rate: 3.0000e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 1.30391 to 0.97622, saving model to outputs/step3_2_temporal_test\\velocity\\models\\fold_7\\best_model.keras\n",
      "178/178 - 3s - 19ms/step - accuracy: 0.5246 - loss: 1.4760 - val_accuracy: 0.6962 - val_loss: 0.9762 - learning_rate: 3.0000e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 0.97622 to 0.75716, saving model to outputs/step3_2_temporal_test\\velocity\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.6109 - loss: 1.2030 - val_accuracy: 0.7762 - val_loss: 0.7572 - learning_rate: 3.0000e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 0.75716 to 0.58868, saving model to outputs/step3_2_temporal_test\\velocity\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.6889 - loss: 0.9976 - val_accuracy: 0.8460 - val_loss: 0.5887 - learning_rate: 3.0000e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 0.58868 to 0.50670, saving model to outputs/step3_2_temporal_test\\velocity\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.7405 - loss: 0.8582 - val_accuracy: 0.8559 - val_loss: 0.5067 - learning_rate: 3.0000e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.50670 to 0.43254, saving model to outputs/step3_2_temporal_test\\velocity\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.7754 - loss: 0.7689 - val_accuracy: 0.8819 - val_loss: 0.4325 - learning_rate: 3.0000e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 0.43254 to 0.37218, saving model to outputs/step3_2_temporal_test\\velocity\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.8001 - loss: 0.6875 - val_accuracy: 0.8957 - val_loss: 0.3722 - learning_rate: 3.0000e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.37218 to 0.34447, saving model to outputs/step3_2_temporal_test\\velocity\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.8227 - loss: 0.6231 - val_accuracy: 0.9059 - val_loss: 0.3445 - learning_rate: 3.0000e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 0.34447 to 0.30838, saving model to outputs/step3_2_temporal_test\\velocity\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.8352 - loss: 0.5745 - val_accuracy: 0.9177 - val_loss: 0.3084 - learning_rate: 3.0000e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 0.30838 to 0.30221, saving model to outputs/step3_2_temporal_test\\velocity\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.8525 - loss: 0.5185 - val_accuracy: 0.9087 - val_loss: 0.3022 - learning_rate: 3.0000e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 0.30221 to 0.23259, saving model to outputs/step3_2_temporal_test\\velocity\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.8638 - loss: 0.4858 - val_accuracy: 0.9353 - val_loss: 0.2326 - learning_rate: 3.0000e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 0.23259 to 0.21586, saving model to outputs/step3_2_temporal_test\\velocity\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.8743 - loss: 0.4530 - val_accuracy: 0.9431 - val_loss: 0.2159 - learning_rate: 3.0000e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 0.21586 to 0.19799, saving model to outputs/step3_2_temporal_test\\velocity\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.8836 - loss: 0.4254 - val_accuracy: 0.9484 - val_loss: 0.1980 - learning_rate: 3.0000e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss improved from 0.19799 to 0.19167, saving model to outputs/step3_2_temporal_test\\velocity\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.8922 - loss: 0.3990 - val_accuracy: 0.9491 - val_loss: 0.1917 - learning_rate: 3.0000e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss improved from 0.19167 to 0.18170, saving model to outputs/step3_2_temporal_test\\velocity\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.8942 - loss: 0.3964 - val_accuracy: 0.9523 - val_loss: 0.1817 - learning_rate: 3.0000e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 0.18170 to 0.17402, saving model to outputs/step3_2_temporal_test\\velocity\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.9018 - loss: 0.3676 - val_accuracy: 0.9561 - val_loss: 0.1740 - learning_rate: 3.0000e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss improved from 0.17402 to 0.15126, saving model to outputs/step3_2_temporal_test\\velocity\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.9084 - loss: 0.3492 - val_accuracy: 0.9629 - val_loss: 0.1513 - learning_rate: 3.0000e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss improved from 0.15126 to 0.14489, saving model to outputs/step3_2_temporal_test\\velocity\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.9115 - loss: 0.3360 - val_accuracy: 0.9629 - val_loss: 0.1449 - learning_rate: 3.0000e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.14489\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.9152 - loss: 0.3208 - val_accuracy: 0.9599 - val_loss: 0.1547 - learning_rate: 3.0000e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss improved from 0.14489 to 0.12809, saving model to outputs/step3_2_temporal_test\\velocity\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.9237 - loss: 0.3051 - val_accuracy: 0.9696 - val_loss: 0.1281 - learning_rate: 3.0000e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss improved from 0.12809 to 0.12589, saving model to outputs/step3_2_temporal_test\\velocity\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.9227 - loss: 0.2948 - val_accuracy: 0.9707 - val_loss: 0.1259 - learning_rate: 3.0000e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.12589\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.9283 - loss: 0.2862 - val_accuracy: 0.9692 - val_loss: 0.1291 - learning_rate: 3.0000e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.12589\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.9306 - loss: 0.2758 - val_accuracy: 0.9719 - val_loss: 0.1267 - learning_rate: 3.0000e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss improved from 0.12589 to 0.11277, saving model to outputs/step3_2_temporal_test\\velocity\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.9322 - loss: 0.2771 - val_accuracy: 0.9749 - val_loss: 0.1128 - learning_rate: 3.0000e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss improved from 0.11277 to 0.10391, saving model to outputs/step3_2_temporal_test\\velocity\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.9369 - loss: 0.2550 - val_accuracy: 0.9786 - val_loss: 0.1039 - learning_rate: 3.0000e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.10391\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.9339 - loss: 0.2687 - val_accuracy: 0.9728 - val_loss: 0.1179 - learning_rate: 3.0000e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.10391\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.9427 - loss: 0.2439 - val_accuracy: 0.9722 - val_loss: 0.1172 - learning_rate: 3.0000e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.10391\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.9378 - loss: 0.2445 - val_accuracy: 0.9781 - val_loss: 0.1048 - learning_rate: 3.0000e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss improved from 0.10391 to 0.10247, saving model to outputs/step3_2_temporal_test\\velocity\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.9395 - loss: 0.2440 - val_accuracy: 0.9814 - val_loss: 0.1025 - learning_rate: 3.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 30.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.7116 ---\n",
      "\n",
      "===== 시계열 특징 조합 테스트: acceleration, segment_stats =====\n",
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "특징 추출(병렬): 100%|██████████| 1111/1111 [00:37<00:00, 29.79it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "청크 병합: 100%|██████████| 1111/1111 [00:13<00:00, 80.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터가 11356개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 1.49822, saving model to outputs/step3_2_temporal_test\\acceleration_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 6s - 36ms/step - accuracy: 0.2746 - loss: 2.2062 - val_accuracy: 0.5017 - val_loss: 1.4982 - learning_rate: 3.0000e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 1.49822 to 1.11673, saving model to outputs/step3_2_temporal_test\\acceleration_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 22ms/step - accuracy: 0.4634 - loss: 1.6246 - val_accuracy: 0.6484 - val_loss: 1.1167 - learning_rate: 3.0000e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 1.11673 to 0.76039, saving model to outputs/step3_2_temporal_test\\acceleration_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 22ms/step - accuracy: 0.5763 - loss: 1.2915 - val_accuracy: 0.7717 - val_loss: 0.7604 - learning_rate: 3.0000e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 0.76039 to 0.61728, saving model to outputs/step3_2_temporal_test\\acceleration_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.6750 - loss: 1.0185 - val_accuracy: 0.8077 - val_loss: 0.6173 - learning_rate: 3.0000e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 0.61728 to 0.50068, saving model to outputs/step3_2_temporal_test\\acceleration_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 22ms/step - accuracy: 0.7279 - loss: 0.8718 - val_accuracy: 0.8514 - val_loss: 0.5007 - learning_rate: 3.0000e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 0.50068 to 0.46900, saving model to outputs/step3_2_temporal_test\\acceleration_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.7671 - loss: 0.7741 - val_accuracy: 0.8657 - val_loss: 0.4690 - learning_rate: 3.0000e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.46900 to 0.43508, saving model to outputs/step3_2_temporal_test\\acceleration_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.7946 - loss: 0.6904 - val_accuracy: 0.8797 - val_loss: 0.4351 - learning_rate: 3.0000e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 0.43508 to 0.34628, saving model to outputs/step3_2_temporal_test\\acceleration_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 22ms/step - accuracy: 0.8184 - loss: 0.6288 - val_accuracy: 0.8994 - val_loss: 0.3463 - learning_rate: 3.0000e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.34628 to 0.34076, saving model to outputs/step3_2_temporal_test\\acceleration_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 22ms/step - accuracy: 0.8351 - loss: 0.5749 - val_accuracy: 0.9070 - val_loss: 0.3408 - learning_rate: 3.0000e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 0.34076 to 0.32721, saving model to outputs/step3_2_temporal_test\\acceleration_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 22ms/step - accuracy: 0.8530 - loss: 0.5303 - val_accuracy: 0.9048 - val_loss: 0.3272 - learning_rate: 3.0000e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 0.32721 to 0.28993, saving model to outputs/step3_2_temporal_test\\acceleration_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.8635 - loss: 0.5018 - val_accuracy: 0.9202 - val_loss: 0.2899 - learning_rate: 3.0000e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 0.28993 to 0.27283, saving model to outputs/step3_2_temporal_test\\acceleration_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.8699 - loss: 0.4644 - val_accuracy: 0.9273 - val_loss: 0.2728 - learning_rate: 3.0000e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 0.27283 to 0.23996, saving model to outputs/step3_2_temporal_test\\acceleration_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.8788 - loss: 0.4476 - val_accuracy: 0.9353 - val_loss: 0.2400 - learning_rate: 3.0000e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 0.23996 to 0.21892, saving model to outputs/step3_2_temporal_test\\acceleration_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.8869 - loss: 0.4086 - val_accuracy: 0.9413 - val_loss: 0.2189 - learning_rate: 3.0000e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss improved from 0.21892 to 0.20297, saving model to outputs/step3_2_temporal_test\\acceleration_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 22ms/step - accuracy: 0.8908 - loss: 0.3986 - val_accuracy: 0.9454 - val_loss: 0.2030 - learning_rate: 3.0000e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss improved from 0.20297 to 0.18746, saving model to outputs/step3_2_temporal_test\\acceleration_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.8993 - loss: 0.3877 - val_accuracy: 0.9509 - val_loss: 0.1875 - learning_rate: 3.0000e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.18746\n",
      "178/178 - 4s - 22ms/step - accuracy: 0.9031 - loss: 0.3694 - val_accuracy: 0.9461 - val_loss: 0.2144 - learning_rate: 3.0000e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss improved from 0.18746 to 0.16623, saving model to outputs/step3_2_temporal_test\\acceleration_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.9071 - loss: 0.3561 - val_accuracy: 0.9588 - val_loss: 0.1662 - learning_rate: 3.0000e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss improved from 0.16623 to 0.15349, saving model to outputs/step3_2_temporal_test\\acceleration_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.9116 - loss: 0.3375 - val_accuracy: 0.9599 - val_loss: 0.1535 - learning_rate: 3.0000e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.15349\n",
      "178/178 - 4s - 22ms/step - accuracy: 0.9191 - loss: 0.3240 - val_accuracy: 0.9623 - val_loss: 0.1560 - learning_rate: 3.0000e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss improved from 0.15349 to 0.15136, saving model to outputs/step3_2_temporal_test\\acceleration_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.9203 - loss: 0.3116 - val_accuracy: 0.9597 - val_loss: 0.1514 - learning_rate: 3.0000e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.15136\n",
      "178/178 - 4s - 22ms/step - accuracy: 0.9230 - loss: 0.3104 - val_accuracy: 0.9600 - val_loss: 0.1547 - learning_rate: 3.0000e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss improved from 0.15136 to 0.14598, saving model to outputs/step3_2_temporal_test\\acceleration_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 22ms/step - accuracy: 0.9276 - loss: 0.2905 - val_accuracy: 0.9675 - val_loss: 0.1460 - learning_rate: 3.0000e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss improved from 0.14598 to 0.14008, saving model to outputs/step3_2_temporal_test\\acceleration_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 22ms/step - accuracy: 0.9233 - loss: 0.2882 - val_accuracy: 0.9672 - val_loss: 0.1401 - learning_rate: 3.0000e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss improved from 0.14008 to 0.12260, saving model to outputs/step3_2_temporal_test\\acceleration_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.9254 - loss: 0.2940 - val_accuracy: 0.9731 - val_loss: 0.1226 - learning_rate: 3.0000e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss improved from 0.12260 to 0.11580, saving model to outputs/step3_2_temporal_test\\acceleration_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 22ms/step - accuracy: 0.9303 - loss: 0.2719 - val_accuracy: 0.9777 - val_loss: 0.1158 - learning_rate: 3.0000e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.11580\n",
      "178/178 - 4s - 22ms/step - accuracy: 0.9347 - loss: 0.2659 - val_accuracy: 0.9726 - val_loss: 0.1211 - learning_rate: 3.0000e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss improved from 0.11580 to 0.11558, saving model to outputs/step3_2_temporal_test\\acceleration_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.9376 - loss: 0.2501 - val_accuracy: 0.9741 - val_loss: 0.1156 - learning_rate: 3.0000e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss improved from 0.11558 to 0.10673, saving model to outputs/step3_2_temporal_test\\acceleration_segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.9360 - loss: 0.2592 - val_accuracy: 0.9775 - val_loss: 0.1067 - learning_rate: 3.0000e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.10673\n",
      "178/178 - 4s - 22ms/step - accuracy: 0.9420 - loss: 0.2350 - val_accuracy: 0.9741 - val_loss: 0.1145 - learning_rate: 3.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 29.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.6361 ---\n",
      "\n",
      "===== 시계열 특징 조합 테스트: acceleration =====\n",
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "특징 추출(병렬): 100%|██████████| 1111/1111 [00:34<00:00, 32.52it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "청크 병합: 100%|██████████| 1111/1111 [00:10<00:00, 105.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터가 11356개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 1.70137, saving model to outputs/step3_2_temporal_test\\acceleration\\models\\fold_7\\best_model.keras\n",
      "178/178 - 9s - 48ms/step - accuracy: 0.2430 - loss: 2.3228 - val_accuracy: 0.4381 - val_loss: 1.7014 - learning_rate: 3.0000e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 1.70137 to 1.31558, saving model to outputs/step3_2_temporal_test\\acceleration\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 22ms/step - accuracy: 0.4086 - loss: 1.7858 - val_accuracy: 0.6007 - val_loss: 1.3156 - learning_rate: 3.0000e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 1.31558 to 1.00398, saving model to outputs/step3_2_temporal_test\\acceleration\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 22ms/step - accuracy: 0.5153 - loss: 1.4850 - val_accuracy: 0.6782 - val_loss: 1.0040 - learning_rate: 3.0000e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 1.00398 to 0.82154, saving model to outputs/step3_2_temporal_test\\acceleration\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 22ms/step - accuracy: 0.6005 - loss: 1.2465 - val_accuracy: 0.7458 - val_loss: 0.8215 - learning_rate: 3.0000e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 0.82154 to 0.67136, saving model to outputs/step3_2_temporal_test\\acceleration\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.6606 - loss: 1.0801 - val_accuracy: 0.8066 - val_loss: 0.6714 - learning_rate: 3.0000e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 0.67136 to 0.56065, saving model to outputs/step3_2_temporal_test\\acceleration\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 26ms/step - accuracy: 0.7101 - loss: 0.9271 - val_accuracy: 0.8370 - val_loss: 0.5607 - learning_rate: 3.0000e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.56065 to 0.45613, saving model to outputs/step3_2_temporal_test\\acceleration\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.7486 - loss: 0.8243 - val_accuracy: 0.8698 - val_loss: 0.4561 - learning_rate: 3.0000e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 0.45613 to 0.43284, saving model to outputs/step3_2_temporal_test\\acceleration\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.7777 - loss: 0.7385 - val_accuracy: 0.8666 - val_loss: 0.4328 - learning_rate: 3.0000e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.43284 to 0.36936, saving model to outputs/step3_2_temporal_test\\acceleration\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.8079 - loss: 0.6627 - val_accuracy: 0.8939 - val_loss: 0.3694 - learning_rate: 3.0000e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 0.36936 to 0.34318, saving model to outputs/step3_2_temporal_test\\acceleration\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.8190 - loss: 0.6152 - val_accuracy: 0.9027 - val_loss: 0.3432 - learning_rate: 3.0000e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 0.34318 to 0.33419, saving model to outputs/step3_2_temporal_test\\acceleration\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.8362 - loss: 0.5655 - val_accuracy: 0.9054 - val_loss: 0.3342 - learning_rate: 3.0000e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 0.33419 to 0.30738, saving model to outputs/step3_2_temporal_test\\acceleration\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.8415 - loss: 0.5331 - val_accuracy: 0.9132 - val_loss: 0.3074 - learning_rate: 3.0000e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 0.30738 to 0.29689, saving model to outputs/step3_2_temporal_test\\acceleration\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.8530 - loss: 0.5028 - val_accuracy: 0.9096 - val_loss: 0.2969 - learning_rate: 3.0000e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 0.29689 to 0.24677, saving model to outputs/step3_2_temporal_test\\acceleration\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.8678 - loss: 0.4713 - val_accuracy: 0.9288 - val_loss: 0.2468 - learning_rate: 3.0000e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.24677\n",
      "178/178 - 4s - 22ms/step - accuracy: 0.8735 - loss: 0.4557 - val_accuracy: 0.9300 - val_loss: 0.2515 - learning_rate: 3.0000e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss improved from 0.24677 to 0.23775, saving model to outputs/step3_2_temporal_test\\acceleration\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.8835 - loss: 0.4229 - val_accuracy: 0.9374 - val_loss: 0.2377 - learning_rate: 3.0000e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 0.23775 to 0.20243, saving model to outputs/step3_2_temporal_test\\acceleration\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.8911 - loss: 0.3979 - val_accuracy: 0.9470 - val_loss: 0.2024 - learning_rate: 3.0000e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss improved from 0.20243 to 0.19820, saving model to outputs/step3_2_temporal_test\\acceleration\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.8920 - loss: 0.3865 - val_accuracy: 0.9460 - val_loss: 0.1982 - learning_rate: 3.0000e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss improved from 0.19820 to 0.18126, saving model to outputs/step3_2_temporal_test\\acceleration\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.8994 - loss: 0.3755 - val_accuracy: 0.9515 - val_loss: 0.1813 - learning_rate: 3.0000e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss improved from 0.18126 to 0.17029, saving model to outputs/step3_2_temporal_test\\acceleration\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.9044 - loss: 0.3556 - val_accuracy: 0.9542 - val_loss: 0.1703 - learning_rate: 3.0000e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss improved from 0.17029 to 0.16900, saving model to outputs/step3_2_temporal_test\\acceleration\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.9106 - loss: 0.3445 - val_accuracy: 0.9561 - val_loss: 0.1690 - learning_rate: 3.0000e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss improved from 0.16900 to 0.15396, saving model to outputs/step3_2_temporal_test\\acceleration\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.9157 - loss: 0.3215 - val_accuracy: 0.9612 - val_loss: 0.1540 - learning_rate: 3.0000e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.15396\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.9224 - loss: 0.3103 - val_accuracy: 0.9585 - val_loss: 0.1652 - learning_rate: 3.0000e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss improved from 0.15396 to 0.14383, saving model to outputs/step3_2_temporal_test\\acceleration\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.9223 - loss: 0.2978 - val_accuracy: 0.9657 - val_loss: 0.1438 - learning_rate: 3.0000e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss improved from 0.14383 to 0.13416, saving model to outputs/step3_2_temporal_test\\acceleration\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.9180 - loss: 0.3052 - val_accuracy: 0.9675 - val_loss: 0.1342 - learning_rate: 3.0000e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss improved from 0.13416 to 0.12218, saving model to outputs/step3_2_temporal_test\\acceleration\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.9269 - loss: 0.2807 - val_accuracy: 0.9725 - val_loss: 0.1222 - learning_rate: 3.0000e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.12218\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.9305 - loss: 0.2752 - val_accuracy: 0.9698 - val_loss: 0.1237 - learning_rate: 3.0000e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss improved from 0.12218 to 0.11271, saving model to outputs/step3_2_temporal_test\\acceleration\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.9291 - loss: 0.2756 - val_accuracy: 0.9771 - val_loss: 0.1127 - learning_rate: 3.0000e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.11271\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.9273 - loss: 0.2805 - val_accuracy: 0.9711 - val_loss: 0.1225 - learning_rate: 3.0000e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.11271\n",
      "178/178 - 4s - 23ms/step - accuracy: 0.9345 - loss: 0.2677 - val_accuracy: 0.9741 - val_loss: 0.1146 - learning_rate: 3.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 28.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.6702 ---\n",
      "\n",
      "===== 시계열 특징 조합 테스트: segment_stats =====\n",
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "특징 추출(병렬): 100%|██████████| 1111/1111 [00:34<00:00, 31.96it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "청크 병합: 100%|██████████| 1111/1111 [00:10<00:00, 106.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터가 11356개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 1.52565, saving model to outputs/step3_2_temporal_test\\segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 7s - 38ms/step - accuracy: 0.2718 - loss: 2.2076 - val_accuracy: 0.5067 - val_loss: 1.5257 - learning_rate: 3.0000e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 1.52565 to 1.14720, saving model to outputs/step3_2_temporal_test\\segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 24ms/step - accuracy: 0.4549 - loss: 1.6603 - val_accuracy: 0.6565 - val_loss: 1.1472 - learning_rate: 3.0000e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 1.14720 to 0.95167, saving model to outputs/step3_2_temporal_test\\segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 24ms/step - accuracy: 0.5554 - loss: 1.3821 - val_accuracy: 0.7102 - val_loss: 0.9517 - learning_rate: 3.0000e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 0.95167 to 0.80093, saving model to outputs/step3_2_temporal_test\\segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 24ms/step - accuracy: 0.6300 - loss: 1.1819 - val_accuracy: 0.7755 - val_loss: 0.8009 - learning_rate: 3.0000e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 0.80093 to 0.72169, saving model to outputs/step3_2_temporal_test\\segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 25ms/step - accuracy: 0.6697 - loss: 1.0666 - val_accuracy: 0.7899 - val_loss: 0.7217 - learning_rate: 3.0000e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 0.72169 to 0.66726, saving model to outputs/step3_2_temporal_test\\segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 25ms/step - accuracy: 0.7080 - loss: 0.9701 - val_accuracy: 0.8063 - val_loss: 0.6673 - learning_rate: 3.0000e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.66726 to 0.56528, saving model to outputs/step3_2_temporal_test\\segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 25ms/step - accuracy: 0.7362 - loss: 0.9008 - val_accuracy: 0.8403 - val_loss: 0.5653 - learning_rate: 3.0000e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 0.56528 to 0.49864, saving model to outputs/step3_2_temporal_test\\segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 24ms/step - accuracy: 0.7526 - loss: 0.8508 - val_accuracy: 0.8559 - val_loss: 0.4986 - learning_rate: 3.0000e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.49864 to 0.47120, saving model to outputs/step3_2_temporal_test\\segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 25ms/step - accuracy: 0.7796 - loss: 0.7663 - val_accuracy: 0.8642 - val_loss: 0.4712 - learning_rate: 3.0000e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 0.47120 to 0.44130, saving model to outputs/step3_2_temporal_test\\segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 25ms/step - accuracy: 0.7977 - loss: 0.7242 - val_accuracy: 0.8732 - val_loss: 0.4413 - learning_rate: 3.0000e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 0.44130 to 0.40740, saving model to outputs/step3_2_temporal_test\\segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 25ms/step - accuracy: 0.8058 - loss: 0.6850 - val_accuracy: 0.8815 - val_loss: 0.4074 - learning_rate: 3.0000e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 0.40740 to 0.37941, saving model to outputs/step3_2_temporal_test\\segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 25ms/step - accuracy: 0.8188 - loss: 0.6445 - val_accuracy: 0.8876 - val_loss: 0.3794 - learning_rate: 3.0000e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 0.37941 to 0.35933, saving model to outputs/step3_2_temporal_test\\segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 25ms/step - accuracy: 0.8278 - loss: 0.6095 - val_accuracy: 0.8948 - val_loss: 0.3593 - learning_rate: 3.0000e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 0.35933 to 0.35025, saving model to outputs/step3_2_temporal_test\\segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 25ms/step - accuracy: 0.8396 - loss: 0.5843 - val_accuracy: 0.9056 - val_loss: 0.3502 - learning_rate: 3.0000e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss improved from 0.35025 to 0.30239, saving model to outputs/step3_2_temporal_test\\segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 25ms/step - accuracy: 0.8483 - loss: 0.5548 - val_accuracy: 0.9157 - val_loss: 0.3024 - learning_rate: 3.0000e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.30239\n",
      "178/178 - 4s - 25ms/step - accuracy: 0.8541 - loss: 0.5391 - val_accuracy: 0.9129 - val_loss: 0.3193 - learning_rate: 3.0000e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 0.30239 to 0.27890, saving model to outputs/step3_2_temporal_test\\segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 25ms/step - accuracy: 0.8601 - loss: 0.5133 - val_accuracy: 0.9220 - val_loss: 0.2789 - learning_rate: 3.0000e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.27890\n",
      "178/178 - 4s - 25ms/step - accuracy: 0.8660 - loss: 0.4926 - val_accuracy: 0.9190 - val_loss: 0.2913 - learning_rate: 3.0000e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss improved from 0.27890 to 0.27697, saving model to outputs/step3_2_temporal_test\\segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 25ms/step - accuracy: 0.8668 - loss: 0.4983 - val_accuracy: 0.9273 - val_loss: 0.2770 - learning_rate: 3.0000e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss improved from 0.27697 to 0.26757, saving model to outputs/step3_2_temporal_test\\segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 25ms/step - accuracy: 0.8748 - loss: 0.4630 - val_accuracy: 0.9274 - val_loss: 0.2676 - learning_rate: 3.0000e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss improved from 0.26757 to 0.24094, saving model to outputs/step3_2_temporal_test\\segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 24ms/step - accuracy: 0.8787 - loss: 0.4576 - val_accuracy: 0.9392 - val_loss: 0.2409 - learning_rate: 3.0000e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.24094\n",
      "178/178 - 4s - 24ms/step - accuracy: 0.8855 - loss: 0.4365 - val_accuracy: 0.9300 - val_loss: 0.2546 - learning_rate: 3.0000e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss improved from 0.24094 to 0.21610, saving model to outputs/step3_2_temporal_test\\segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 24ms/step - accuracy: 0.8879 - loss: 0.4252 - val_accuracy: 0.9451 - val_loss: 0.2161 - learning_rate: 3.0000e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss improved from 0.21610 to 0.21519, saving model to outputs/step3_2_temporal_test\\segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 24ms/step - accuracy: 0.8926 - loss: 0.4189 - val_accuracy: 0.9424 - val_loss: 0.2152 - learning_rate: 3.0000e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.21519\n",
      "178/178 - 4s - 24ms/step - accuracy: 0.8959 - loss: 0.3975 - val_accuracy: 0.9382 - val_loss: 0.2301 - learning_rate: 3.0000e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss improved from 0.21519 to 0.20877, saving model to outputs/step3_2_temporal_test\\segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 25ms/step - accuracy: 0.9020 - loss: 0.3822 - val_accuracy: 0.9466 - val_loss: 0.2088 - learning_rate: 3.0000e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.20877\n",
      "178/178 - 4s - 24ms/step - accuracy: 0.9003 - loss: 0.3818 - val_accuracy: 0.9434 - val_loss: 0.2233 - learning_rate: 3.0000e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss improved from 0.20877 to 0.19698, saving model to outputs/step3_2_temporal_test\\segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 24ms/step - accuracy: 0.9037 - loss: 0.3691 - val_accuracy: 0.9512 - val_loss: 0.1970 - learning_rate: 3.0000e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss improved from 0.19698 to 0.17892, saving model to outputs/step3_2_temporal_test\\segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 25ms/step - accuracy: 0.9065 - loss: 0.3681 - val_accuracy: 0.9539 - val_loss: 0.1789 - learning_rate: 3.0000e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss improved from 0.17892 to 0.17884, saving model to outputs/step3_2_temporal_test\\segment_stats\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 24ms/step - accuracy: 0.9082 - loss: 0.3538 - val_accuracy: 0.9557 - val_loss: 0.1788 - learning_rate: 3.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 30.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.6961 ---\n",
      "\n",
      "===== 시계열 특징 조합 테스트: Position Only =====\n",
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "특징 추출(병렬): 100%|██████████| 1111/1111 [00:33<00:00, 33.09it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "청크 병합: 100%|██████████| 1111/1111 [00:07<00:00, 139.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터가 11356개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 1.65252, saving model to outputs/step3_2_temporal_test\\Position Only\\models\\fold_7\\best_model.keras\n",
      "178/178 - 6s - 35ms/step - accuracy: 0.2523 - loss: 2.2895 - val_accuracy: 0.4567 - val_loss: 1.6525 - learning_rate: 3.0000e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 1.65252 to 1.18868, saving model to outputs/step3_2_temporal_test\\Position Only\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.4335 - loss: 1.7277 - val_accuracy: 0.6469 - val_loss: 1.1887 - learning_rate: 3.0000e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 1.18868 to 0.96975, saving model to outputs/step3_2_temporal_test\\Position Only\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.5486 - loss: 1.4051 - val_accuracy: 0.6963 - val_loss: 0.9698 - learning_rate: 3.0000e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 0.96975 to 0.82696, saving model to outputs/step3_2_temporal_test\\Position Only\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.5995 - loss: 1.2382 - val_accuracy: 0.7427 - val_loss: 0.8270 - learning_rate: 3.0000e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 0.82696 to 0.76733, saving model to outputs/step3_2_temporal_test\\Position Only\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.6424 - loss: 1.1387 - val_accuracy: 0.7710 - val_loss: 0.7673 - learning_rate: 3.0000e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 0.76733 to 0.71694, saving model to outputs/step3_2_temporal_test\\Position Only\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.6675 - loss: 1.0532 - val_accuracy: 0.7843 - val_loss: 0.7169 - learning_rate: 3.0000e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.71694 to 0.66902, saving model to outputs/step3_2_temporal_test\\Position Only\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.6870 - loss: 0.9996 - val_accuracy: 0.7985 - val_loss: 0.6690 - learning_rate: 3.0000e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 0.66902 to 0.63176, saving model to outputs/step3_2_temporal_test\\Position Only\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.7144 - loss: 0.9358 - val_accuracy: 0.8104 - val_loss: 0.6318 - learning_rate: 3.0000e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.63176 to 0.57526, saving model to outputs/step3_2_temporal_test\\Position Only\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.7361 - loss: 0.8793 - val_accuracy: 0.8241 - val_loss: 0.5753 - learning_rate: 3.0000e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 0.57526 to 0.52940, saving model to outputs/step3_2_temporal_test\\Position Only\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.7516 - loss: 0.8306 - val_accuracy: 0.8386 - val_loss: 0.5294 - learning_rate: 3.0000e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 0.52940 to 0.49463, saving model to outputs/step3_2_temporal_test\\Position Only\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.7636 - loss: 0.7951 - val_accuracy: 0.8535 - val_loss: 0.4946 - learning_rate: 3.0000e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 0.49463 to 0.45089, saving model to outputs/step3_2_temporal_test\\Position Only\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.7776 - loss: 0.7494 - val_accuracy: 0.8704 - val_loss: 0.4509 - learning_rate: 3.0000e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 0.45089 to 0.43018, saving model to outputs/step3_2_temporal_test\\Position Only\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.7931 - loss: 0.7081 - val_accuracy: 0.8770 - val_loss: 0.4302 - learning_rate: 3.0000e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 0.43018 to 0.42833, saving model to outputs/step3_2_temporal_test\\Position Only\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.8030 - loss: 0.6745 - val_accuracy: 0.8756 - val_loss: 0.4283 - learning_rate: 3.0000e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss improved from 0.42833 to 0.39312, saving model to outputs/step3_2_temporal_test\\Position Only\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.8174 - loss: 0.6448 - val_accuracy: 0.8877 - val_loss: 0.3931 - learning_rate: 3.0000e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss improved from 0.39312 to 0.37093, saving model to outputs/step3_2_temporal_test\\Position Only\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.8182 - loss: 0.6356 - val_accuracy: 0.8912 - val_loss: 0.3709 - learning_rate: 3.0000e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 0.37093 to 0.33981, saving model to outputs/step3_2_temporal_test\\Position Only\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.8234 - loss: 0.6019 - val_accuracy: 0.8993 - val_loss: 0.3398 - learning_rate: 3.0000e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss improved from 0.33981 to 0.33049, saving model to outputs/step3_2_temporal_test\\Position Only\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.8357 - loss: 0.5552 - val_accuracy: 0.9035 - val_loss: 0.3305 - learning_rate: 3.0000e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.33049\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.8393 - loss: 0.5581 - val_accuracy: 0.8960 - val_loss: 0.3496 - learning_rate: 3.0000e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss improved from 0.33049 to 0.31534, saving model to outputs/step3_2_temporal_test\\Position Only\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.8528 - loss: 0.5064 - val_accuracy: 0.9050 - val_loss: 0.3153 - learning_rate: 3.0000e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss improved from 0.31534 to 0.30260, saving model to outputs/step3_2_temporal_test\\Position Only\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.8542 - loss: 0.5050 - val_accuracy: 0.9135 - val_loss: 0.3026 - learning_rate: 3.0000e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss improved from 0.30260 to 0.27513, saving model to outputs/step3_2_temporal_test\\Position Only\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.8621 - loss: 0.4826 - val_accuracy: 0.9205 - val_loss: 0.2751 - learning_rate: 3.0000e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.27513\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.8701 - loss: 0.4591 - val_accuracy: 0.9024 - val_loss: 0.3312 - learning_rate: 3.0000e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss improved from 0.27513 to 0.25912, saving model to outputs/step3_2_temporal_test\\Position Only\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.8681 - loss: 0.4543 - val_accuracy: 0.9229 - val_loss: 0.2591 - learning_rate: 3.0000e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss improved from 0.25912 to 0.25247, saving model to outputs/step3_2_temporal_test\\Position Only\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.8775 - loss: 0.4537 - val_accuracy: 0.9252 - val_loss: 0.2525 - learning_rate: 3.0000e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss improved from 0.25247 to 0.21765, saving model to outputs/step3_2_temporal_test\\Position Only\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.8816 - loss: 0.4126 - val_accuracy: 0.9361 - val_loss: 0.2176 - learning_rate: 3.0000e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss improved from 0.21765 to 0.21211, saving model to outputs/step3_2_temporal_test\\Position Only\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.8817 - loss: 0.4081 - val_accuracy: 0.9406 - val_loss: 0.2121 - learning_rate: 3.0000e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.21211\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.8889 - loss: 0.3951 - val_accuracy: 0.9404 - val_loss: 0.2163 - learning_rate: 3.0000e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.21211\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.8940 - loss: 0.3689 - val_accuracy: 0.9425 - val_loss: 0.2132 - learning_rate: 3.0000e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss improved from 0.21211 to 0.18744, saving model to outputs/step3_2_temporal_test\\Position Only\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.9007 - loss: 0.3631 - val_accuracy: 0.9481 - val_loss: 0.1874 - learning_rate: 3.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 30.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "시계열 특징 조합 탐색: 100%|██████████| 8/8 [23:13<00:00, 174.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.5841 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "최적 시계열 특징 조합 (고속 테스트 기준): velocity (F1: 0.7116)\n",
      "{'velocity': True, 'acceleration': False, 'segment_stats': False}\n"
     ]
    }
   ],
   "source": [
    "# === tqdm: 텍스트 기반으로 고정 (ipywidgets 불필요) ===\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except Exception:\n",
    "    def tqdm(iterable=None, **kwargs):\n",
    "        return iterable\n",
    "\n",
    "# === Fold 유틸 (앞에서 정의돼 있으면 생략 가능) ===\n",
    "def norm_fold_id(fid):\n",
    "    s = str(fid)\n",
    "    return s if s.startswith(\"fold_\") else f\"fold_{int(s)}\"\n",
    "\n",
    "def fold_as_int(fid):\n",
    "    return int(str(fid).split('_')[-1])\n",
    "\n",
    "tuning_fold_id = norm_fold_id(tuning_fold_id)   # ex) 'fold_7'\n",
    "tuning_fold_int = fold_as_int(tuning_fold_id)   # ex) 7\n",
    "\n",
    "# === 시계열 특징 조합 설정 ===\n",
    "temporal_features = ['velocity', 'acceleration', 'segment_stats']\n",
    "temporal_combinations = list(itertools.product([True, False], repeat=len(temporal_features)))\n",
    "temporal_test_output_dir = 'outputs/step3_2_temporal_test'\n",
    "results_temporal = {}\n",
    "\n",
    "# === 라벨/경로 유틸 ===\n",
    "def combo_label_from_flags(names, flags, empty_label=\"Position Only\"):\n",
    "    chosen = [n for n, f in zip(names, flags) if f]\n",
    "    return \", \".join(chosen) if chosen else empty_label\n",
    "\n",
    "def safe_dirname(label: str) -> str:\n",
    "    return label.replace(\", \", \"_\").replace(\"/\", \"-\")\n",
    "\n",
    "def report_path(base_dir, label, fold_id):\n",
    "    return os.path.join(base_dir, safe_dirname(label), 'models', fold_id, 'classification_report.csv')\n",
    "\n",
    "# === 1) 모든 시계열 조합 결과 존재 여부 확인 ===\n",
    "all_results_exist = True\n",
    "for combo in temporal_combinations:\n",
    "    label = combo_label_from_flags(temporal_features, combo)\n",
    "    rp = report_path(temporal_test_output_dir, label, tuning_fold_id)\n",
    "    if not os.path.exists(rp):\n",
    "        all_results_exist = False\n",
    "        break\n",
    "\n",
    "# === 2) 결과 로드 또는 새로 실행 ===\n",
    "if all_results_exist:\n",
    "    print(f\"✅ 기존 시계열 특징 테스트 결과('{temporal_test_output_dir}')를 찾았습니다. 결과를 재사용합니다.\")\n",
    "    for combo in tqdm(temporal_combinations, desc=\"기존 결과 로드 중\"):\n",
    "        label = combo_label_from_flags(temporal_features, combo)\n",
    "        rp = report_path(temporal_test_output_dir, label, tuning_fold_id)\n",
    "        report_df = pd.read_csv(rp, index_col=0)\n",
    "        results_temporal[label] = float(report_df.loc['macro avg', 'f1-score'])\n",
    "else:\n",
    "    print(\"기존 시계열 특징 테스트 결과가 없거나 불완전합니다. 새로 테스트를 시작합니다.\")\n",
    "    for combo in tqdm(temporal_combinations, desc=\"시계열 특징 조합 탐색\"):\n",
    "        temporal_dict = dict(zip(temporal_features, combo))\n",
    "        label = combo_label_from_flags(temporal_features, combo)\n",
    "        print(f\"\\n===== 시계열 특징 조합 테스트: {label} =====\")\n",
    "\n",
    "        test_cfg = copy.deepcopy(config.MANUAL_CONFIG)\n",
    "        test_cfg['paths']['OUTPUT_DIR'] = os.path.join(temporal_test_output_dir, safe_dirname(label))\n",
    "        test_cfg['model_arch']['variant'] = best_architecture\n",
    "        test_cfg['feature_extraction']['FEATURE_CONFIG'] = best_feature_config\n",
    "        test_cfg['feature_extraction']['TEMPORAL_FEATURES'] = temporal_dict\n",
    "\n",
    "        # 고속 테스트 세팅\n",
    "        test_cfg['training']['epochs'] = 30\n",
    "        test_cfg['training']['early_stopping_patience'] = 5\n",
    "\n",
    "        # run_single_fold는 정수 fold 사용\n",
    "        f1 = run_single_fold(test_cfg, fold_id=tuning_fold_int, data_subset_fraction=0.3)\n",
    "        results_temporal[label] = f1\n",
    "\n",
    "# === 3) 시각화 및 최적 조합 저장 ===\n",
    "if not results_temporal:\n",
    "    raise RuntimeError(\"시계열 특징 조합 결과가 비어 있습니다. 상위 단계 실행을 확인하세요.\")\n",
    "\n",
    "results_df = (\n",
    "    pd.DataFrame(list(results_temporal.items()), columns=['Temporal Combo', 'F1 Score'])\n",
    "    .sort_values('F1 Score', ascending=False)\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='F1 Score', y='Temporal Combo', data=results_df, orient='h',\n",
    "            order=results_df['Temporal Combo'])\n",
    "plt.title(f'시계열 특징 조합별 성능 비교 ({tuning_fold_id}, 고속 테스트)')\n",
    "plt.xlabel('Macro F1 Score')\n",
    "plt.ylabel('Temporal Combination')\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "best_temporal_combo_name = results_df.iloc[0]['Temporal Combo']\n",
    "selected_temporal = [] if best_temporal_combo_name == \"Position Only\" else best_temporal_combo_name.split(\", \")\n",
    "best_temporal_config = {f: (f in selected_temporal) for f in temporal_features}\n",
    "\n",
    "print(f\"\\n최적 시계열 특징 조합 (고속 테스트 기준): {best_temporal_combo_name} \"\n",
    "      f\"(F1: {results_df.iloc[0]['F1 Score']:.4f})\")\n",
    "print(best_temporal_config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3f54a2",
   "metadata": {},
   "source": [
    "# # 최종 하이퍼파라미터 튜닝 (Optuna)\n",
    "\n",
    "🔍 Optuna 기반 단일 Fold 하이퍼파라미터 탐색\n",
    "\n",
    "이전 단계에서 고정한 아키텍처 / 특징 조합을 사용하고,\n",
    "**기준 폴드({tuning_fold_id})**에서 데이터 30% 서브셋으로 빠르게 Optuna 탐색을 수행합니다.\n",
    "\n",
    "⚙️ 구성\n",
    "\n",
    "베이스 설정:\n",
    "\n",
    "model_arch.variant = {best_architecture}\n",
    "\n",
    "FEATURE_CONFIG = best_feature_config\n",
    "\n",
    "TEMPORAL_FEATURES = best_temporal_config\n",
    "\n",
    "탐색 대상: config.define_search_space(trial)에서 정의\n",
    "\n",
    "실행 전략:\n",
    "\n",
    "run_single_fold(cfg, fold_id={fold_as_int(tuning_fold_id)}, data_subset_fraction=0.3)\n",
    "\n",
    "epochs=30, early_stopping_patience=5 (고속)\n",
    "\n",
    "저장 위치:\n",
    "\n",
    "각 Trial: outputs/step3_3_optuna/trial_<N>/\n",
    "\n",
    "Study DB: outputs/optuna_study.db (SQLite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f94bda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 18:05:12,866] A new study created in RDB with name: Single_Fold_HPO_v2\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "특징 추출(병렬): 100%|██████████| 1111/1111 [00:15<00:00, 69.93it/s] \n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "청크 병합: 100%|██████████| 1111/1111 [00:10<00:00, 110.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터가 9972개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 1.88404, saving model to outputs/step3_3_optuna\\trial_0\\models\\fold_7\\best_model.keras\n",
      "78/78 - 7s - 91ms/step - accuracy: 0.2082 - loss: 2.4121 - val_accuracy: 0.4043 - val_loss: 1.8840 - learning_rate: 1.3818e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 1.88404 to 1.60561, saving model to outputs/step3_3_optuna\\trial_0\\models\\fold_7\\best_model.keras\n",
      "78/78 - 5s - 61ms/step - accuracy: 0.3342 - loss: 2.0047 - val_accuracy: 0.4846 - val_loss: 1.6056 - learning_rate: 1.3818e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 1.60561 to 1.38413, saving model to outputs/step3_3_optuna\\trial_0\\models\\fold_7\\best_model.keras\n",
      "78/78 - 5s - 62ms/step - accuracy: 0.4033 - loss: 1.7936 - val_accuracy: 0.5724 - val_loss: 1.3841 - learning_rate: 1.3818e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 1.38413 to 1.24104, saving model to outputs/step3_3_optuna\\trial_0\\models\\fold_7\\best_model.keras\n",
      "78/78 - 5s - 62ms/step - accuracy: 0.4649 - loss: 1.6405 - val_accuracy: 0.6112 - val_loss: 1.2410 - learning_rate: 1.3818e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 1.24104 to 1.11671, saving model to outputs/step3_3_optuna\\trial_0\\models\\fold_7\\best_model.keras\n",
      "78/78 - 5s - 63ms/step - accuracy: 0.5170 - loss: 1.5109 - val_accuracy: 0.6652 - val_loss: 1.1167 - learning_rate: 1.3818e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 1.11671 to 0.98146, saving model to outputs/step3_3_optuna\\trial_0\\models\\fold_7\\best_model.keras\n",
      "78/78 - 5s - 63ms/step - accuracy: 0.5532 - loss: 1.3855 - val_accuracy: 0.7125 - val_loss: 0.9815 - learning_rate: 1.3818e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.98146 to 0.85832, saving model to outputs/step3_3_optuna\\trial_0\\models\\fold_7\\best_model.keras\n",
      "78/78 - 5s - 62ms/step - accuracy: 0.6052 - loss: 1.2554 - val_accuracy: 0.7525 - val_loss: 0.8583 - learning_rate: 1.3818e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 0.85832 to 0.76371, saving model to outputs/step3_3_optuna\\trial_0\\models\\fold_7\\best_model.keras\n",
      "78/78 - 5s - 62ms/step - accuracy: 0.6371 - loss: 1.1601 - val_accuracy: 0.7810 - val_loss: 0.7637 - learning_rate: 1.3818e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.76371 to 0.65043, saving model to outputs/step3_3_optuna\\trial_0\\models\\fold_7\\best_model.keras\n",
      "78/78 - 5s - 67ms/step - accuracy: 0.6765 - loss: 1.0529 - val_accuracy: 0.8210 - val_loss: 0.6504 - learning_rate: 1.3818e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 0.65043 to 0.56459, saving model to outputs/step3_3_optuna\\trial_0\\models\\fold_7\\best_model.keras\n",
      "78/78 - 5s - 61ms/step - accuracy: 0.7134 - loss: 0.9566 - val_accuracy: 0.8459 - val_loss: 0.5646 - learning_rate: 1.3818e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 0.56459 to 0.51136, saving model to outputs/step3_3_optuna\\trial_0\\models\\fold_7\\best_model.keras\n",
      "78/78 - 5s - 63ms/step - accuracy: 0.7402 - loss: 0.8775 - val_accuracy: 0.8630 - val_loss: 0.5114 - learning_rate: 1.3818e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 0.51136 to 0.45571, saving model to outputs/step3_3_optuna\\trial_0\\models\\fold_7\\best_model.keras\n",
      "78/78 - 5s - 63ms/step - accuracy: 0.7550 - loss: 0.8258 - val_accuracy: 0.8713 - val_loss: 0.4557 - learning_rate: 1.3818e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 0.45571 to 0.41876, saving model to outputs/step3_3_optuna\\trial_0\\models\\fold_7\\best_model.keras\n",
      "78/78 - 5s - 64ms/step - accuracy: 0.7752 - loss: 0.7644 - val_accuracy: 0.8890 - val_loss: 0.4188 - learning_rate: 1.3818e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 0.41876 to 0.37874, saving model to outputs/step3_3_optuna\\trial_0\\models\\fold_7\\best_model.keras\n",
      "78/78 - 5s - 63ms/step - accuracy: 0.7895 - loss: 0.7141 - val_accuracy: 0.8962 - val_loss: 0.3787 - learning_rate: 1.3818e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss improved from 0.37874 to 0.34606, saving model to outputs/step3_3_optuna\\trial_0\\models\\fold_7\\best_model.keras\n",
      "78/78 - 5s - 63ms/step - accuracy: 0.8051 - loss: 0.6704 - val_accuracy: 0.9054 - val_loss: 0.3461 - learning_rate: 1.3818e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss improved from 0.34606 to 0.31210, saving model to outputs/step3_3_optuna\\trial_0\\models\\fold_7\\best_model.keras\n",
      "78/78 - 5s - 64ms/step - accuracy: 0.8209 - loss: 0.6255 - val_accuracy: 0.9155 - val_loss: 0.3121 - learning_rate: 1.3818e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 0.31210 to 0.28878, saving model to outputs/step3_3_optuna\\trial_0\\models\\fold_7\\best_model.keras\n",
      "78/78 - 5s - 63ms/step - accuracy: 0.8359 - loss: 0.5772 - val_accuracy: 0.9223 - val_loss: 0.2888 - learning_rate: 1.3818e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss improved from 0.28878 to 0.26859, saving model to outputs/step3_3_optuna\\trial_0\\models\\fold_7\\best_model.keras\n",
      "78/78 - 5s - 64ms/step - accuracy: 0.8402 - loss: 0.5532 - val_accuracy: 0.9294 - val_loss: 0.2686 - learning_rate: 1.3818e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss improved from 0.26859 to 0.25016, saving model to outputs/step3_3_optuna\\trial_0\\models\\fold_7\\best_model.keras\n",
      "78/78 - 5s - 64ms/step - accuracy: 0.8494 - loss: 0.5374 - val_accuracy: 0.9318 - val_loss: 0.2502 - learning_rate: 1.3818e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss improved from 0.25016 to 0.22837, saving model to outputs/step3_3_optuna\\trial_0\\models\\fold_7\\best_model.keras\n",
      "78/78 - 5s - 64ms/step - accuracy: 0.8669 - loss: 0.4870 - val_accuracy: 0.9403 - val_loss: 0.2284 - learning_rate: 1.3818e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss improved from 0.22837 to 0.21998, saving model to outputs/step3_3_optuna\\trial_0\\models\\fold_7\\best_model.keras\n",
      "78/78 - 5s - 64ms/step - accuracy: 0.8665 - loss: 0.4823 - val_accuracy: 0.9420 - val_loss: 0.2200 - learning_rate: 1.3818e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss improved from 0.21998 to 0.20595, saving model to outputs/step3_3_optuna\\trial_0\\models\\fold_7\\best_model.keras\n",
      "78/78 - 6s - 72ms/step - accuracy: 0.8756 - loss: 0.4570 - val_accuracy: 0.9438 - val_loss: 0.2059 - learning_rate: 1.3818e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss improved from 0.20595 to 0.18864, saving model to outputs/step3_3_optuna\\trial_0\\models\\fold_7\\best_model.keras\n",
      "78/78 - 7s - 95ms/step - accuracy: 0.8787 - loss: 0.4350 - val_accuracy: 0.9494 - val_loss: 0.1886 - learning_rate: 1.3818e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss improved from 0.18864 to 0.18629, saving model to outputs/step3_3_optuna\\trial_0\\models\\fold_7\\best_model.keras\n",
      "78/78 - 7s - 94ms/step - accuracy: 0.8869 - loss: 0.4156 - val_accuracy: 0.9472 - val_loss: 0.1863 - learning_rate: 1.3818e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss improved from 0.18629 to 0.16508, saving model to outputs/step3_3_optuna\\trial_0\\models\\fold_7\\best_model.keras\n",
      "78/78 - 5s - 65ms/step - accuracy: 0.8940 - loss: 0.3983 - val_accuracy: 0.9569 - val_loss: 0.1651 - learning_rate: 1.3818e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss improved from 0.16508 to 0.16188, saving model to outputs/step3_3_optuna\\trial_0\\models\\fold_7\\best_model.keras\n",
      "78/78 - 8s - 105ms/step - accuracy: 0.8959 - loss: 0.3790 - val_accuracy: 0.9560 - val_loss: 0.1619 - learning_rate: 1.3818e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss improved from 0.16188 to 0.14762, saving model to outputs/step3_3_optuna\\trial_0\\models\\fold_7\\best_model.keras\n",
      "78/78 - 11s - 137ms/step - accuracy: 0.9047 - loss: 0.3588 - val_accuracy: 0.9608 - val_loss: 0.1476 - learning_rate: 1.3818e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss improved from 0.14762 to 0.13944, saving model to outputs/step3_3_optuna\\trial_0\\models\\fold_7\\best_model.keras\n",
      "78/78 - 10s - 134ms/step - accuracy: 0.9082 - loss: 0.3476 - val_accuracy: 0.9625 - val_loss: 0.1394 - learning_rate: 1.3818e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss improved from 0.13944 to 0.13778, saving model to outputs/step3_3_optuna\\trial_0\\models\\fold_7\\best_model.keras\n",
      "78/78 - 10s - 133ms/step - accuracy: 0.9133 - loss: 0.3270 - val_accuracy: 0.9635 - val_loss: 0.1378 - learning_rate: 1.3818e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss improved from 0.13778 to 0.12254, saving model to outputs/step3_3_optuna\\trial_0\\models\\fold_7\\best_model.keras\n",
      "78/78 - 9s - 117ms/step - accuracy: 0.9110 - loss: 0.3296 - val_accuracy: 0.9705 - val_loss: 0.1225 - learning_rate: 1.3818e-04\n",
      "Restoring model weights from the end of the best epoch: 30.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 18:08:53,071] Trial 0 finished with value: 0.4985587857039703 and parameters: {'WINDOW_SIZE': 45, 'STRIDE': 3, 'gru1_units': 256, 'gru2_units': 64, 'dense_units': 32, 'dropout_rate': 0.34888210283451576, 'l2': 4.183005726872174e-05, 'cf_filters': 64, 'cf_kernel': 7, 'cf_pool': 2, 'cf_drop': 0.0, 'cf_l2': 9.358328862556192e-05, 'learning_rate': 0.00013817747664768122, 'batch_size': 128}. Best is trial 0 with value: 0.4985587857039703.\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.4986 ---\n",
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "특징 추출(병렬): 100%|██████████| 1111/1111 [00:30<00:00, 36.67it/s] \n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "청크 병합: 100%|██████████| 1111/1111 [00:13<00:00, 80.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터가 6924개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 1.45524, saving model to outputs/step3_3_optuna\\trial_1\\models\\fold_7\\best_model.keras\n",
      "109/109 - 9s - 85ms/step - accuracy: 0.3018 - loss: 2.0755 - val_accuracy: 0.5604 - val_loss: 1.4552 - learning_rate: 6.5982e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 1.45524 to 0.93679, saving model to outputs/step3_3_optuna\\trial_1\\models\\fold_7\\best_model.keras\n",
      "109/109 - 4s - 33ms/step - accuracy: 0.4896 - loss: 1.5354 - val_accuracy: 0.7280 - val_loss: 0.9368 - learning_rate: 6.5982e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 0.93679 to 0.69794, saving model to outputs/step3_3_optuna\\trial_1\\models\\fold_7\\best_model.keras\n",
      "109/109 - 2s - 20ms/step - accuracy: 0.6083 - loss: 1.1942 - val_accuracy: 0.7833 - val_loss: 0.6979 - learning_rate: 6.5982e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 0.69794 to 0.59933, saving model to outputs/step3_3_optuna\\trial_1\\models\\fold_7\\best_model.keras\n",
      "109/109 - 2s - 20ms/step - accuracy: 0.7039 - loss: 0.9470 - val_accuracy: 0.8144 - val_loss: 0.5993 - learning_rate: 6.5982e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 0.59933 to 0.47213, saving model to outputs/step3_3_optuna\\trial_1\\models\\fold_7\\best_model.keras\n",
      "109/109 - 2s - 20ms/step - accuracy: 0.7491 - loss: 0.8192 - val_accuracy: 0.8594 - val_loss: 0.4721 - learning_rate: 6.5982e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 0.47213 to 0.42507, saving model to outputs/step3_3_optuna\\trial_1\\models\\fold_7\\best_model.keras\n",
      "109/109 - 2s - 21ms/step - accuracy: 0.7773 - loss: 0.7250 - val_accuracy: 0.8711 - val_loss: 0.4251 - learning_rate: 6.5982e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.42507 to 0.40754, saving model to outputs/step3_3_optuna\\trial_1\\models\\fold_7\\best_model.keras\n",
      "109/109 - 2s - 21ms/step - accuracy: 0.7955 - loss: 0.6683 - val_accuracy: 0.8743 - val_loss: 0.4075 - learning_rate: 6.5982e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 0.40754 to 0.32693, saving model to outputs/step3_3_optuna\\trial_1\\models\\fold_7\\best_model.keras\n",
      "109/109 - 2s - 22ms/step - accuracy: 0.8124 - loss: 0.6134 - val_accuracy: 0.8999 - val_loss: 0.3269 - learning_rate: 6.5982e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.32693 to 0.32419, saving model to outputs/step3_3_optuna\\trial_1\\models\\fold_7\\best_model.keras\n",
      "109/109 - 2s - 23ms/step - accuracy: 0.8323 - loss: 0.5557 - val_accuracy: 0.8935 - val_loss: 0.3242 - learning_rate: 6.5982e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.32419\n",
      "109/109 - 2s - 21ms/step - accuracy: 0.8458 - loss: 0.5237 - val_accuracy: 0.8832 - val_loss: 0.3500 - learning_rate: 6.5982e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 0.32419 to 0.26580, saving model to outputs/step3_3_optuna\\trial_1\\models\\fold_7\\best_model.keras\n",
      "109/109 - 2s - 21ms/step - accuracy: 0.8436 - loss: 0.5080 - val_accuracy: 0.9156 - val_loss: 0.2658 - learning_rate: 6.5982e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 0.26580 to 0.25279, saving model to outputs/step3_3_optuna\\trial_1\\models\\fold_7\\best_model.keras\n",
      "109/109 - 2s - 21ms/step - accuracy: 0.8537 - loss: 0.4811 - val_accuracy: 0.9185 - val_loss: 0.2528 - learning_rate: 6.5982e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 0.25279 to 0.20277, saving model to outputs/step3_3_optuna\\trial_1\\models\\fold_7\\best_model.keras\n",
      "109/109 - 2s - 21ms/step - accuracy: 0.8778 - loss: 0.4187 - val_accuracy: 0.9337 - val_loss: 0.2028 - learning_rate: 6.5982e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 0.20277 to 0.19973, saving model to outputs/step3_3_optuna\\trial_1\\models\\fold_7\\best_model.keras\n",
      "109/109 - 2s - 21ms/step - accuracy: 0.8806 - loss: 0.4045 - val_accuracy: 0.9384 - val_loss: 0.1997 - learning_rate: 6.5982e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.19973\n",
      "109/109 - 2s - 20ms/step - accuracy: 0.8904 - loss: 0.3606 - val_accuracy: 0.9367 - val_loss: 0.2021 - learning_rate: 6.5982e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss improved from 0.19973 to 0.18258, saving model to outputs/step3_3_optuna\\trial_1\\models\\fold_7\\best_model.keras\n",
      "109/109 - 2s - 22ms/step - accuracy: 0.8917 - loss: 0.3696 - val_accuracy: 0.9431 - val_loss: 0.1826 - learning_rate: 6.5982e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.18258\n",
      "109/109 - 2s - 20ms/step - accuracy: 0.8956 - loss: 0.3543 - val_accuracy: 0.9433 - val_loss: 0.1834 - learning_rate: 6.5982e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss improved from 0.18258 to 0.15789, saving model to outputs/step3_3_optuna\\trial_1\\models\\fold_7\\best_model.keras\n",
      "109/109 - 2s - 22ms/step - accuracy: 0.9015 - loss: 0.3239 - val_accuracy: 0.9475 - val_loss: 0.1579 - learning_rate: 6.5982e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss improved from 0.15789 to 0.12932, saving model to outputs/step3_3_optuna\\trial_1\\models\\fold_7\\best_model.keras\n",
      "109/109 - 2s - 21ms/step - accuracy: 0.9084 - loss: 0.3123 - val_accuracy: 0.9634 - val_loss: 0.1293 - learning_rate: 6.5982e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.12932\n",
      "109/109 - 2s - 21ms/step - accuracy: 0.9097 - loss: 0.3127 - val_accuracy: 0.9644 - val_loss: 0.1307 - learning_rate: 6.5982e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.12932\n",
      "109/109 - 2s - 21ms/step - accuracy: 0.9181 - loss: 0.2945 - val_accuracy: 0.9595 - val_loss: 0.1337 - learning_rate: 6.5982e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.12932\n",
      "109/109 - 2s - 21ms/step - accuracy: 0.9242 - loss: 0.2604 - val_accuracy: 0.9595 - val_loss: 0.1293 - learning_rate: 6.5982e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss improved from 0.12932 to 0.09668, saving model to outputs/step3_3_optuna\\trial_1\\models\\fold_7\\best_model.keras\n",
      "109/109 - 2s - 21ms/step - accuracy: 0.9198 - loss: 0.2764 - val_accuracy: 0.9755 - val_loss: 0.0967 - learning_rate: 6.5982e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.09668\n",
      "109/109 - 2s - 21ms/step - accuracy: 0.9323 - loss: 0.2439 - val_accuracy: 0.9705 - val_loss: 0.1080 - learning_rate: 6.5982e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.09668\n",
      "109/109 - 2s - 21ms/step - accuracy: 0.9206 - loss: 0.2646 - val_accuracy: 0.9624 - val_loss: 0.1216 - learning_rate: 6.5982e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.09668\n",
      "109/109 - 2s - 21ms/step - accuracy: 0.9365 - loss: 0.2288 - val_accuracy: 0.9713 - val_loss: 0.1089 - learning_rate: 6.5982e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss improved from 0.09668 to 0.08942, saving model to outputs/step3_3_optuna\\trial_1\\models\\fold_7\\best_model.keras\n",
      "109/109 - 2s - 21ms/step - accuracy: 0.9372 - loss: 0.2269 - val_accuracy: 0.9745 - val_loss: 0.0894 - learning_rate: 6.5982e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss improved from 0.08942 to 0.08623, saving model to outputs/step3_3_optuna\\trial_1\\models\\fold_7\\best_model.keras\n",
      "109/109 - 2s - 21ms/step - accuracy: 0.9434 - loss: 0.2112 - val_accuracy: 0.9774 - val_loss: 0.0862 - learning_rate: 6.5982e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss improved from 0.08623 to 0.08536, saving model to outputs/step3_3_optuna\\trial_1\\models\\fold_7\\best_model.keras\n",
      "109/109 - 2s - 22ms/step - accuracy: 0.9454 - loss: 0.2055 - val_accuracy: 0.9786 - val_loss: 0.0854 - learning_rate: 6.5982e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.08536\n",
      "109/109 - 2s - 21ms/step - accuracy: 0.9500 - loss: 0.1869 - val_accuracy: 0.9784 - val_loss: 0.0866 - learning_rate: 6.5982e-04\n",
      "Restoring model weights from the end of the best epoch: 29.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 18:11:09,882] Trial 1 finished with value: 0.5268810847635079 and parameters: {'WINDOW_SIZE': 30, 'STRIDE': 5, 'gru1_units': 160, 'gru2_units': 64, 'dense_units': 64, 'dropout_rate': 0.45845788468053517, 'l2': 1.3790211087895705e-05, 'cf_filters': 48, 'cf_kernel': 5, 'cf_pool': 2, 'cf_drop': 0.1, 'cf_l2': 0.00011241734941679379, 'learning_rate': 0.0006598191962716286, 'batch_size': 64}. Best is trial 1 with value: 0.5268810847635079.\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.5269 ---\n",
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "특징 추출(병렬): 100%|██████████| 1111/1111 [00:19<00:00, 57.42it/s] \n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "청크 병합: 100%|██████████| 1111/1111 [00:11<00:00, 97.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터가 29365개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 0.58684, saving model to outputs/step3_3_optuna\\trial_2\\models\\fold_7\\best_model.keras\n",
      "306/306 - 6s - 20ms/step - accuracy: 0.4873 - loss: 1.5437 - val_accuracy: 0.8380 - val_loss: 0.5868 - learning_rate: 6.1373e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 0.58684 to 0.28566, saving model to outputs/step3_3_optuna\\trial_2\\models\\fold_7\\best_model.keras\n",
      "306/306 - 4s - 14ms/step - accuracy: 0.7662 - loss: 0.7184 - val_accuracy: 0.9090 - val_loss: 0.2857 - learning_rate: 6.1373e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 0.28566 to 0.19404, saving model to outputs/step3_3_optuna\\trial_2\\models\\fold_7\\best_model.keras\n",
      "306/306 - 4s - 15ms/step - accuracy: 0.8546 - loss: 0.4591 - val_accuracy: 0.9378 - val_loss: 0.1940 - learning_rate: 6.1373e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 0.19404 to 0.13163, saving model to outputs/step3_3_optuna\\trial_2\\models\\fold_7\\best_model.keras\n",
      "306/306 - 4s - 14ms/step - accuracy: 0.8937 - loss: 0.3415 - val_accuracy: 0.9583 - val_loss: 0.1316 - learning_rate: 6.1373e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 0.13163 to 0.10890, saving model to outputs/step3_3_optuna\\trial_2\\models\\fold_7\\best_model.keras\n",
      "306/306 - 4s - 14ms/step - accuracy: 0.9163 - loss: 0.2761 - val_accuracy: 0.9659 - val_loss: 0.1089 - learning_rate: 6.1373e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 0.10890 to 0.08558, saving model to outputs/step3_3_optuna\\trial_2\\models\\fold_7\\best_model.keras\n",
      "306/306 - 4s - 14ms/step - accuracy: 0.9315 - loss: 0.2298 - val_accuracy: 0.9727 - val_loss: 0.0856 - learning_rate: 6.1373e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.08558 to 0.05704, saving model to outputs/step3_3_optuna\\trial_2\\models\\fold_7\\best_model.keras\n",
      "306/306 - 4s - 14ms/step - accuracy: 0.9444 - loss: 0.1902 - val_accuracy: 0.9838 - val_loss: 0.0570 - learning_rate: 6.1373e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.05704\n",
      "306/306 - 4s - 14ms/step - accuracy: 0.9501 - loss: 0.1728 - val_accuracy: 0.9767 - val_loss: 0.0750 - learning_rate: 6.1373e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.05704 to 0.04753, saving model to outputs/step3_3_optuna\\trial_2\\models\\fold_7\\best_model.keras\n",
      "306/306 - 4s - 14ms/step - accuracy: 0.9588 - loss: 0.1460 - val_accuracy: 0.9863 - val_loss: 0.0475 - learning_rate: 6.1373e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 0.04753 to 0.04049, saving model to outputs/step3_3_optuna\\trial_2\\models\\fold_7\\best_model.keras\n",
      "306/306 - 4s - 14ms/step - accuracy: 0.9603 - loss: 0.1409 - val_accuracy: 0.9893 - val_loss: 0.0405 - learning_rate: 6.1373e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 0.04049 to 0.03067, saving model to outputs/step3_3_optuna\\trial_2\\models\\fold_7\\best_model.keras\n",
      "306/306 - 4s - 14ms/step - accuracy: 0.9666 - loss: 0.1252 - val_accuracy: 0.9939 - val_loss: 0.0307 - learning_rate: 6.1373e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.03067\n",
      "306/306 - 4s - 14ms/step - accuracy: 0.9683 - loss: 0.1151 - val_accuracy: 0.9931 - val_loss: 0.0311 - learning_rate: 6.1373e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 0.03067 to 0.02683, saving model to outputs/step3_3_optuna\\trial_2\\models\\fold_7\\best_model.keras\n",
      "306/306 - 4s - 14ms/step - accuracy: 0.9722 - loss: 0.1085 - val_accuracy: 0.9951 - val_loss: 0.0268 - learning_rate: 6.1373e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 0.02683 to 0.02210, saving model to outputs/step3_3_optuna\\trial_2\\models\\fold_7\\best_model.keras\n",
      "306/306 - 4s - 14ms/step - accuracy: 0.9749 - loss: 0.0963 - val_accuracy: 0.9963 - val_loss: 0.0221 - learning_rate: 6.1373e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.02210\n",
      "306/306 - 4s - 14ms/step - accuracy: 0.9769 - loss: 0.0909 - val_accuracy: 0.9944 - val_loss: 0.0279 - learning_rate: 6.1373e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.02210\n",
      "306/306 - 4s - 14ms/step - accuracy: 0.9765 - loss: 0.0910 - val_accuracy: 0.9961 - val_loss: 0.0221 - learning_rate: 6.1373e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.02210\n",
      "306/306 - 4s - 14ms/step - accuracy: 0.9804 - loss: 0.0804 - val_accuracy: 0.9971 - val_loss: 0.0228 - learning_rate: 6.1373e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss improved from 0.02210 to 0.02106, saving model to outputs/step3_3_optuna\\trial_2\\models\\fold_7\\best_model.keras\n",
      "306/306 - 4s - 14ms/step - accuracy: 0.9817 - loss: 0.0746 - val_accuracy: 0.9964 - val_loss: 0.0211 - learning_rate: 6.1373e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss improved from 0.02106 to 0.02062, saving model to outputs/step3_3_optuna\\trial_2\\models\\fold_7\\best_model.keras\n",
      "306/306 - 4s - 14ms/step - accuracy: 0.9814 - loss: 0.0768 - val_accuracy: 0.9968 - val_loss: 0.0206 - learning_rate: 6.1373e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.02062\n",
      "306/306 - 4s - 14ms/step - accuracy: 0.9828 - loss: 0.0715 - val_accuracy: 0.9962 - val_loss: 0.0238 - learning_rate: 6.1373e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.02062\n",
      "306/306 - 4s - 14ms/step - accuracy: 0.9838 - loss: 0.0707 - val_accuracy: 0.9960 - val_loss: 0.0237 - learning_rate: 6.1373e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.02062\n",
      "306/306 - 4s - 14ms/step - accuracy: 0.9822 - loss: 0.0750 - val_accuracy: 0.9929 - val_loss: 0.0314 - learning_rate: 6.1373e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss improved from 0.02062 to 0.01793, saving model to outputs/step3_3_optuna\\trial_2\\models\\fold_7\\best_model.keras\n",
      "306/306 - 4s - 14ms/step - accuracy: 0.9821 - loss: 0.0731 - val_accuracy: 0.9983 - val_loss: 0.0179 - learning_rate: 6.1373e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.01793\n",
      "306/306 - 4s - 14ms/step - accuracy: 0.9836 - loss: 0.0708 - val_accuracy: 0.9975 - val_loss: 0.0200 - learning_rate: 6.1373e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss improved from 0.01793 to 0.01683, saving model to outputs/step3_3_optuna\\trial_2\\models\\fold_7\\best_model.keras\n",
      "306/306 - 4s - 14ms/step - accuracy: 0.9842 - loss: 0.0676 - val_accuracy: 0.9982 - val_loss: 0.0168 - learning_rate: 6.1373e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.01683\n",
      "306/306 - 4s - 14ms/step - accuracy: 0.9860 - loss: 0.0622 - val_accuracy: 0.9983 - val_loss: 0.0171 - learning_rate: 6.1373e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.01683\n",
      "306/306 - 4s - 14ms/step - accuracy: 0.9874 - loss: 0.0561 - val_accuracy: 0.9981 - val_loss: 0.0188 - learning_rate: 6.1373e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss improved from 0.01683 to 0.01621, saving model to outputs/step3_3_optuna\\trial_2\\models\\fold_7\\best_model.keras\n",
      "306/306 - 4s - 14ms/step - accuracy: 0.9870 - loss: 0.0598 - val_accuracy: 0.9987 - val_loss: 0.0162 - learning_rate: 6.1373e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.01621\n",
      "306/306 - 4s - 14ms/step - accuracy: 0.9874 - loss: 0.0571 - val_accuracy: 0.9953 - val_loss: 0.0248 - learning_rate: 6.1373e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.01621\n",
      "306/306 - 4s - 14ms/step - accuracy: 0.9872 - loss: 0.0579 - val_accuracy: 0.9984 - val_loss: 0.0171 - learning_rate: 6.1373e-04\n",
      "Restoring model weights from the end of the best epoch: 28.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 18:14:10,421] Trial 2 finished with value: 0.6388116314073684 and parameters: {'WINDOW_SIZE': 45, 'STRIDE': 1, 'gru1_units': 96, 'gru2_units': 0, 'dense_units': 64, 'dropout_rate': 0.5177684114485722, 'l2': 1.877009946833477e-05, 'cf_filters': 64, 'cf_kernel': 3, 'cf_pool': 3, 'cf_drop': 0.1, 'cf_l2': 3.564658329261417e-05, 'learning_rate': 0.0006137308080007238, 'batch_size': 96}. Best is trial 2 with value: 0.6388116314073684.\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.6388 ---\n",
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "특징 추출(병렬): 100%|██████████| 1111/1111 [00:19<00:00, 57.60it/s] \n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "청크 병합: 100%|██████████| 1111/1111 [00:12<00:00, 91.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터가 33515개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 1.20714, saving model to outputs/step3_3_optuna\\trial_3\\models\\fold_7\\best_model.keras\n",
      "262/262 - 10s - 37ms/step - accuracy: 0.3780 - loss: 1.9536 - val_accuracy: 0.6288 - val_loss: 1.2071 - learning_rate: 1.9337e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 1.20714 to 0.72644, saving model to outputs/step3_3_optuna\\trial_3\\models\\fold_7\\best_model.keras\n",
      "262/262 - 7s - 28ms/step - accuracy: 0.6091 - loss: 1.2681 - val_accuracy: 0.7908 - val_loss: 0.7264 - learning_rate: 1.9337e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 0.72644 to 0.50244, saving model to outputs/step3_3_optuna\\trial_3\\models\\fold_7\\best_model.keras\n",
      "262/262 - 7s - 28ms/step - accuracy: 0.7398 - loss: 0.8856 - val_accuracy: 0.8683 - val_loss: 0.5024 - learning_rate: 1.9337e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 0.50244 to 0.39152, saving model to outputs/step3_3_optuna\\trial_3\\models\\fold_7\\best_model.keras\n",
      "262/262 - 7s - 28ms/step - accuracy: 0.8089 - loss: 0.6859 - val_accuracy: 0.8998 - val_loss: 0.3915 - learning_rate: 1.9337e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 0.39152 to 0.30413, saving model to outputs/step3_3_optuna\\trial_3\\models\\fold_7\\best_model.keras\n",
      "262/262 - 7s - 28ms/step - accuracy: 0.8512 - loss: 0.5522 - val_accuracy: 0.9275 - val_loss: 0.3041 - learning_rate: 1.9337e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 0.30413 to 0.24821, saving model to outputs/step3_3_optuna\\trial_3\\models\\fold_7\\best_model.keras\n",
      "262/262 - 7s - 28ms/step - accuracy: 0.8821 - loss: 0.4556 - val_accuracy: 0.9462 - val_loss: 0.2482 - learning_rate: 1.9337e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.24821 to 0.21286, saving model to outputs/step3_3_optuna\\trial_3\\models\\fold_7\\best_model.keras\n",
      "262/262 - 7s - 28ms/step - accuracy: 0.8992 - loss: 0.3984 - val_accuracy: 0.9572 - val_loss: 0.2129 - learning_rate: 1.9337e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 0.21286 to 0.18556, saving model to outputs/step3_3_optuna\\trial_3\\models\\fold_7\\best_model.keras\n",
      "262/262 - 7s - 28ms/step - accuracy: 0.9128 - loss: 0.3542 - val_accuracy: 0.9675 - val_loss: 0.1856 - learning_rate: 1.9337e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.18556 to 0.16804, saving model to outputs/step3_3_optuna\\trial_3\\models\\fold_7\\best_model.keras\n",
      "262/262 - 9s - 33ms/step - accuracy: 0.9262 - loss: 0.3160 - val_accuracy: 0.9703 - val_loss: 0.1680 - learning_rate: 1.9337e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 0.16804 to 0.14799, saving model to outputs/step3_3_optuna\\trial_3\\models\\fold_7\\best_model.keras\n",
      "262/262 - 11s - 40ms/step - accuracy: 0.9347 - loss: 0.2879 - val_accuracy: 0.9772 - val_loss: 0.1480 - learning_rate: 1.9337e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 0.14799 to 0.13673, saving model to outputs/step3_3_optuna\\trial_3\\models\\fold_7\\best_model.keras\n",
      "262/262 - 10s - 38ms/step - accuracy: 0.9408 - loss: 0.2635 - val_accuracy: 0.9814 - val_loss: 0.1367 - learning_rate: 1.9337e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 0.13673 to 0.12400, saving model to outputs/step3_3_optuna\\trial_3\\models\\fold_7\\best_model.keras\n",
      "262/262 - 7s - 28ms/step - accuracy: 0.9455 - loss: 0.2484 - val_accuracy: 0.9841 - val_loss: 0.1240 - learning_rate: 1.9337e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 0.12400 to 0.11828, saving model to outputs/step3_3_optuna\\trial_3\\models\\fold_7\\best_model.keras\n",
      "262/262 - 7s - 28ms/step - accuracy: 0.9519 - loss: 0.2284 - val_accuracy: 0.9855 - val_loss: 0.1183 - learning_rate: 1.9337e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 0.11828 to 0.11174, saving model to outputs/step3_3_optuna\\trial_3\\models\\fold_7\\best_model.keras\n",
      "262/262 - 7s - 28ms/step - accuracy: 0.9537 - loss: 0.2166 - val_accuracy: 0.9884 - val_loss: 0.1117 - learning_rate: 1.9337e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss improved from 0.11174 to 0.10430, saving model to outputs/step3_3_optuna\\trial_3\\models\\fold_7\\best_model.keras\n",
      "262/262 - 7s - 28ms/step - accuracy: 0.9570 - loss: 0.2087 - val_accuracy: 0.9901 - val_loss: 0.1043 - learning_rate: 1.9337e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.10430\n",
      "262/262 - 7s - 28ms/step - accuracy: 0.9594 - loss: 0.1961 - val_accuracy: 0.9887 - val_loss: 0.1068 - learning_rate: 1.9337e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 0.10430 to 0.10031, saving model to outputs/step3_3_optuna\\trial_3\\models\\fold_7\\best_model.keras\n",
      "262/262 - 7s - 28ms/step - accuracy: 0.9634 - loss: 0.1882 - val_accuracy: 0.9900 - val_loss: 0.1003 - learning_rate: 1.9337e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss improved from 0.10031 to 0.09601, saving model to outputs/step3_3_optuna\\trial_3\\models\\fold_7\\best_model.keras\n",
      "262/262 - 7s - 28ms/step - accuracy: 0.9658 - loss: 0.1807 - val_accuracy: 0.9911 - val_loss: 0.0960 - learning_rate: 1.9337e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss improved from 0.09601 to 0.08948, saving model to outputs/step3_3_optuna\\trial_3\\models\\fold_7\\best_model.keras\n",
      "262/262 - 7s - 28ms/step - accuracy: 0.9684 - loss: 0.1688 - val_accuracy: 0.9936 - val_loss: 0.0895 - learning_rate: 1.9337e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss improved from 0.08948 to 0.08784, saving model to outputs/step3_3_optuna\\trial_3\\models\\fold_7\\best_model.keras\n",
      "262/262 - 7s - 28ms/step - accuracy: 0.9698 - loss: 0.1664 - val_accuracy: 0.9935 - val_loss: 0.0878 - learning_rate: 1.9337e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss improved from 0.08784 to 0.08553, saving model to outputs/step3_3_optuna\\trial_3\\models\\fold_7\\best_model.keras\n",
      "262/262 - 7s - 28ms/step - accuracy: 0.9716 - loss: 0.1577 - val_accuracy: 0.9939 - val_loss: 0.0855 - learning_rate: 1.9337e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss improved from 0.08553 to 0.08239, saving model to outputs/step3_3_optuna\\trial_3\\models\\fold_7\\best_model.keras\n",
      "262/262 - 7s - 28ms/step - accuracy: 0.9737 - loss: 0.1531 - val_accuracy: 0.9945 - val_loss: 0.0824 - learning_rate: 1.9337e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss improved from 0.08239 to 0.08117, saving model to outputs/step3_3_optuna\\trial_3\\models\\fold_7\\best_model.keras\n",
      "262/262 - 7s - 28ms/step - accuracy: 0.9751 - loss: 0.1456 - val_accuracy: 0.9949 - val_loss: 0.0812 - learning_rate: 1.9337e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss improved from 0.08117 to 0.08087, saving model to outputs/step3_3_optuna\\trial_3\\models\\fold_7\\best_model.keras\n",
      "262/262 - 7s - 28ms/step - accuracy: 0.9760 - loss: 0.1405 - val_accuracy: 0.9939 - val_loss: 0.0809 - learning_rate: 1.9337e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss improved from 0.08087 to 0.07504, saving model to outputs/step3_3_optuna\\trial_3\\models\\fold_7\\best_model.keras\n",
      "262/262 - 7s - 28ms/step - accuracy: 0.9770 - loss: 0.1380 - val_accuracy: 0.9963 - val_loss: 0.0750 - learning_rate: 1.9337e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.07504\n",
      "262/262 - 9s - 35ms/step - accuracy: 0.9783 - loss: 0.1323 - val_accuracy: 0.9951 - val_loss: 0.0766 - learning_rate: 1.9337e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.07504\n",
      "262/262 - 12s - 47ms/step - accuracy: 0.9780 - loss: 0.1302 - val_accuracy: 0.9955 - val_loss: 0.0751 - learning_rate: 1.9337e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss improved from 0.07504 to 0.07457, saving model to outputs/step3_3_optuna\\trial_3\\models\\fold_7\\best_model.keras\n",
      "262/262 - 21s - 79ms/step - accuracy: 0.9789 - loss: 0.1292 - val_accuracy: 0.9951 - val_loss: 0.0746 - learning_rate: 1.9337e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss improved from 0.07457 to 0.06975, saving model to outputs/step3_3_optuna\\trial_3\\models\\fold_7\\best_model.keras\n",
      "262/262 - 20s - 78ms/step - accuracy: 0.9802 - loss: 0.1231 - val_accuracy: 0.9964 - val_loss: 0.0698 - learning_rate: 1.9337e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss improved from 0.06975 to 0.06921, saving model to outputs/step3_3_optuna\\trial_3\\models\\fold_7\\best_model.keras\n",
      "262/262 - 21s - 79ms/step - accuracy: 0.9812 - loss: 0.1208 - val_accuracy: 0.9968 - val_loss: 0.0692 - learning_rate: 1.9337e-04\n",
      "Restoring model weights from the end of the best epoch: 30.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 18:19:37,709] Trial 3 finished with value: 0.623058673891697 and parameters: {'WINDOW_SIZE': 30, 'STRIDE': 1, 'gru1_units': 128, 'gru2_units': 96, 'dense_units': 96, 'dropout_rate': 0.3927585481820283, 'l2': 0.00023295102242852417, 'cf_filters': 64, 'cf_kernel': 7, 'cf_pool': 2, 'cf_drop': 0.0, 'cf_l2': 5.531769709054302e-05, 'learning_rate': 0.00019337223835385302, 'batch_size': 128}. Best is trial 2 with value: 0.6388116314073684.\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.6231 ---\n",
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "특징 추출(병렬): 100%|██████████| 1111/1111 [00:28<00:00, 38.62it/s] \n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "청크 병합: 100%|██████████| 1111/1111 [00:13<00:00, 83.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터가 7754개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 1.93266, saving model to outputs/step3_3_optuna\\trial_4\\models\\fold_7\\best_model.keras\n",
      "61/61 - 5s - 89ms/step - accuracy: 0.1729 - loss: 2.4829 - val_accuracy: 0.3751 - val_loss: 1.9327 - learning_rate: 2.5369e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 1.93266 to 1.66127, saving model to outputs/step3_3_optuna\\trial_4\\models\\fold_7\\best_model.keras\n",
      "61/61 - 1s - 15ms/step - accuracy: 0.3053 - loss: 2.0770 - val_accuracy: 0.4393 - val_loss: 1.6613 - learning_rate: 2.5369e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 1.66127 to 1.47629, saving model to outputs/step3_3_optuna\\trial_4\\models\\fold_7\\best_model.keras\n",
      "61/61 - 1s - 15ms/step - accuracy: 0.3732 - loss: 1.8443 - val_accuracy: 0.4816 - val_loss: 1.4763 - learning_rate: 2.5369e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 1.47629 to 1.33575, saving model to outputs/step3_3_optuna\\trial_4\\models\\fold_7\\best_model.keras\n",
      "61/61 - 1s - 16ms/step - accuracy: 0.4391 - loss: 1.6720 - val_accuracy: 0.5373 - val_loss: 1.3358 - learning_rate: 2.5369e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 1.33575 to 1.20503, saving model to outputs/step3_3_optuna\\trial_4\\models\\fold_7\\best_model.keras\n",
      "61/61 - 1s - 15ms/step - accuracy: 0.4730 - loss: 1.5578 - val_accuracy: 0.6017 - val_loss: 1.2050 - learning_rate: 2.5369e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 1.20503 to 1.07030, saving model to outputs/step3_3_optuna\\trial_4\\models\\fold_7\\best_model.keras\n",
      "61/61 - 1s - 15ms/step - accuracy: 0.5217 - loss: 1.4329 - val_accuracy: 0.6508 - val_loss: 1.0703 - learning_rate: 2.5369e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 1.07030 to 0.93880, saving model to outputs/step3_3_optuna\\trial_4\\models\\fold_7\\best_model.keras\n",
      "61/61 - 1s - 15ms/step - accuracy: 0.5734 - loss: 1.2945 - val_accuracy: 0.7019 - val_loss: 0.9388 - learning_rate: 2.5369e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 0.93880 to 0.84491, saving model to outputs/step3_3_optuna\\trial_4\\models\\fold_7\\best_model.keras\n",
      "61/61 - 1s - 15ms/step - accuracy: 0.6143 - loss: 1.1967 - val_accuracy: 0.7288 - val_loss: 0.8449 - learning_rate: 2.5369e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.84491 to 0.78264, saving model to outputs/step3_3_optuna\\trial_4\\models\\fold_7\\best_model.keras\n",
      "61/61 - 1s - 15ms/step - accuracy: 0.6350 - loss: 1.1158 - val_accuracy: 0.7451 - val_loss: 0.7826 - learning_rate: 2.5369e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 0.78264 to 0.71391, saving model to outputs/step3_3_optuna\\trial_4\\models\\fold_7\\best_model.keras\n",
      "61/61 - 1s - 15ms/step - accuracy: 0.6666 - loss: 1.0354 - val_accuracy: 0.7703 - val_loss: 0.7139 - learning_rate: 2.5369e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 0.71391 to 0.66256, saving model to outputs/step3_3_optuna\\trial_4\\models\\fold_7\\best_model.keras\n",
      "61/61 - 1s - 15ms/step - accuracy: 0.6857 - loss: 0.9917 - val_accuracy: 0.7865 - val_loss: 0.6626 - learning_rate: 2.5369e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 0.66256 to 0.60623, saving model to outputs/step3_3_optuna\\trial_4\\models\\fold_7\\best_model.keras\n",
      "61/61 - 1s - 15ms/step - accuracy: 0.7035 - loss: 0.9345 - val_accuracy: 0.8047 - val_loss: 0.6062 - learning_rate: 2.5369e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 0.60623 to 0.57890, saving model to outputs/step3_3_optuna\\trial_4\\models\\fold_7\\best_model.keras\n",
      "61/61 - 1s - 15ms/step - accuracy: 0.7208 - loss: 0.8788 - val_accuracy: 0.8154 - val_loss: 0.5789 - learning_rate: 2.5369e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 0.57890 to 0.55713, saving model to outputs/step3_3_optuna\\trial_4\\models\\fold_7\\best_model.keras\n",
      "61/61 - 1s - 15ms/step - accuracy: 0.7296 - loss: 0.8607 - val_accuracy: 0.8174 - val_loss: 0.5571 - learning_rate: 2.5369e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss improved from 0.55713 to 0.53035, saving model to outputs/step3_3_optuna\\trial_4\\models\\fold_7\\best_model.keras\n",
      "61/61 - 1s - 15ms/step - accuracy: 0.7435 - loss: 0.8068 - val_accuracy: 0.8327 - val_loss: 0.5303 - learning_rate: 2.5369e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss improved from 0.53035 to 0.52448, saving model to outputs/step3_3_optuna\\trial_4\\models\\fold_7\\best_model.keras\n",
      "61/61 - 1s - 16ms/step - accuracy: 0.7508 - loss: 0.7758 - val_accuracy: 0.8264 - val_loss: 0.5245 - learning_rate: 2.5369e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 0.52448 to 0.48973, saving model to outputs/step3_3_optuna\\trial_4\\models\\fold_7\\best_model.keras\n",
      "61/61 - 1s - 15ms/step - accuracy: 0.7583 - loss: 0.7677 - val_accuracy: 0.8415 - val_loss: 0.4897 - learning_rate: 2.5369e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss improved from 0.48973 to 0.47277, saving model to outputs/step3_3_optuna\\trial_4\\models\\fold_7\\best_model.keras\n",
      "61/61 - 1s - 16ms/step - accuracy: 0.7752 - loss: 0.7182 - val_accuracy: 0.8485 - val_loss: 0.4728 - learning_rate: 2.5369e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss improved from 0.47277 to 0.45706, saving model to outputs/step3_3_optuna\\trial_4\\models\\fold_7\\best_model.keras\n",
      "61/61 - 1s - 16ms/step - accuracy: 0.7768 - loss: 0.6891 - val_accuracy: 0.8529 - val_loss: 0.4571 - learning_rate: 2.5369e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.45706\n",
      "61/61 - 1s - 15ms/step - accuracy: 0.7810 - loss: 0.6717 - val_accuracy: 0.8520 - val_loss: 0.4612 - learning_rate: 2.5369e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss improved from 0.45706 to 0.42658, saving model to outputs/step3_3_optuna\\trial_4\\models\\fold_7\\best_model.keras\n",
      "61/61 - 1s - 16ms/step - accuracy: 0.7899 - loss: 0.6508 - val_accuracy: 0.8632 - val_loss: 0.4266 - learning_rate: 2.5369e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.42658\n",
      "61/61 - 1s - 15ms/step - accuracy: 0.7960 - loss: 0.6390 - val_accuracy: 0.8531 - val_loss: 0.4387 - learning_rate: 2.5369e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss improved from 0.42658 to 0.40842, saving model to outputs/step3_3_optuna\\trial_4\\models\\fold_7\\best_model.keras\n",
      "61/61 - 1s - 16ms/step - accuracy: 0.7991 - loss: 0.6330 - val_accuracy: 0.8672 - val_loss: 0.4084 - learning_rate: 2.5369e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss improved from 0.40842 to 0.39551, saving model to outputs/step3_3_optuna\\trial_4\\models\\fold_7\\best_model.keras\n",
      "61/61 - 1s - 16ms/step - accuracy: 0.8085 - loss: 0.6068 - val_accuracy: 0.8691 - val_loss: 0.3955 - learning_rate: 2.5369e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss improved from 0.39551 to 0.38722, saving model to outputs/step3_3_optuna\\trial_4\\models\\fold_7\\best_model.keras\n",
      "61/61 - 1s - 16ms/step - accuracy: 0.8118 - loss: 0.5817 - val_accuracy: 0.8755 - val_loss: 0.3872 - learning_rate: 2.5369e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.38722\n",
      "61/61 - 1s - 15ms/step - accuracy: 0.8145 - loss: 0.5868 - val_accuracy: 0.8715 - val_loss: 0.3953 - learning_rate: 2.5369e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss improved from 0.38722 to 0.38517, saving model to outputs/step3_3_optuna\\trial_4\\models\\fold_7\\best_model.keras\n",
      "61/61 - 1s - 16ms/step - accuracy: 0.8205 - loss: 0.5603 - val_accuracy: 0.8762 - val_loss: 0.3852 - learning_rate: 2.5369e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.38517\n",
      "61/61 - 1s - 15ms/step - accuracy: 0.8260 - loss: 0.5443 - val_accuracy: 0.8654 - val_loss: 0.3949 - learning_rate: 2.5369e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss improved from 0.38517 to 0.35726, saving model to outputs/step3_3_optuna\\trial_4\\models\\fold_7\\best_model.keras\n",
      "61/61 - 1s - 16ms/step - accuracy: 0.8204 - loss: 0.5522 - val_accuracy: 0.8838 - val_loss: 0.3573 - learning_rate: 2.5369e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss improved from 0.35726 to 0.35571, saving model to outputs/step3_3_optuna\\trial_4\\models\\fold_7\\best_model.keras\n",
      "61/61 - 1s - 16ms/step - accuracy: 0.8299 - loss: 0.5335 - val_accuracy: 0.8801 - val_loss: 0.3557 - learning_rate: 2.5369e-04\n",
      "Restoring model weights from the end of the best epoch: 30.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 18:21:04,896] Trial 4 finished with value: 0.4616450669882331 and parameters: {'WINDOW_SIZE': 15, 'STRIDE': 5, 'gru1_units': 128, 'gru2_units': 64, 'dense_units': 64, 'dropout_rate': 0.3443101071099264, 'l2': 6.416495467205229e-05, 'cf_filters': 48, 'cf_kernel': 5, 'cf_pool': 3, 'cf_drop': 0.1, 'cf_l2': 1.0616607937172173e-05, 'learning_rate': 0.00025369152225300246, 'batch_size': 128}. Best is trial 2 with value: 0.6388116314073684.\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.4616 ---\n",
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "특징 추출(병렬): 100%|██████████| 1111/1111 [00:18<00:00, 59.04it/s] \n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "청크 병합: 100%|██████████| 1111/1111 [00:11<00:00, 97.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터가 6094개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 2.04575, saving model to outputs/step3_3_optuna\\trial_5\\models\\fold_7\\best_model.keras\n",
      "48/48 - 6s - 121ms/step - accuracy: 0.1400 - loss: 2.5497 - val_accuracy: 0.3855 - val_loss: 2.0457 - learning_rate: 2.8060e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 2.04575 to 1.66853, saving model to outputs/step3_3_optuna\\trial_5\\models\\fold_7\\best_model.keras\n",
      "48/48 - 3s - 62ms/step - accuracy: 0.2873 - loss: 2.1195 - val_accuracy: 0.4544 - val_loss: 1.6685 - learning_rate: 2.8060e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 1.66853 to 1.39012, saving model to outputs/step3_3_optuna\\trial_5\\models\\fold_7\\best_model.keras\n",
      "48/48 - 3s - 64ms/step - accuracy: 0.3814 - loss: 1.8299 - val_accuracy: 0.5696 - val_loss: 1.3901 - learning_rate: 2.8060e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 1.39012 to 1.13944, saving model to outputs/step3_3_optuna\\trial_5\\models\\fold_7\\best_model.keras\n",
      "48/48 - 3s - 60ms/step - accuracy: 0.4769 - loss: 1.5670 - val_accuracy: 0.6550 - val_loss: 1.1394 - learning_rate: 2.8060e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 1.13944 to 0.97854, saving model to outputs/step3_3_optuna\\trial_5\\models\\fold_7\\best_model.keras\n",
      "48/48 - 3s - 62ms/step - accuracy: 0.5450 - loss: 1.3681 - val_accuracy: 0.6932 - val_loss: 0.9785 - learning_rate: 2.8060e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 0.97854 to 0.84054, saving model to outputs/step3_3_optuna\\trial_5\\models\\fold_7\\best_model.keras\n",
      "48/48 - 3s - 63ms/step - accuracy: 0.6075 - loss: 1.2068 - val_accuracy: 0.7420 - val_loss: 0.8405 - learning_rate: 2.8060e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.84054 to 0.74178, saving model to outputs/step3_3_optuna\\trial_5\\models\\fold_7\\best_model.keras\n",
      "48/48 - 3s - 62ms/step - accuracy: 0.6454 - loss: 1.0801 - val_accuracy: 0.7721 - val_loss: 0.7418 - learning_rate: 2.8060e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 0.74178 to 0.64861, saving model to outputs/step3_3_optuna\\trial_5\\models\\fold_7\\best_model.keras\n",
      "48/48 - 3s - 60ms/step - accuracy: 0.6840 - loss: 0.9725 - val_accuracy: 0.8059 - val_loss: 0.6486 - learning_rate: 2.8060e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.64861 to 0.57839, saving model to outputs/step3_3_optuna\\trial_5\\models\\fold_7\\best_model.keras\n",
      "48/48 - 3s - 61ms/step - accuracy: 0.7196 - loss: 0.8824 - val_accuracy: 0.8271 - val_loss: 0.5784 - learning_rate: 2.8060e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 0.57839 to 0.51724, saving model to outputs/step3_3_optuna\\trial_5\\models\\fold_7\\best_model.keras\n",
      "48/48 - 3s - 59ms/step - accuracy: 0.7529 - loss: 0.7960 - val_accuracy: 0.8441 - val_loss: 0.5172 - learning_rate: 2.8060e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 0.51724 to 0.47087, saving model to outputs/step3_3_optuna\\trial_5\\models\\fold_7\\best_model.keras\n",
      "48/48 - 3s - 60ms/step - accuracy: 0.7701 - loss: 0.7312 - val_accuracy: 0.8597 - val_loss: 0.4709 - learning_rate: 2.8060e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 0.47087 to 0.43795, saving model to outputs/step3_3_optuna\\trial_5\\models\\fold_7\\best_model.keras\n",
      "48/48 - 3s - 59ms/step - accuracy: 0.7878 - loss: 0.6676 - val_accuracy: 0.8725 - val_loss: 0.4380 - learning_rate: 2.8060e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 0.43795 to 0.40887, saving model to outputs/step3_3_optuna\\trial_5\\models\\fold_7\\best_model.keras\n",
      "48/48 - 3s - 59ms/step - accuracy: 0.8011 - loss: 0.6247 - val_accuracy: 0.8803 - val_loss: 0.4089 - learning_rate: 2.8060e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 0.40887 to 0.38298, saving model to outputs/step3_3_optuna\\trial_5\\models\\fold_7\\best_model.keras\n",
      "48/48 - 3s - 61ms/step - accuracy: 0.8101 - loss: 0.5997 - val_accuracy: 0.8881 - val_loss: 0.3830 - learning_rate: 2.8060e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss improved from 0.38298 to 0.37155, saving model to outputs/step3_3_optuna\\trial_5\\models\\fold_7\\best_model.keras\n",
      "48/48 - 3s - 57ms/step - accuracy: 0.8188 - loss: 0.5705 - val_accuracy: 0.8893 - val_loss: 0.3716 - learning_rate: 2.8060e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss improved from 0.37155 to 0.34129, saving model to outputs/step3_3_optuna\\trial_5\\models\\fold_7\\best_model.keras\n",
      "48/48 - 3s - 59ms/step - accuracy: 0.8290 - loss: 0.5399 - val_accuracy: 0.8979 - val_loss: 0.3413 - learning_rate: 2.8060e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 0.34129 to 0.31058, saving model to outputs/step3_3_optuna\\trial_5\\models\\fold_7\\best_model.keras\n",
      "48/48 - 3s - 59ms/step - accuracy: 0.8371 - loss: 0.5100 - val_accuracy: 0.9091 - val_loss: 0.3106 - learning_rate: 2.8060e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss improved from 0.31058 to 0.29085, saving model to outputs/step3_3_optuna\\trial_5\\models\\fold_7\\best_model.keras\n",
      "48/48 - 3s - 60ms/step - accuracy: 0.8456 - loss: 0.4862 - val_accuracy: 0.9124 - val_loss: 0.2909 - learning_rate: 2.8060e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss improved from 0.29085 to 0.26595, saving model to outputs/step3_3_optuna\\trial_5\\models\\fold_7\\best_model.keras\n",
      "48/48 - 3s - 58ms/step - accuracy: 0.8472 - loss: 0.4687 - val_accuracy: 0.9219 - val_loss: 0.2659 - learning_rate: 2.8060e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.26595\n",
      "48/48 - 3s - 59ms/step - accuracy: 0.8669 - loss: 0.4380 - val_accuracy: 0.9252 - val_loss: 0.2673 - learning_rate: 2.8060e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss improved from 0.26595 to 0.24359, saving model to outputs/step3_3_optuna\\trial_5\\models\\fold_7\\best_model.keras\n",
      "48/48 - 3s - 60ms/step - accuracy: 0.8699 - loss: 0.4142 - val_accuracy: 0.9364 - val_loss: 0.2436 - learning_rate: 2.8060e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss improved from 0.24359 to 0.23210, saving model to outputs/step3_3_optuna\\trial_5\\models\\fold_7\\best_model.keras\n",
      "48/48 - 3s - 61ms/step - accuracy: 0.8815 - loss: 0.3981 - val_accuracy: 0.9339 - val_loss: 0.2321 - learning_rate: 2.8060e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss improved from 0.23210 to 0.21593, saving model to outputs/step3_3_optuna\\trial_5\\models\\fold_7\\best_model.keras\n",
      "48/48 - 3s - 61ms/step - accuracy: 0.8796 - loss: 0.3791 - val_accuracy: 0.9414 - val_loss: 0.2159 - learning_rate: 2.8060e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss improved from 0.21593 to 0.21135, saving model to outputs/step3_3_optuna\\trial_5\\models\\fold_7\\best_model.keras\n",
      "48/48 - 3s - 60ms/step - accuracy: 0.8846 - loss: 0.3665 - val_accuracy: 0.9428 - val_loss: 0.2114 - learning_rate: 2.8060e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.21135\n",
      "48/48 - 3s - 57ms/step - accuracy: 0.8933 - loss: 0.3585 - val_accuracy: 0.9350 - val_loss: 0.2307 - learning_rate: 2.8060e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss improved from 0.21135 to 0.20400, saving model to outputs/step3_3_optuna\\trial_5\\models\\fold_7\\best_model.keras\n",
      "48/48 - 3s - 59ms/step - accuracy: 0.8978 - loss: 0.3354 - val_accuracy: 0.9464 - val_loss: 0.2040 - learning_rate: 2.8060e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss improved from 0.20400 to 0.18713, saving model to outputs/step3_3_optuna\\trial_5\\models\\fold_7\\best_model.keras\n",
      "48/48 - 3s - 53ms/step - accuracy: 0.8953 - loss: 0.3336 - val_accuracy: 0.9481 - val_loss: 0.1871 - learning_rate: 2.8060e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.18713\n",
      "48/48 - 2s - 45ms/step - accuracy: 0.8994 - loss: 0.3249 - val_accuracy: 0.9478 - val_loss: 0.1913 - learning_rate: 2.8060e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss improved from 0.18713 to 0.17941, saving model to outputs/step3_3_optuna\\trial_5\\models\\fold_7\\best_model.keras\n",
      "48/48 - 2s - 45ms/step - accuracy: 0.9078 - loss: 0.2996 - val_accuracy: 0.9529 - val_loss: 0.1794 - learning_rate: 2.8060e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss improved from 0.17941 to 0.17922, saving model to outputs/step3_3_optuna\\trial_5\\models\\fold_7\\best_model.keras\n",
      "48/48 - 2s - 45ms/step - accuracy: 0.9111 - loss: 0.2985 - val_accuracy: 0.9529 - val_loss: 0.1792 - learning_rate: 2.8060e-04\n",
      "Restoring model weights from the end of the best epoch: 30.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 18:23:13,223] Trial 5 finished with value: 0.5572818211698163 and parameters: {'WINDOW_SIZE': 45, 'STRIDE': 5, 'gru1_units': 128, 'gru2_units': 64, 'dense_units': 96, 'dropout_rate': 0.3526428135581585, 'l2': 4.543083780465704e-05, 'cf_filters': 64, 'cf_kernel': 3, 'cf_pool': 2, 'cf_drop': 0.2, 'cf_l2': 1.0836138902170862e-05, 'learning_rate': 0.00028059556805022336, 'batch_size': 128}. Best is trial 2 with value: 0.6388116314073684.\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.5573 ---\n",
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "특징 추출(병렬): 100%|██████████| 1111/1111 [00:31<00:00, 35.12it/s] \n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "청크 병합: 100%|██████████| 1111/1111 [00:15<00:00, 71.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터가 11356개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 2.08959, saving model to outputs/step3_3_optuna\\trial_6\\models\\fold_7\\best_model.keras\n",
      "89/89 - 9s - 97ms/step - accuracy: 0.1502 - loss: 2.6139 - val_accuracy: 0.3305 - val_loss: 2.0896 - learning_rate: 2.3273e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 2.08959 to 1.81213, saving model to outputs/step3_3_optuna\\trial_6\\models\\fold_7\\best_model.keras\n",
      "89/89 - 3s - 37ms/step - accuracy: 0.2711 - loss: 2.1811 - val_accuracy: 0.3916 - val_loss: 1.8121 - learning_rate: 2.3273e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 1.81213 to 1.65030, saving model to outputs/step3_3_optuna\\trial_6\\models\\fold_7\\best_model.keras\n",
      "89/89 - 2s - 18ms/step - accuracy: 0.3366 - loss: 1.9884 - val_accuracy: 0.4441 - val_loss: 1.6503 - learning_rate: 2.3273e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 1.65030 to 1.51008, saving model to outputs/step3_3_optuna\\trial_6\\models\\fold_7\\best_model.keras\n",
      "89/89 - 2s - 18ms/step - accuracy: 0.3827 - loss: 1.8391 - val_accuracy: 0.4978 - val_loss: 1.5101 - learning_rate: 2.3273e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 1.51008 to 1.31306, saving model to outputs/step3_3_optuna\\trial_6\\models\\fold_7\\best_model.keras\n",
      "89/89 - 2s - 18ms/step - accuracy: 0.4292 - loss: 1.7149 - val_accuracy: 0.6246 - val_loss: 1.3131 - learning_rate: 2.3273e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 1.31306 to 1.10588, saving model to outputs/step3_3_optuna\\trial_6\\models\\fold_7\\best_model.keras\n",
      "89/89 - 2s - 18ms/step - accuracy: 0.4806 - loss: 1.5703 - val_accuracy: 0.6923 - val_loss: 1.1059 - learning_rate: 2.3273e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 1.10588 to 0.95265, saving model to outputs/step3_3_optuna\\trial_6\\models\\fold_7\\best_model.keras\n",
      "89/89 - 2s - 18ms/step - accuracy: 0.5356 - loss: 1.4267 - val_accuracy: 0.7366 - val_loss: 0.9527 - learning_rate: 2.3273e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 0.95265 to 0.83846, saving model to outputs/step3_3_optuna\\trial_6\\models\\fold_7\\best_model.keras\n",
      "89/89 - 2s - 17ms/step - accuracy: 0.5714 - loss: 1.3053 - val_accuracy: 0.7650 - val_loss: 0.8385 - learning_rate: 2.3273e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.83846 to 0.73015, saving model to outputs/step3_3_optuna\\trial_6\\models\\fold_7\\best_model.keras\n",
      "89/89 - 2s - 18ms/step - accuracy: 0.6129 - loss: 1.1901 - val_accuracy: 0.8042 - val_loss: 0.7302 - learning_rate: 2.3273e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 0.73015 to 0.64482, saving model to outputs/step3_3_optuna\\trial_6\\models\\fold_7\\best_model.keras\n",
      "89/89 - 2s - 18ms/step - accuracy: 0.6456 - loss: 1.1028 - val_accuracy: 0.8177 - val_loss: 0.6448 - learning_rate: 2.3273e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 0.64482 to 0.57670, saving model to outputs/step3_3_optuna\\trial_6\\models\\fold_7\\best_model.keras\n",
      "89/89 - 2s - 19ms/step - accuracy: 0.6763 - loss: 1.0215 - val_accuracy: 0.8358 - val_loss: 0.5767 - learning_rate: 2.3273e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 0.57670 to 0.54038, saving model to outputs/step3_3_optuna\\trial_6\\models\\fold_7\\best_model.keras\n",
      "89/89 - 2s - 19ms/step - accuracy: 0.6962 - loss: 0.9549 - val_accuracy: 0.8376 - val_loss: 0.5404 - learning_rate: 2.3273e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 0.54038 to 0.49836, saving model to outputs/step3_3_optuna\\trial_6\\models\\fold_7\\best_model.keras\n",
      "89/89 - 2s - 19ms/step - accuracy: 0.7091 - loss: 0.9162 - val_accuracy: 0.8538 - val_loss: 0.4984 - learning_rate: 2.3273e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.49836\n",
      "89/89 - 2s - 19ms/step - accuracy: 0.7306 - loss: 0.8697 - val_accuracy: 0.8445 - val_loss: 0.5128 - learning_rate: 2.3273e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss improved from 0.49836 to 0.44179, saving model to outputs/step3_3_optuna\\trial_6\\models\\fold_7\\best_model.keras\n",
      "89/89 - 2s - 19ms/step - accuracy: 0.7378 - loss: 0.8356 - val_accuracy: 0.8705 - val_loss: 0.4418 - learning_rate: 2.3273e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss improved from 0.44179 to 0.43140, saving model to outputs/step3_3_optuna\\trial_6\\models\\fold_7\\best_model.keras\n",
      "89/89 - 2s - 19ms/step - accuracy: 0.7544 - loss: 0.7945 - val_accuracy: 0.8738 - val_loss: 0.4314 - learning_rate: 2.3273e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 0.43140 to 0.41212, saving model to outputs/step3_3_optuna\\trial_6\\models\\fold_7\\best_model.keras\n",
      "89/89 - 2s - 19ms/step - accuracy: 0.7622 - loss: 0.7775 - val_accuracy: 0.8774 - val_loss: 0.4121 - learning_rate: 2.3273e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss improved from 0.41212 to 0.38521, saving model to outputs/step3_3_optuna\\trial_6\\models\\fold_7\\best_model.keras\n",
      "89/89 - 2s - 19ms/step - accuracy: 0.7770 - loss: 0.7320 - val_accuracy: 0.8847 - val_loss: 0.3852 - learning_rate: 2.3273e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.38521\n",
      "89/89 - 2s - 19ms/step - accuracy: 0.7784 - loss: 0.7219 - val_accuracy: 0.8743 - val_loss: 0.3897 - learning_rate: 2.3273e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss improved from 0.38521 to 0.34859, saving model to outputs/step3_3_optuna\\trial_6\\models\\fold_7\\best_model.keras\n",
      "89/89 - 2s - 19ms/step - accuracy: 0.7848 - loss: 0.7000 - val_accuracy: 0.8996 - val_loss: 0.3486 - learning_rate: 2.3273e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss improved from 0.34859 to 0.34520, saving model to outputs/step3_3_optuna\\trial_6\\models\\fold_7\\best_model.keras\n",
      "89/89 - 2s - 19ms/step - accuracy: 0.7982 - loss: 0.6683 - val_accuracy: 0.8963 - val_loss: 0.3452 - learning_rate: 2.3273e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss improved from 0.34520 to 0.33305, saving model to outputs/step3_3_optuna\\trial_6\\models\\fold_7\\best_model.keras\n",
      "89/89 - 2s - 19ms/step - accuracy: 0.8054 - loss: 0.6515 - val_accuracy: 0.9032 - val_loss: 0.3330 - learning_rate: 2.3273e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss improved from 0.33305 to 0.31892, saving model to outputs/step3_3_optuna\\trial_6\\models\\fold_7\\best_model.keras\n",
      "89/89 - 2s - 19ms/step - accuracy: 0.8059 - loss: 0.6369 - val_accuracy: 0.9056 - val_loss: 0.3189 - learning_rate: 2.3273e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss improved from 0.31892 to 0.31571, saving model to outputs/step3_3_optuna\\trial_6\\models\\fold_7\\best_model.keras\n",
      "89/89 - 2s - 19ms/step - accuracy: 0.8165 - loss: 0.6140 - val_accuracy: 0.9039 - val_loss: 0.3157 - learning_rate: 2.3273e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss improved from 0.31571 to 0.29733, saving model to outputs/step3_3_optuna\\trial_6\\models\\fold_7\\best_model.keras\n",
      "89/89 - 2s - 19ms/step - accuracy: 0.8175 - loss: 0.6162 - val_accuracy: 0.9129 - val_loss: 0.2973 - learning_rate: 2.3273e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.29733\n",
      "89/89 - 2s - 19ms/step - accuracy: 0.8266 - loss: 0.5861 - val_accuracy: 0.9096 - val_loss: 0.3014 - learning_rate: 2.3273e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss improved from 0.29733 to 0.28218, saving model to outputs/step3_3_optuna\\trial_6\\models\\fold_7\\best_model.keras\n",
      "89/89 - 2s - 20ms/step - accuracy: 0.8293 - loss: 0.5826 - val_accuracy: 0.9150 - val_loss: 0.2822 - learning_rate: 2.3273e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.28218\n",
      "89/89 - 2s - 19ms/step - accuracy: 0.8336 - loss: 0.5603 - val_accuracy: 0.9102 - val_loss: 0.2948 - learning_rate: 2.3273e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss improved from 0.28218 to 0.27046, saving model to outputs/step3_3_optuna\\trial_6\\models\\fold_7\\best_model.keras\n",
      "89/89 - 2s - 19ms/step - accuracy: 0.8375 - loss: 0.5518 - val_accuracy: 0.9219 - val_loss: 0.2705 - learning_rate: 2.3273e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss improved from 0.27046 to 0.26553, saving model to outputs/step3_3_optuna\\trial_6\\models\\fold_7\\best_model.keras\n",
      "89/89 - 2s - 20ms/step - accuracy: 0.8453 - loss: 0.5335 - val_accuracy: 0.9175 - val_loss: 0.2655 - learning_rate: 2.3273e-04\n",
      "Restoring model weights from the end of the best epoch: 30.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 18:25:16,509] Trial 6 finished with value: 0.4923913029174484 and parameters: {'WINDOW_SIZE': 30, 'STRIDE': 3, 'gru1_units': 96, 'gru2_units': 64, 'dense_units': 64, 'dropout_rate': 0.5886597826800236, 'l2': 8.760278108778674e-05, 'cf_filters': 48, 'cf_kernel': 3, 'cf_pool': 3, 'cf_drop': 0.2, 'cf_l2': 1.547198903040639e-05, 'learning_rate': 0.00023272623593032413, 'batch_size': 128}. Best is trial 2 with value: 0.6388116314073684.\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.4924 ---\n",
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n",
      "학습 데이터가 7754개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 1.45642, saving model to outputs/step3_3_optuna\\trial_7\\models\\fold_7\\best_model.keras\n",
      "122/122 - 3s - 24ms/step - accuracy: 0.2975 - loss: 2.0712 - val_accuracy: 0.5723 - val_loss: 1.4564 - learning_rate: 4.3038e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 1.45642 to 0.93524, saving model to outputs/step3_3_optuna\\trial_7\\models\\fold_7\\best_model.keras\n",
      "122/122 - 1s - 10ms/step - accuracy: 0.4981 - loss: 1.4899 - val_accuracy: 0.7177 - val_loss: 0.9352 - learning_rate: 4.3038e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 0.93524 to 0.71999, saving model to outputs/step3_3_optuna\\trial_7\\models\\fold_7\\best_model.keras\n",
      "122/122 - 1s - 10ms/step - accuracy: 0.6145 - loss: 1.1624 - val_accuracy: 0.7826 - val_loss: 0.7200 - learning_rate: 4.3038e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 0.71999 to 0.60212, saving model to outputs/step3_3_optuna\\trial_7\\models\\fold_7\\best_model.keras\n",
      "122/122 - 1s - 11ms/step - accuracy: 0.6780 - loss: 0.9690 - val_accuracy: 0.8025 - val_loss: 0.6021 - learning_rate: 4.3038e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 0.60212 to 0.54289, saving model to outputs/step3_3_optuna\\trial_7\\models\\fold_7\\best_model.keras\n",
      "122/122 - 1s - 11ms/step - accuracy: 0.7142 - loss: 0.8495 - val_accuracy: 0.8167 - val_loss: 0.5429 - learning_rate: 4.3038e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 0.54289 to 0.44267, saving model to outputs/step3_3_optuna\\trial_7\\models\\fold_7\\best_model.keras\n",
      "122/122 - 1s - 11ms/step - accuracy: 0.7499 - loss: 0.7568 - val_accuracy: 0.8571 - val_loss: 0.4427 - learning_rate: 4.3038e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.44267 to 0.41263, saving model to outputs/step3_3_optuna\\trial_7\\models\\fold_7\\best_model.keras\n",
      "122/122 - 1s - 11ms/step - accuracy: 0.7769 - loss: 0.6815 - val_accuracy: 0.8650 - val_loss: 0.4126 - learning_rate: 4.3038e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.41263\n",
      "122/122 - 1s - 11ms/step - accuracy: 0.7792 - loss: 0.6669 - val_accuracy: 0.8527 - val_loss: 0.4368 - learning_rate: 4.3038e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.41263 to 0.36653, saving model to outputs/step3_3_optuna\\trial_7\\models\\fold_7\\best_model.keras\n",
      "122/122 - 1s - 11ms/step - accuracy: 0.8040 - loss: 0.6015 - val_accuracy: 0.8740 - val_loss: 0.3665 - learning_rate: 4.3038e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 0.36653 to 0.33054, saving model to outputs/step3_3_optuna\\trial_7\\models\\fold_7\\best_model.keras\n",
      "122/122 - 1s - 11ms/step - accuracy: 0.8134 - loss: 0.5675 - val_accuracy: 0.8862 - val_loss: 0.3305 - learning_rate: 4.3038e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 0.33054 to 0.30594, saving model to outputs/step3_3_optuna\\trial_7\\models\\fold_7\\best_model.keras\n",
      "122/122 - 1s - 11ms/step - accuracy: 0.8298 - loss: 0.5185 - val_accuracy: 0.8968 - val_loss: 0.3059 - learning_rate: 4.3038e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.30594\n",
      "122/122 - 1s - 11ms/step - accuracy: 0.8407 - loss: 0.4966 - val_accuracy: 0.8878 - val_loss: 0.3261 - learning_rate: 4.3038e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 0.30594 to 0.29453, saving model to outputs/step3_3_optuna\\trial_7\\models\\fold_7\\best_model.keras\n",
      "122/122 - 1s - 11ms/step - accuracy: 0.8446 - loss: 0.4807 - val_accuracy: 0.8970 - val_loss: 0.2945 - learning_rate: 4.3038e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.29453\n",
      "122/122 - 1s - 11ms/step - accuracy: 0.8514 - loss: 0.4639 - val_accuracy: 0.8963 - val_loss: 0.3053 - learning_rate: 4.3038e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss improved from 0.29453 to 0.26972, saving model to outputs/step3_3_optuna\\trial_7\\models\\fold_7\\best_model.keras\n",
      "122/122 - 1s - 11ms/step - accuracy: 0.8456 - loss: 0.4655 - val_accuracy: 0.9095 - val_loss: 0.2697 - learning_rate: 4.3038e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss improved from 0.26972 to 0.25914, saving model to outputs/step3_3_optuna\\trial_7\\models\\fold_7\\best_model.keras\n",
      "122/122 - 1s - 11ms/step - accuracy: 0.8641 - loss: 0.4221 - val_accuracy: 0.9088 - val_loss: 0.2591 - learning_rate: 4.3038e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 0.25914 to 0.25622, saving model to outputs/step3_3_optuna\\trial_7\\models\\fold_7\\best_model.keras\n",
      "122/122 - 1s - 11ms/step - accuracy: 0.8668 - loss: 0.4082 - val_accuracy: 0.9130 - val_loss: 0.2562 - learning_rate: 4.3038e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss improved from 0.25622 to 0.23121, saving model to outputs/step3_3_optuna\\trial_7\\models\\fold_7\\best_model.keras\n",
      "122/122 - 1s - 11ms/step - accuracy: 0.8750 - loss: 0.3903 - val_accuracy: 0.9182 - val_loss: 0.2312 - learning_rate: 4.3038e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss improved from 0.23121 to 0.22105, saving model to outputs/step3_3_optuna\\trial_7\\models\\fold_7\\best_model.keras\n",
      "122/122 - 1s - 11ms/step - accuracy: 0.8790 - loss: 0.3773 - val_accuracy: 0.9235 - val_loss: 0.2211 - learning_rate: 4.3038e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss improved from 0.22105 to 0.20538, saving model to outputs/step3_3_optuna\\trial_7\\models\\fold_7\\best_model.keras\n",
      "122/122 - 1s - 11ms/step - accuracy: 0.8826 - loss: 0.3552 - val_accuracy: 0.9316 - val_loss: 0.2054 - learning_rate: 4.3038e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.20538\n",
      "122/122 - 1s - 10ms/step - accuracy: 0.8832 - loss: 0.3559 - val_accuracy: 0.9253 - val_loss: 0.2137 - learning_rate: 4.3038e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.20538\n",
      "122/122 - 1s - 10ms/step - accuracy: 0.8864 - loss: 0.3521 - val_accuracy: 0.9303 - val_loss: 0.2093 - learning_rate: 4.3038e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss improved from 0.20538 to 0.20469, saving model to outputs/step3_3_optuna\\trial_7\\models\\fold_7\\best_model.keras\n",
      "122/122 - 1s - 11ms/step - accuracy: 0.8903 - loss: 0.3329 - val_accuracy: 0.9305 - val_loss: 0.2047 - learning_rate: 4.3038e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss improved from 0.20469 to 0.18131, saving model to outputs/step3_3_optuna\\trial_7\\models\\fold_7\\best_model.keras\n",
      "122/122 - 1s - 11ms/step - accuracy: 0.8890 - loss: 0.3316 - val_accuracy: 0.9410 - val_loss: 0.1813 - learning_rate: 4.3038e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.18131\n",
      "122/122 - 1s - 11ms/step - accuracy: 0.9002 - loss: 0.3144 - val_accuracy: 0.9274 - val_loss: 0.2010 - learning_rate: 4.3038e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.18131\n",
      "122/122 - 1s - 10ms/step - accuracy: 0.9119 - loss: 0.2875 - val_accuracy: 0.9316 - val_loss: 0.1944 - learning_rate: 4.3038e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss improved from 0.18131 to 0.15922, saving model to outputs/step3_3_optuna\\trial_7\\models\\fold_7\\best_model.keras\n",
      "122/122 - 1s - 11ms/step - accuracy: 0.9074 - loss: 0.2906 - val_accuracy: 0.9443 - val_loss: 0.1592 - learning_rate: 4.3038e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.15922\n",
      "122/122 - 1s - 11ms/step - accuracy: 0.9120 - loss: 0.2780 - val_accuracy: 0.9377 - val_loss: 0.1807 - learning_rate: 4.3038e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.15922\n",
      "122/122 - 1s - 11ms/step - accuracy: 0.9133 - loss: 0.2775 - val_accuracy: 0.9399 - val_loss: 0.1836 - learning_rate: 4.3038e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.15922\n",
      "122/122 - 1s - 11ms/step - accuracy: 0.9127 - loss: 0.2710 - val_accuracy: 0.9395 - val_loss: 0.1790 - learning_rate: 4.3038e-04\n",
      "Restoring model weights from the end of the best epoch: 27.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 18:26:00,817] Trial 7 finished with value: 0.5253191032365042 and parameters: {'WINDOW_SIZE': 15, 'STRIDE': 5, 'gru1_units': 256, 'gru2_units': 0, 'dense_units': 96, 'dropout_rate': 0.5312332843298708, 'l2': 1.2209603825784977e-05, 'cf_filters': 48, 'cf_kernel': 3, 'cf_pool': 3, 'cf_drop': 0.0, 'cf_l2': 1.2164449391497758e-05, 'learning_rate': 0.00043038316820811985, 'batch_size': 64}. Best is trial 2 with value: 0.6388116314073684.\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.5253 ---\n",
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "특징 추출(병렬): 100%|██████████| 1111/1111 [00:18<00:00, 59.53it/s] \n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "청크 병합: 100%|██████████| 1111/1111 [00:11<00:00, 94.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터가 37665개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 0.73801, saving model to outputs/step3_3_optuna\\trial_8\\models\\fold_7\\best_model.keras\n",
      "393/393 - 12s - 31ms/step - accuracy: 0.4911 - loss: 1.5941 - val_accuracy: 0.7742 - val_loss: 0.7380 - learning_rate: 5.8780e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 0.73801 to 0.50356, saving model to outputs/step3_3_optuna\\trial_8\\models\\fold_7\\best_model.keras\n",
      "393/393 - 6s - 16ms/step - accuracy: 0.7406 - loss: 0.8621 - val_accuracy: 0.8533 - val_loss: 0.5036 - learning_rate: 5.8780e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 0.50356 to 0.39741, saving model to outputs/step3_3_optuna\\trial_8\\models\\fold_7\\best_model.keras\n",
      "393/393 - 10s - 27ms/step - accuracy: 0.8051 - loss: 0.6618 - val_accuracy: 0.8826 - val_loss: 0.3974 - learning_rate: 5.8780e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 0.39741 to 0.30263, saving model to outputs/step3_3_optuna\\trial_8\\models\\fold_7\\best_model.keras\n",
      "393/393 - 10s - 26ms/step - accuracy: 0.8459 - loss: 0.5404 - val_accuracy: 0.9191 - val_loss: 0.3026 - learning_rate: 5.8780e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 0.30263 to 0.26516, saving model to outputs/step3_3_optuna\\trial_8\\models\\fold_7\\best_model.keras\n",
      "393/393 - 6s - 16ms/step - accuracy: 0.8736 - loss: 0.4557 - val_accuracy: 0.9319 - val_loss: 0.2652 - learning_rate: 5.8780e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 0.26516 to 0.23422, saving model to outputs/step3_3_optuna\\trial_8\\models\\fold_7\\best_model.keras\n",
      "393/393 - 10s - 26ms/step - accuracy: 0.8902 - loss: 0.4145 - val_accuracy: 0.9435 - val_loss: 0.2342 - learning_rate: 5.8780e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.23422 to 0.19269, saving model to outputs/step3_3_optuna\\trial_8\\models\\fold_7\\best_model.keras\n",
      "393/393 - 10s - 27ms/step - accuracy: 0.9028 - loss: 0.3738 - val_accuracy: 0.9585 - val_loss: 0.1927 - learning_rate: 5.8780e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 0.19269 to 0.18801, saving model to outputs/step3_3_optuna\\trial_8\\models\\fold_7\\best_model.keras\n",
      "393/393 - 7s - 17ms/step - accuracy: 0.9129 - loss: 0.3408 - val_accuracy: 0.9586 - val_loss: 0.1880 - learning_rate: 5.8780e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.18801 to 0.16682, saving model to outputs/step3_3_optuna\\trial_8\\models\\fold_7\\best_model.keras\n",
      "393/393 - 10s - 26ms/step - accuracy: 0.9222 - loss: 0.3145 - val_accuracy: 0.9643 - val_loss: 0.1668 - learning_rate: 5.8780e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.16682\n",
      "393/393 - 10s - 26ms/step - accuracy: 0.9240 - loss: 0.3005 - val_accuracy: 0.9559 - val_loss: 0.1867 - learning_rate: 5.8780e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 0.16682 to 0.13603, saving model to outputs/step3_3_optuna\\trial_8\\models\\fold_7\\best_model.keras\n",
      "393/393 - 10s - 27ms/step - accuracy: 0.9300 - loss: 0.2827 - val_accuracy: 0.9745 - val_loss: 0.1360 - learning_rate: 5.8780e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.13603\n",
      "393/393 - 10s - 26ms/step - accuracy: 0.9359 - loss: 0.2661 - val_accuracy: 0.9753 - val_loss: 0.1376 - learning_rate: 5.8780e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 0.13603 to 0.11885, saving model to outputs/step3_3_optuna\\trial_8\\models\\fold_7\\best_model.keras\n",
      "393/393 - 10s - 26ms/step - accuracy: 0.9389 - loss: 0.2547 - val_accuracy: 0.9811 - val_loss: 0.1189 - learning_rate: 5.8780e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 0.11885 to 0.11047, saving model to outputs/step3_3_optuna\\trial_8\\models\\fold_7\\best_model.keras\n",
      "393/393 - 7s - 17ms/step - accuracy: 0.9460 - loss: 0.2325 - val_accuracy: 0.9830 - val_loss: 0.1105 - learning_rate: 5.8780e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss improved from 0.11047 to 0.10320, saving model to outputs/step3_3_optuna\\trial_8\\models\\fold_7\\best_model.keras\n",
      "393/393 - 9s - 23ms/step - accuracy: 0.9486 - loss: 0.2217 - val_accuracy: 0.9871 - val_loss: 0.1032 - learning_rate: 5.8780e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.10320\n",
      "393/393 - 4s - 10ms/step - accuracy: 0.9508 - loss: 0.2156 - val_accuracy: 0.9835 - val_loss: 0.1095 - learning_rate: 5.8780e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 0.10320 to 0.10311, saving model to outputs/step3_3_optuna\\trial_8\\models\\fold_7\\best_model.keras\n",
      "393/393 - 4s - 10ms/step - accuracy: 0.9520 - loss: 0.2124 - val_accuracy: 0.9852 - val_loss: 0.1031 - learning_rate: 5.8780e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss improved from 0.10311 to 0.09337, saving model to outputs/step3_3_optuna\\trial_8\\models\\fold_7\\best_model.keras\n",
      "393/393 - 4s - 10ms/step - accuracy: 0.9543 - loss: 0.2030 - val_accuracy: 0.9885 - val_loss: 0.0934 - learning_rate: 5.8780e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.09337\n",
      "393/393 - 4s - 10ms/step - accuracy: 0.9596 - loss: 0.1902 - val_accuracy: 0.9883 - val_loss: 0.0934 - learning_rate: 5.8780e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.09337\n",
      "393/393 - 4s - 10ms/step - accuracy: 0.9586 - loss: 0.1903 - val_accuracy: 0.9861 - val_loss: 0.0974 - learning_rate: 5.8780e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.09337\n",
      "393/393 - 4s - 10ms/step - accuracy: 0.9591 - loss: 0.1865 - val_accuracy: 0.9867 - val_loss: 0.0955 - learning_rate: 5.8780e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss improved from 0.09337 to 0.08536, saving model to outputs/step3_3_optuna\\trial_8\\models\\fold_7\\best_model.keras\n",
      "393/393 - 5s - 13ms/step - accuracy: 0.9633 - loss: 0.1756 - val_accuracy: 0.9907 - val_loss: 0.0854 - learning_rate: 5.8780e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss improved from 0.08536 to 0.08095, saving model to outputs/step3_3_optuna\\trial_8\\models\\fold_7\\best_model.keras\n",
      "393/393 - 5s - 13ms/step - accuracy: 0.9629 - loss: 0.1714 - val_accuracy: 0.9915 - val_loss: 0.0809 - learning_rate: 5.8780e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.08095\n",
      "393/393 - 5s - 13ms/step - accuracy: 0.9659 - loss: 0.1672 - val_accuracy: 0.9900 - val_loss: 0.0836 - learning_rate: 5.8780e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.08095\n",
      "393/393 - 5s - 13ms/step - accuracy: 0.9651 - loss: 0.1631 - val_accuracy: 0.9911 - val_loss: 0.0811 - learning_rate: 5.8780e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.08095\n",
      "393/393 - 5s - 13ms/step - accuracy: 0.9660 - loss: 0.1607 - val_accuracy: 0.9908 - val_loss: 0.0813 - learning_rate: 5.8780e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss improved from 0.08095 to 0.07629, saving model to outputs/step3_3_optuna\\trial_8\\models\\fold_7\\best_model.keras\n",
      "393/393 - 4s - 11ms/step - accuracy: 0.9676 - loss: 0.1566 - val_accuracy: 0.9924 - val_loss: 0.0763 - learning_rate: 5.8780e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss improved from 0.07629 to 0.07145, saving model to outputs/step3_3_optuna\\trial_8\\models\\fold_7\\best_model.keras\n",
      "393/393 - 4s - 10ms/step - accuracy: 0.9710 - loss: 0.1472 - val_accuracy: 0.9935 - val_loss: 0.0715 - learning_rate: 5.8780e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.07145\n",
      "393/393 - 4s - 10ms/step - accuracy: 0.9706 - loss: 0.1481 - val_accuracy: 0.9916 - val_loss: 0.0764 - learning_rate: 5.8780e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.07145\n",
      "393/393 - 4s - 10ms/step - accuracy: 0.9722 - loss: 0.1435 - val_accuracy: 0.9899 - val_loss: 0.0781 - learning_rate: 5.8780e-04\n",
      "Restoring model weights from the end of the best epoch: 28.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 18:30:18,417] Trial 8 finished with value: 0.5725193124887816 and parameters: {'WINDOW_SIZE': 15, 'STRIDE': 1, 'gru1_units': 96, 'gru2_units': 96, 'dense_units': 96, 'dropout_rate': 0.48794670894222497, 'l2': 0.0002542689696416619, 'cf_filters': 64, 'cf_kernel': 5, 'cf_pool': 3, 'cf_drop': 0.1, 'cf_l2': 3.3869845763928826e-05, 'learning_rate': 0.0005877986759124606, 'batch_size': 96}. Best is trial 2 with value: 0.6388116314073684.\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.5725 ---\n",
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n",
      "학습 데이터가 29365개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 1.22021, saving model to outputs/step3_3_optuna\\trial_9\\models\\fold_7\\best_model.keras\n",
      "306/306 - 13s - 41ms/step - accuracy: 0.3181 - loss: 2.0496 - val_accuracy: 0.6186 - val_loss: 1.2202 - learning_rate: 1.4919e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 1.22021 to 0.71889, saving model to outputs/step3_3_optuna\\trial_9\\models\\fold_7\\best_model.keras\n",
      "306/306 - 10s - 32ms/step - accuracy: 0.5729 - loss: 1.2958 - val_accuracy: 0.7910 - val_loss: 0.7189 - learning_rate: 1.4919e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 0.71889 to 0.48478, saving model to outputs/step3_3_optuna\\trial_9\\models\\fold_7\\best_model.keras\n",
      "306/306 - 10s - 32ms/step - accuracy: 0.7030 - loss: 0.9292 - val_accuracy: 0.8535 - val_loss: 0.4848 - learning_rate: 1.4919e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 0.48478 to 0.35966, saving model to outputs/step3_3_optuna\\trial_9\\models\\fold_7\\best_model.keras\n",
      "306/306 - 10s - 33ms/step - accuracy: 0.7703 - loss: 0.7277 - val_accuracy: 0.8913 - val_loss: 0.3597 - learning_rate: 1.4919e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 0.35966 to 0.28464, saving model to outputs/step3_3_optuna\\trial_9\\models\\fold_7\\best_model.keras\n",
      "306/306 - 10s - 32ms/step - accuracy: 0.8123 - loss: 0.5996 - val_accuracy: 0.9172 - val_loss: 0.2846 - learning_rate: 1.4919e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 0.28464 to 0.23644, saving model to outputs/step3_3_optuna\\trial_9\\models\\fold_7\\best_model.keras\n",
      "306/306 - 10s - 32ms/step - accuracy: 0.8369 - loss: 0.5187 - val_accuracy: 0.9285 - val_loss: 0.2364 - learning_rate: 1.4919e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.23644 to 0.19972, saving model to outputs/step3_3_optuna\\trial_9\\models\\fold_7\\best_model.keras\n",
      "306/306 - 10s - 32ms/step - accuracy: 0.8592 - loss: 0.4566 - val_accuracy: 0.9408 - val_loss: 0.1997 - learning_rate: 1.4919e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 0.19972 to 0.16946, saving model to outputs/step3_3_optuna\\trial_9\\models\\fold_7\\best_model.keras\n",
      "306/306 - 16s - 53ms/step - accuracy: 0.8793 - loss: 0.3987 - val_accuracy: 0.9479 - val_loss: 0.1695 - learning_rate: 1.4919e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.16946 to 0.13530, saving model to outputs/step3_3_optuna\\trial_9\\models\\fold_7\\best_model.keras\n",
      "306/306 - 21s - 67ms/step - accuracy: 0.8900 - loss: 0.3613 - val_accuracy: 0.9622 - val_loss: 0.1353 - learning_rate: 1.4919e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 0.13530 to 0.11716, saving model to outputs/step3_3_optuna\\trial_9\\models\\fold_7\\best_model.keras\n",
      "306/306 - 16s - 51ms/step - accuracy: 0.8999 - loss: 0.3377 - val_accuracy: 0.9704 - val_loss: 0.1172 - learning_rate: 1.4919e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 0.11716 to 0.11684, saving model to outputs/step3_3_optuna\\trial_9\\models\\fold_7\\best_model.keras\n",
      "306/306 - 9s - 30ms/step - accuracy: 0.9134 - loss: 0.2941 - val_accuracy: 0.9646 - val_loss: 0.1168 - learning_rate: 1.4919e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 0.11684 to 0.09361, saving model to outputs/step3_3_optuna\\trial_9\\models\\fold_7\\best_model.keras\n",
      "306/306 - 10s - 33ms/step - accuracy: 0.9197 - loss: 0.2742 - val_accuracy: 0.9755 - val_loss: 0.0936 - learning_rate: 1.4919e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 0.09361 to 0.08673, saving model to outputs/step3_3_optuna\\trial_9\\models\\fold_7\\best_model.keras\n",
      "306/306 - 10s - 33ms/step - accuracy: 0.9219 - loss: 0.2610 - val_accuracy: 0.9759 - val_loss: 0.0867 - learning_rate: 1.4919e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 0.08673 to 0.08036, saving model to outputs/step3_3_optuna\\trial_9\\models\\fold_7\\best_model.keras\n",
      "306/306 - 10s - 34ms/step - accuracy: 0.9303 - loss: 0.2370 - val_accuracy: 0.9788 - val_loss: 0.0804 - learning_rate: 1.4919e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss improved from 0.08036 to 0.06936, saving model to outputs/step3_3_optuna\\trial_9\\models\\fold_7\\best_model.keras\n",
      "306/306 - 11s - 36ms/step - accuracy: 0.9370 - loss: 0.2223 - val_accuracy: 0.9826 - val_loss: 0.0694 - learning_rate: 1.4919e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss improved from 0.06936 to 0.06397, saving model to outputs/step3_3_optuna\\trial_9\\models\\fold_7\\best_model.keras\n",
      "306/306 - 13s - 43ms/step - accuracy: 0.9411 - loss: 0.2077 - val_accuracy: 0.9847 - val_loss: 0.0640 - learning_rate: 1.4919e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 0.06397 to 0.05939, saving model to outputs/step3_3_optuna\\trial_9\\models\\fold_7\\best_model.keras\n",
      "306/306 - 13s - 43ms/step - accuracy: 0.9435 - loss: 0.2012 - val_accuracy: 0.9867 - val_loss: 0.0594 - learning_rate: 1.4919e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss improved from 0.05939 to 0.05425, saving model to outputs/step3_3_optuna\\trial_9\\models\\fold_7\\best_model.keras\n",
      "306/306 - 13s - 43ms/step - accuracy: 0.9489 - loss: 0.1859 - val_accuracy: 0.9884 - val_loss: 0.0542 - learning_rate: 1.4919e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.05425\n",
      "306/306 - 11s - 37ms/step - accuracy: 0.9495 - loss: 0.1789 - val_accuracy: 0.9878 - val_loss: 0.0604 - learning_rate: 1.4919e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.05425\n",
      "306/306 - 10s - 33ms/step - accuracy: 0.9518 - loss: 0.1720 - val_accuracy: 0.9790 - val_loss: 0.0743 - learning_rate: 1.4919e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss improved from 0.05425 to 0.04940, saving model to outputs/step3_3_optuna\\trial_9\\models\\fold_7\\best_model.keras\n",
      "306/306 - 10s - 33ms/step - accuracy: 0.9553 - loss: 0.1614 - val_accuracy: 0.9899 - val_loss: 0.0494 - learning_rate: 1.4919e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss improved from 0.04940 to 0.04356, saving model to outputs/step3_3_optuna\\trial_9\\models\\fold_7\\best_model.keras\n",
      "306/306 - 10s - 33ms/step - accuracy: 0.9591 - loss: 0.1516 - val_accuracy: 0.9918 - val_loss: 0.0436 - learning_rate: 1.4919e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.04356\n",
      "306/306 - 10s - 32ms/step - accuracy: 0.9612 - loss: 0.1479 - val_accuracy: 0.9920 - val_loss: 0.0448 - learning_rate: 1.4919e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss improved from 0.04356 to 0.04147, saving model to outputs/step3_3_optuna\\trial_9\\models\\fold_7\\best_model.keras\n",
      "306/306 - 10s - 32ms/step - accuracy: 0.9602 - loss: 0.1448 - val_accuracy: 0.9937 - val_loss: 0.0415 - learning_rate: 1.4919e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss improved from 0.04147 to 0.03985, saving model to outputs/step3_3_optuna\\trial_9\\models\\fold_7\\best_model.keras\n",
      "306/306 - 10s - 32ms/step - accuracy: 0.9626 - loss: 0.1395 - val_accuracy: 0.9937 - val_loss: 0.0398 - learning_rate: 1.4919e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.03985\n",
      "306/306 - 10s - 32ms/step - accuracy: 0.9663 - loss: 0.1309 - val_accuracy: 0.9924 - val_loss: 0.0431 - learning_rate: 1.4919e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss improved from 0.03985 to 0.03861, saving model to outputs/step3_3_optuna\\trial_9\\models\\fold_7\\best_model.keras\n",
      "306/306 - 10s - 32ms/step - accuracy: 0.9677 - loss: 0.1228 - val_accuracy: 0.9940 - val_loss: 0.0386 - learning_rate: 1.4919e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss improved from 0.03861 to 0.03435, saving model to outputs/step3_3_optuna\\trial_9\\models\\fold_7\\best_model.keras\n",
      "306/306 - 10s - 32ms/step - accuracy: 0.9680 - loss: 0.1253 - val_accuracy: 0.9959 - val_loss: 0.0343 - learning_rate: 1.4919e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss improved from 0.03435 to 0.03409, saving model to outputs/step3_3_optuna\\trial_9\\models\\fold_7\\best_model.keras\n",
      "306/306 - 10s - 32ms/step - accuracy: 0.9703 - loss: 0.1177 - val_accuracy: 0.9955 - val_loss: 0.0341 - learning_rate: 1.4919e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss improved from 0.03409 to 0.03301, saving model to outputs/step3_3_optuna\\trial_9\\models\\fold_7\\best_model.keras\n",
      "306/306 - 10s - 32ms/step - accuracy: 0.9699 - loss: 0.1183 - val_accuracy: 0.9966 - val_loss: 0.0330 - learning_rate: 1.4919e-04\n",
      "Restoring model weights from the end of the best epoch: 30.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 18:35:59,033] Trial 9 finished with value: 0.602674001513331 and parameters: {'WINDOW_SIZE': 45, 'STRIDE': 1, 'gru1_units': 128, 'gru2_units': 96, 'dense_units': 64, 'dropout_rate': 0.36259774217862506, 'l2': 4.432698905333786e-05, 'cf_filters': 48, 'cf_kernel': 7, 'cf_pool': 2, 'cf_drop': 0.2, 'cf_l2': 3.0500043702872043e-05, 'learning_rate': 0.0001491911283367546, 'batch_size': 96}. Best is trial 2 with value: 0.6388116314073684.\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.6027 ---\n",
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n",
      "학습 데이터가 29365개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 1.14480, saving model to outputs/step3_3_optuna\\trial_10\\models\\fold_7\\best_model.keras\n",
      "306/306 - 6s - 21ms/step - accuracy: 0.3265 - loss: 1.9987 - val_accuracy: 0.7005 - val_loss: 1.1448 - learning_rate: 3.7157e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 1.14480 to 0.63412, saving model to outputs/step3_3_optuna\\trial_10\\models\\fold_7\\best_model.keras\n",
      "306/306 - 5s - 16ms/step - accuracy: 0.5533 - loss: 1.3417 - val_accuracy: 0.8486 - val_loss: 0.6341 - learning_rate: 3.7157e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 0.63412 to 0.39703, saving model to outputs/step3_3_optuna\\trial_10\\models\\fold_7\\best_model.keras\n",
      "306/306 - 5s - 16ms/step - accuracy: 0.6712 - loss: 0.9952 - val_accuracy: 0.9067 - val_loss: 0.3970 - learning_rate: 3.7157e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 0.39703 to 0.28573, saving model to outputs/step3_3_optuna\\trial_10\\models\\fold_7\\best_model.keras\n",
      "306/306 - 5s - 16ms/step - accuracy: 0.7358 - loss: 0.7963 - val_accuracy: 0.9330 - val_loss: 0.2857 - learning_rate: 3.7157e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 0.28573 to 0.20296, saving model to outputs/step3_3_optuna\\trial_10\\models\\fold_7\\best_model.keras\n",
      "306/306 - 5s - 16ms/step - accuracy: 0.7806 - loss: 0.6682 - val_accuracy: 0.9549 - val_loss: 0.2030 - learning_rate: 3.7157e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 0.20296 to 0.16845, saving model to outputs/step3_3_optuna\\trial_10\\models\\fold_7\\best_model.keras\n",
      "306/306 - 5s - 16ms/step - accuracy: 0.8177 - loss: 0.5658 - val_accuracy: 0.9579 - val_loss: 0.1684 - learning_rate: 3.7157e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.16845 to 0.12851, saving model to outputs/step3_3_optuna\\trial_10\\models\\fold_7\\best_model.keras\n",
      "306/306 - 5s - 16ms/step - accuracy: 0.8379 - loss: 0.5060 - val_accuracy: 0.9666 - val_loss: 0.1285 - learning_rate: 3.7157e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 0.12851 to 0.10618, saving model to outputs/step3_3_optuna\\trial_10\\models\\fold_7\\best_model.keras\n",
      "306/306 - 5s - 16ms/step - accuracy: 0.8572 - loss: 0.4544 - val_accuracy: 0.9760 - val_loss: 0.1062 - learning_rate: 3.7157e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.10618 to 0.09507, saving model to outputs/step3_3_optuna\\trial_10\\models\\fold_7\\best_model.keras\n",
      "306/306 - 5s - 16ms/step - accuracy: 0.8714 - loss: 0.4179 - val_accuracy: 0.9817 - val_loss: 0.0951 - learning_rate: 3.7157e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 0.09507 to 0.08984, saving model to outputs/step3_3_optuna\\trial_10\\models\\fold_7\\best_model.keras\n",
      "306/306 - 5s - 16ms/step - accuracy: 0.8839 - loss: 0.3870 - val_accuracy: 0.9814 - val_loss: 0.0898 - learning_rate: 3.7157e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 0.08984 to 0.07288, saving model to outputs/step3_3_optuna\\trial_10\\models\\fold_7\\best_model.keras\n",
      "306/306 - 5s - 16ms/step - accuracy: 0.8919 - loss: 0.3671 - val_accuracy: 0.9869 - val_loss: 0.0729 - learning_rate: 3.7157e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.07288\n",
      "306/306 - 5s - 16ms/step - accuracy: 0.8965 - loss: 0.3494 - val_accuracy: 0.9889 - val_loss: 0.0739 - learning_rate: 3.7157e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.07288\n",
      "306/306 - 5s - 16ms/step - accuracy: 0.9004 - loss: 0.3339 - val_accuracy: 0.9866 - val_loss: 0.0778 - learning_rate: 3.7157e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 0.07288 to 0.05948, saving model to outputs/step3_3_optuna\\trial_10\\models\\fold_7\\best_model.keras\n",
      "306/306 - 5s - 16ms/step - accuracy: 0.9081 - loss: 0.3123 - val_accuracy: 0.9930 - val_loss: 0.0595 - learning_rate: 3.7157e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.05948\n",
      "306/306 - 5s - 15ms/step - accuracy: 0.9081 - loss: 0.3095 - val_accuracy: 0.9914 - val_loss: 0.0669 - learning_rate: 3.7157e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss improved from 0.05948 to 0.05735, saving model to outputs/step3_3_optuna\\trial_10\\models\\fold_7\\best_model.keras\n",
      "306/306 - 5s - 16ms/step - accuracy: 0.9118 - loss: 0.3008 - val_accuracy: 0.9944 - val_loss: 0.0573 - learning_rate: 3.7157e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 0.05735 to 0.05642, saving model to outputs/step3_3_optuna\\trial_10\\models\\fold_7\\best_model.keras\n",
      "306/306 - 5s - 16ms/step - accuracy: 0.9167 - loss: 0.2779 - val_accuracy: 0.9946 - val_loss: 0.0564 - learning_rate: 3.7157e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss improved from 0.05642 to 0.05640, saving model to outputs/step3_3_optuna\\trial_10\\models\\fold_7\\best_model.keras\n",
      "306/306 - 5s - 15ms/step - accuracy: 0.9134 - loss: 0.2852 - val_accuracy: 0.9938 - val_loss: 0.0564 - learning_rate: 3.7157e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss improved from 0.05640 to 0.05402, saving model to outputs/step3_3_optuna\\trial_10\\models\\fold_7\\best_model.keras\n",
      "306/306 - 5s - 16ms/step - accuracy: 0.9218 - loss: 0.2662 - val_accuracy: 0.9946 - val_loss: 0.0540 - learning_rate: 3.7157e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.05402\n",
      "306/306 - 5s - 16ms/step - accuracy: 0.9185 - loss: 0.2739 - val_accuracy: 0.9915 - val_loss: 0.0595 - learning_rate: 3.7157e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss improved from 0.05402 to 0.04892, saving model to outputs/step3_3_optuna\\trial_10\\models\\fold_7\\best_model.keras\n",
      "306/306 - 5s - 18ms/step - accuracy: 0.9233 - loss: 0.2554 - val_accuracy: 0.9957 - val_loss: 0.0489 - learning_rate: 3.7157e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.04892\n",
      "306/306 - 6s - 19ms/step - accuracy: 0.9251 - loss: 0.2528 - val_accuracy: 0.9948 - val_loss: 0.0537 - learning_rate: 3.7157e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.04892\n",
      "306/306 - 7s - 24ms/step - accuracy: 0.9245 - loss: 0.2555 - val_accuracy: 0.9959 - val_loss: 0.0490 - learning_rate: 3.7157e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.04892\n",
      "306/306 - 10s - 34ms/step - accuracy: 0.9254 - loss: 0.2513 - val_accuracy: 0.9941 - val_loss: 0.0545 - learning_rate: 3.7157e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss improved from 0.04892 to 0.04667, saving model to outputs/step3_3_optuna\\trial_10\\models\\fold_7\\best_model.keras\n",
      "306/306 - 7s - 24ms/step - accuracy: 0.9269 - loss: 0.2426 - val_accuracy: 0.9961 - val_loss: 0.0467 - learning_rate: 3.7157e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.04667\n",
      "306/306 - 11s - 37ms/step - accuracy: 0.9293 - loss: 0.2352 - val_accuracy: 0.9962 - val_loss: 0.0490 - learning_rate: 3.7157e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss improved from 0.04667 to 0.04615, saving model to outputs/step3_3_optuna\\trial_10\\models\\fold_7\\best_model.keras\n",
      "306/306 - 7s - 24ms/step - accuracy: 0.9264 - loss: 0.2472 - val_accuracy: 0.9971 - val_loss: 0.0462 - learning_rate: 3.7157e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss improved from 0.04615 to 0.04493, saving model to outputs/step3_3_optuna\\trial_10\\models\\fold_7\\best_model.keras\n",
      "306/306 - 7s - 24ms/step - accuracy: 0.9306 - loss: 0.2334 - val_accuracy: 0.9970 - val_loss: 0.0449 - learning_rate: 3.7157e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.04493\n",
      "306/306 - 9s - 28ms/step - accuracy: 0.9282 - loss: 0.2399 - val_accuracy: 0.9975 - val_loss: 0.0453 - learning_rate: 3.7157e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.04493\n",
      "306/306 - 10s - 34ms/step - accuracy: 0.9328 - loss: 0.2211 - val_accuracy: 0.9967 - val_loss: 0.0457 - learning_rate: 3.7157e-04\n",
      "Restoring model weights from the end of the best epoch: 28.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 18:39:06,883] Trial 10 finished with value: 0.6558324397414802 and parameters: {'WINDOW_SIZE': 45, 'STRIDE': 1, 'gru1_units': 96, 'gru2_units': 0, 'dense_units': 32, 'dropout_rate': 0.5857731553524049, 'l2': 2.17231270042126e-05, 'cf_filters': 80, 'cf_kernel': 3, 'cf_pool': 3, 'cf_drop': 0.1, 'cf_l2': 0.00026302923188389915, 'learning_rate': 0.0003715699443102406, 'batch_size': 96}. Best is trial 10 with value: 0.6558324397414802.\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.6558 ---\n",
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n",
      "학습 데이터가 29365개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 1.06809, saving model to outputs/step3_3_optuna\\trial_11\\models\\fold_7\\best_model.keras\n",
      "306/306 - 12s - 38ms/step - accuracy: 0.3467 - loss: 1.9481 - val_accuracy: 0.7148 - val_loss: 1.0681 - learning_rate: 4.0775e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 1.06809 to 0.56735, saving model to outputs/step3_3_optuna\\trial_11\\models\\fold_7\\best_model.keras\n",
      "306/306 - 10s - 31ms/step - accuracy: 0.5821 - loss: 1.2619 - val_accuracy: 0.8653 - val_loss: 0.5674 - learning_rate: 4.0775e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 0.56735 to 0.35230, saving model to outputs/step3_3_optuna\\trial_11\\models\\fold_7\\best_model.keras\n",
      "306/306 - 11s - 37ms/step - accuracy: 0.7016 - loss: 0.9114 - val_accuracy: 0.9147 - val_loss: 0.3523 - learning_rate: 4.0775e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 0.35230 to 0.26244, saving model to outputs/step3_3_optuna\\trial_11\\models\\fold_7\\best_model.keras\n",
      "306/306 - 8s - 25ms/step - accuracy: 0.7615 - loss: 0.7282 - val_accuracy: 0.9356 - val_loss: 0.2624 - learning_rate: 4.0775e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 0.26244 to 0.18756, saving model to outputs/step3_3_optuna\\trial_11\\models\\fold_7\\best_model.keras\n",
      "306/306 - 10s - 34ms/step - accuracy: 0.8027 - loss: 0.6154 - val_accuracy: 0.9537 - val_loss: 0.1876 - learning_rate: 4.0775e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 0.18756 to 0.15452, saving model to outputs/step3_3_optuna\\trial_11\\models\\fold_7\\best_model.keras\n",
      "306/306 - 11s - 37ms/step - accuracy: 0.8414 - loss: 0.5100 - val_accuracy: 0.9583 - val_loss: 0.1545 - learning_rate: 4.0775e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.15452 to 0.11866, saving model to outputs/step3_3_optuna\\trial_11\\models\\fold_7\\best_model.keras\n",
      "306/306 - 10s - 34ms/step - accuracy: 0.8612 - loss: 0.4587 - val_accuracy: 0.9717 - val_loss: 0.1187 - learning_rate: 4.0775e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 0.11866 to 0.09911, saving model to outputs/step3_3_optuna\\trial_11\\models\\fold_7\\best_model.keras\n",
      "306/306 - 9s - 30ms/step - accuracy: 0.8752 - loss: 0.4077 - val_accuracy: 0.9768 - val_loss: 0.0991 - learning_rate: 4.0775e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.09911 to 0.09250, saving model to outputs/step3_3_optuna\\trial_11\\models\\fold_7\\best_model.keras\n",
      "306/306 - 10s - 34ms/step - accuracy: 0.8854 - loss: 0.3836 - val_accuracy: 0.9808 - val_loss: 0.0925 - learning_rate: 4.0775e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 0.09250 to 0.08086, saving model to outputs/step3_3_optuna\\trial_11\\models\\fold_7\\best_model.keras\n",
      "306/306 - 10s - 34ms/step - accuracy: 0.8931 - loss: 0.3557 - val_accuracy: 0.9849 - val_loss: 0.0809 - learning_rate: 4.0775e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 0.08086 to 0.06887, saving model to outputs/step3_3_optuna\\trial_11\\models\\fold_7\\best_model.keras\n",
      "306/306 - 10s - 34ms/step - accuracy: 0.9015 - loss: 0.3351 - val_accuracy: 0.9900 - val_loss: 0.0689 - learning_rate: 4.0775e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.06887\n",
      "306/306 - 10s - 34ms/step - accuracy: 0.9068 - loss: 0.3177 - val_accuracy: 0.9854 - val_loss: 0.0784 - learning_rate: 4.0775e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 0.06887 to 0.06562, saving model to outputs/step3_3_optuna\\trial_11\\models\\fold_7\\best_model.keras\n",
      "306/306 - 10s - 34ms/step - accuracy: 0.9097 - loss: 0.3063 - val_accuracy: 0.9907 - val_loss: 0.0656 - learning_rate: 4.0775e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.06562\n",
      "306/306 - 10s - 34ms/step - accuracy: 0.9174 - loss: 0.2837 - val_accuracy: 0.9884 - val_loss: 0.0720 - learning_rate: 4.0775e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss improved from 0.06562 to 0.05943, saving model to outputs/step3_3_optuna\\trial_11\\models\\fold_7\\best_model.keras\n",
      "306/306 - 8s - 25ms/step - accuracy: 0.9177 - loss: 0.2816 - val_accuracy: 0.9939 - val_loss: 0.0594 - learning_rate: 4.0775e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss improved from 0.05943 to 0.05825, saving model to outputs/step3_3_optuna\\trial_11\\models\\fold_7\\best_model.keras\n",
      "306/306 - 10s - 34ms/step - accuracy: 0.9206 - loss: 0.2759 - val_accuracy: 0.9939 - val_loss: 0.0582 - learning_rate: 4.0775e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 0.05825 to 0.05549, saving model to outputs/step3_3_optuna\\trial_11\\models\\fold_7\\best_model.keras\n",
      "306/306 - 10s - 33ms/step - accuracy: 0.9240 - loss: 0.2577 - val_accuracy: 0.9940 - val_loss: 0.0555 - learning_rate: 4.0775e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.05549\n",
      "306/306 - 11s - 37ms/step - accuracy: 0.9262 - loss: 0.2547 - val_accuracy: 0.9943 - val_loss: 0.0567 - learning_rate: 4.0775e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.05549\n",
      "306/306 - 9s - 30ms/step - accuracy: 0.9300 - loss: 0.2445 - val_accuracy: 0.9934 - val_loss: 0.0569 - learning_rate: 4.0775e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss improved from 0.05549 to 0.05163, saving model to outputs/step3_3_optuna\\trial_11\\models\\fold_7\\best_model.keras\n",
      "306/306 - 11s - 37ms/step - accuracy: 0.9246 - loss: 0.2565 - val_accuracy: 0.9953 - val_loss: 0.0516 - learning_rate: 4.0775e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss improved from 0.05163 to 0.04974, saving model to outputs/step3_3_optuna\\trial_11\\models\\fold_7\\best_model.keras\n",
      "306/306 - 7s - 24ms/step - accuracy: 0.9312 - loss: 0.2374 - val_accuracy: 0.9960 - val_loss: 0.0497 - learning_rate: 4.0775e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.04974\n",
      "306/306 - 7s - 24ms/step - accuracy: 0.9326 - loss: 0.2330 - val_accuracy: 0.9967 - val_loss: 0.0498 - learning_rate: 4.0775e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.04974\n",
      "306/306 - 10s - 34ms/step - accuracy: 0.9350 - loss: 0.2246 - val_accuracy: 0.9944 - val_loss: 0.0527 - learning_rate: 4.0775e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.04974\n",
      "306/306 - 7s - 24ms/step - accuracy: 0.9329 - loss: 0.2249 - val_accuracy: 0.9937 - val_loss: 0.0539 - learning_rate: 4.0775e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss improved from 0.04974 to 0.04638, saving model to outputs/step3_3_optuna\\trial_11\\models\\fold_7\\best_model.keras\n",
      "306/306 - 12s - 38ms/step - accuracy: 0.9368 - loss: 0.2217 - val_accuracy: 0.9968 - val_loss: 0.0464 - learning_rate: 4.0775e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.04638\n",
      "306/306 - 7s - 24ms/step - accuracy: 0.9391 - loss: 0.2126 - val_accuracy: 0.9966 - val_loss: 0.0479 - learning_rate: 4.0775e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.04638\n",
      "306/306 - 10s - 34ms/step - accuracy: 0.9366 - loss: 0.2173 - val_accuracy: 0.9965 - val_loss: 0.0476 - learning_rate: 4.0775e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.04638\n",
      "306/306 - 8s - 25ms/step - accuracy: 0.9389 - loss: 0.2158 - val_accuracy: 0.9943 - val_loss: 0.0516 - learning_rate: 4.0775e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.04638\n",
      "306/306 - 10s - 34ms/step - accuracy: 0.9414 - loss: 0.2039 - val_accuracy: 0.9968 - val_loss: 0.0482 - learning_rate: 4.0775e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.04638\n",
      "306/306 - 7s - 24ms/step - accuracy: 0.9397 - loss: 0.2086 - val_accuracy: 0.9952 - val_loss: 0.0516 - learning_rate: 4.0775e-04\n",
      "Epoch 30: early stopping\n",
      "Restoring model weights from the end of the best epoch: 25.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 18:44:06,641] Trial 11 finished with value: 0.6448798240963777 and parameters: {'WINDOW_SIZE': 45, 'STRIDE': 1, 'gru1_units': 96, 'gru2_units': 0, 'dense_units': 32, 'dropout_rate': 0.573466301602085, 'l2': 2.2221063164991225e-05, 'cf_filters': 80, 'cf_kernel': 3, 'cf_pool': 3, 'cf_drop': 0.1, 'cf_l2': 0.0002620585724485123, 'learning_rate': 0.0004077526018071077, 'batch_size': 96}. Best is trial 10 with value: 0.6558324397414802.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.6449 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n",
      "학습 데이터가 9972개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 1.87460, saving model to outputs/step3_3_optuna\\trial_12\\models\\fold_7\\best_model.keras\n",
      "104/104 - 7s - 69ms/step - accuracy: 0.1885 - loss: 2.3789 - val_accuracy: 0.4426 - val_loss: 1.8746 - learning_rate: 3.8218e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 1.87460 to 1.46331, saving model to outputs/step3_3_optuna\\trial_12\\models\\fold_7\\best_model.keras\n",
      "104/104 - 4s - 43ms/step - accuracy: 0.3320 - loss: 1.9794 - val_accuracy: 0.5735 - val_loss: 1.4633 - learning_rate: 3.8218e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 1.46331 to 1.08998, saving model to outputs/step3_3_optuna\\trial_12\\models\\fold_7\\best_model.keras\n",
      "104/104 - 5s - 51ms/step - accuracy: 0.4338 - loss: 1.6805 - val_accuracy: 0.6912 - val_loss: 1.0900 - learning_rate: 3.8218e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 1.08998 to 0.89690, saving model to outputs/step3_3_optuna\\trial_12\\models\\fold_7\\best_model.keras\n",
      "104/104 - 5s - 48ms/step - accuracy: 0.4968 - loss: 1.4893 - val_accuracy: 0.7598 - val_loss: 0.8969 - learning_rate: 3.8218e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 0.89690 to 0.74082, saving model to outputs/step3_3_optuna\\trial_12\\models\\fold_7\\best_model.keras\n",
      "104/104 - 5s - 50ms/step - accuracy: 0.5558 - loss: 1.3201 - val_accuracy: 0.7997 - val_loss: 0.7408 - learning_rate: 3.8218e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 0.74082 to 0.63757, saving model to outputs/step3_3_optuna\\trial_12\\models\\fold_7\\best_model.keras\n",
      "104/104 - 5s - 52ms/step - accuracy: 0.6040 - loss: 1.1737 - val_accuracy: 0.8297 - val_loss: 0.6376 - learning_rate: 3.8218e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.63757 to 0.54032, saving model to outputs/step3_3_optuna\\trial_12\\models\\fold_7\\best_model.keras\n",
      "104/104 - 5s - 48ms/step - accuracy: 0.6348 - loss: 1.0753 - val_accuracy: 0.8475 - val_loss: 0.5403 - learning_rate: 3.8218e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 0.54032 to 0.47050, saving model to outputs/step3_3_optuna\\trial_12\\models\\fold_7\\best_model.keras\n",
      "104/104 - 5s - 50ms/step - accuracy: 0.6668 - loss: 0.9776 - val_accuracy: 0.8732 - val_loss: 0.4705 - learning_rate: 3.8218e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.47050 to 0.42194, saving model to outputs/step3_3_optuna\\trial_12\\models\\fold_7\\best_model.keras\n",
      "104/104 - 5s - 50ms/step - accuracy: 0.6974 - loss: 0.9077 - val_accuracy: 0.8875 - val_loss: 0.4219 - learning_rate: 3.8218e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 0.42194 to 0.36644, saving model to outputs/step3_3_optuna\\trial_12\\models\\fold_7\\best_model.keras\n",
      "104/104 - 5s - 50ms/step - accuracy: 0.7137 - loss: 0.8627 - val_accuracy: 0.9102 - val_loss: 0.3664 - learning_rate: 3.8218e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 0.36644 to 0.33747, saving model to outputs/step3_3_optuna\\trial_12\\models\\fold_7\\best_model.keras\n",
      "104/104 - 5s - 50ms/step - accuracy: 0.7320 - loss: 0.8073 - val_accuracy: 0.9201 - val_loss: 0.3375 - learning_rate: 3.8218e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 0.33747 to 0.30136, saving model to outputs/step3_3_optuna\\trial_12\\models\\fold_7\\best_model.keras\n",
      "104/104 - 5s - 52ms/step - accuracy: 0.7537 - loss: 0.7490 - val_accuracy: 0.9253 - val_loss: 0.3014 - learning_rate: 3.8218e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 0.30136 to 0.27087, saving model to outputs/step3_3_optuna\\trial_12\\models\\fold_7\\best_model.keras\n",
      "104/104 - 5s - 47ms/step - accuracy: 0.7663 - loss: 0.7113 - val_accuracy: 0.9381 - val_loss: 0.2709 - learning_rate: 3.8218e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 0.27087 to 0.24670, saving model to outputs/step3_3_optuna\\trial_12\\models\\fold_7\\best_model.keras\n",
      "104/104 - 5s - 51ms/step - accuracy: 0.7828 - loss: 0.6712 - val_accuracy: 0.9400 - val_loss: 0.2467 - learning_rate: 3.8218e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss improved from 0.24670 to 0.23365, saving model to outputs/step3_3_optuna\\trial_12\\models\\fold_7\\best_model.keras\n",
      "104/104 - 5s - 50ms/step - accuracy: 0.7922 - loss: 0.6521 - val_accuracy: 0.9424 - val_loss: 0.2336 - learning_rate: 3.8218e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss improved from 0.23365 to 0.20816, saving model to outputs/step3_3_optuna\\trial_12\\models\\fold_7\\best_model.keras\n",
      "104/104 - 3s - 25ms/step - accuracy: 0.8087 - loss: 0.6079 - val_accuracy: 0.9482 - val_loss: 0.2082 - learning_rate: 3.8218e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 0.20816 to 0.19970, saving model to outputs/step3_3_optuna\\trial_12\\models\\fold_7\\best_model.keras\n",
      "104/104 - 5s - 51ms/step - accuracy: 0.8156 - loss: 0.5840 - val_accuracy: 0.9543 - val_loss: 0.1997 - learning_rate: 3.8218e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss improved from 0.19970 to 0.18094, saving model to outputs/step3_3_optuna\\trial_12\\models\\fold_7\\best_model.keras\n",
      "104/104 - 5s - 48ms/step - accuracy: 0.8300 - loss: 0.5429 - val_accuracy: 0.9557 - val_loss: 0.1809 - learning_rate: 3.8218e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss improved from 0.18094 to 0.17736, saving model to outputs/step3_3_optuna\\trial_12\\models\\fold_7\\best_model.keras\n",
      "104/104 - 5s - 50ms/step - accuracy: 0.8281 - loss: 0.5410 - val_accuracy: 0.9591 - val_loss: 0.1774 - learning_rate: 3.8218e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss improved from 0.17736 to 0.14710, saving model to outputs/step3_3_optuna\\trial_12\\models\\fold_7\\best_model.keras\n",
      "104/104 - 5s - 50ms/step - accuracy: 0.8496 - loss: 0.4906 - val_accuracy: 0.9686 - val_loss: 0.1471 - learning_rate: 3.8218e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss improved from 0.14710 to 0.14507, saving model to outputs/step3_3_optuna\\trial_12\\models\\fold_7\\best_model.keras\n",
      "104/104 - 3s - 26ms/step - accuracy: 0.8468 - loss: 0.4900 - val_accuracy: 0.9671 - val_loss: 0.1451 - learning_rate: 3.8218e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss improved from 0.14507 to 0.13909, saving model to outputs/step3_3_optuna\\trial_12\\models\\fold_7\\best_model.keras\n",
      "104/104 - 5s - 49ms/step - accuracy: 0.8547 - loss: 0.4739 - val_accuracy: 0.9726 - val_loss: 0.1391 - learning_rate: 3.8218e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss improved from 0.13909 to 0.12704, saving model to outputs/step3_3_optuna\\trial_12\\models\\fold_7\\best_model.keras\n",
      "104/104 - 5s - 50ms/step - accuracy: 0.8525 - loss: 0.4801 - val_accuracy: 0.9729 - val_loss: 0.1270 - learning_rate: 3.8218e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss improved from 0.12704 to 0.12585, saving model to outputs/step3_3_optuna\\trial_12\\models\\fold_7\\best_model.keras\n",
      "104/104 - 5s - 50ms/step - accuracy: 0.8620 - loss: 0.4526 - val_accuracy: 0.9746 - val_loss: 0.1258 - learning_rate: 3.8218e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss improved from 0.12585 to 0.11804, saving model to outputs/step3_3_optuna\\trial_12\\models\\fold_7\\best_model.keras\n",
      "104/104 - 3s - 25ms/step - accuracy: 0.8742 - loss: 0.4230 - val_accuracy: 0.9755 - val_loss: 0.1180 - learning_rate: 3.8218e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss improved from 0.11804 to 0.11242, saving model to outputs/step3_3_optuna\\trial_12\\models\\fold_7\\best_model.keras\n",
      "104/104 - 5s - 50ms/step - accuracy: 0.8747 - loss: 0.4175 - val_accuracy: 0.9777 - val_loss: 0.1124 - learning_rate: 3.8218e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss improved from 0.11242 to 0.10953, saving model to outputs/step3_3_optuna\\trial_12\\models\\fold_7\\best_model.keras\n",
      "104/104 - 5s - 50ms/step - accuracy: 0.8741 - loss: 0.4181 - val_accuracy: 0.9794 - val_loss: 0.1095 - learning_rate: 3.8218e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.10953\n",
      "104/104 - 3s - 26ms/step - accuracy: 0.8850 - loss: 0.3943 - val_accuracy: 0.9673 - val_loss: 0.1390 - learning_rate: 3.8218e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss improved from 0.10953 to 0.09220, saving model to outputs/step3_3_optuna\\trial_12\\models\\fold_7\\best_model.keras\n",
      "104/104 - 3s - 25ms/step - accuracy: 0.8804 - loss: 0.4006 - val_accuracy: 0.9855 - val_loss: 0.0922 - learning_rate: 3.8218e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.09220\n",
      "104/104 - 5s - 49ms/step - accuracy: 0.8845 - loss: 0.3900 - val_accuracy: 0.9770 - val_loss: 0.1117 - learning_rate: 3.8218e-04\n",
      "Restoring model weights from the end of the best epoch: 29.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 18:46:37,495] Trial 12 finished with value: 0.6176424717710046 and parameters: {'WINDOW_SIZE': 45, 'STRIDE': 3, 'gru1_units': 96, 'gru2_units': 0, 'dense_units': 32, 'dropout_rate': 0.5923024917085505, 'l2': 2.386639023859599e-05, 'cf_filters': 80, 'cf_kernel': 3, 'cf_pool': 3, 'cf_drop': 0.1, 'cf_l2': 0.00028566245253586317, 'learning_rate': 0.00038217564269368165, 'batch_size': 96}. Best is trial 10 with value: 0.6558324397414802.\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.6176 ---\n",
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n",
      "학습 데이터가 29365개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 0.99590, saving model to outputs/step3_3_optuna\\trial_13\\models\\fold_7\\best_model.keras\n",
      "306/306 - 14s - 45ms/step - accuracy: 0.3587 - loss: 1.9126 - val_accuracy: 0.7193 - val_loss: 0.9959 - learning_rate: 4.0756e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 0.99590 to 0.52105, saving model to outputs/step3_3_optuna\\trial_13\\models\\fold_7\\best_model.keras\n",
      "306/306 - 9s - 28ms/step - accuracy: 0.6093 - loss: 1.1796 - val_accuracy: 0.8691 - val_loss: 0.5211 - learning_rate: 4.0756e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 0.52105 to 0.32343, saving model to outputs/step3_3_optuna\\trial_13\\models\\fold_7\\best_model.keras\n",
      "306/306 - 10s - 34ms/step - accuracy: 0.7196 - loss: 0.8564 - val_accuracy: 0.9204 - val_loss: 0.3234 - learning_rate: 4.0756e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 0.32343 to 0.22399, saving model to outputs/step3_3_optuna\\trial_13\\models\\fold_7\\best_model.keras\n",
      "306/306 - 11s - 37ms/step - accuracy: 0.7785 - loss: 0.6865 - val_accuracy: 0.9476 - val_loss: 0.2240 - learning_rate: 4.0756e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 0.22399 to 0.15948, saving model to outputs/step3_3_optuna\\trial_13\\models\\fold_7\\best_model.keras\n",
      "306/306 - 9s - 30ms/step - accuracy: 0.8188 - loss: 0.5716 - val_accuracy: 0.9612 - val_loss: 0.1595 - learning_rate: 4.0756e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 0.15948 to 0.12865, saving model to outputs/step3_3_optuna\\trial_13\\models\\fold_7\\best_model.keras\n",
      "306/306 - 11s - 37ms/step - accuracy: 0.8557 - loss: 0.4749 - val_accuracy: 0.9683 - val_loss: 0.1286 - learning_rate: 4.0756e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.12865 to 0.12256, saving model to outputs/step3_3_optuna\\trial_13\\models\\fold_7\\best_model.keras\n",
      "306/306 - 9s - 30ms/step - accuracy: 0.8710 - loss: 0.4211 - val_accuracy: 0.9680 - val_loss: 0.1226 - learning_rate: 4.0756e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 0.12256 to 0.08458, saving model to outputs/step3_3_optuna\\trial_13\\models\\fold_7\\best_model.keras\n",
      "306/306 - 11s - 37ms/step - accuracy: 0.8866 - loss: 0.3796 - val_accuracy: 0.9823 - val_loss: 0.0846 - learning_rate: 4.0756e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.08458 to 0.08242, saving model to outputs/step3_3_optuna\\trial_13\\models\\fold_7\\best_model.keras\n",
      "306/306 - 9s - 30ms/step - accuracy: 0.8958 - loss: 0.3530 - val_accuracy: 0.9837 - val_loss: 0.0824 - learning_rate: 4.0756e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 0.08242 to 0.07058, saving model to outputs/step3_3_optuna\\trial_13\\models\\fold_7\\best_model.keras\n",
      "306/306 - 10s - 34ms/step - accuracy: 0.9031 - loss: 0.3254 - val_accuracy: 0.9870 - val_loss: 0.0706 - learning_rate: 4.0756e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 0.07058 to 0.06521, saving model to outputs/step3_3_optuna\\trial_13\\models\\fold_7\\best_model.keras\n",
      "306/306 - 10s - 34ms/step - accuracy: 0.9111 - loss: 0.3059 - val_accuracy: 0.9893 - val_loss: 0.0652 - learning_rate: 4.0756e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.06521\n",
      "306/306 - 8s - 25ms/step - accuracy: 0.9176 - loss: 0.2874 - val_accuracy: 0.9877 - val_loss: 0.0691 - learning_rate: 4.0756e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.06521\n",
      "306/306 - 8s - 25ms/step - accuracy: 0.9239 - loss: 0.2656 - val_accuracy: 0.9889 - val_loss: 0.0652 - learning_rate: 4.0756e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 0.06521 to 0.05657, saving model to outputs/step3_3_optuna\\trial_13\\models\\fold_7\\best_model.keras\n",
      "306/306 - 8s - 25ms/step - accuracy: 0.9285 - loss: 0.2535 - val_accuracy: 0.9920 - val_loss: 0.0566 - learning_rate: 4.0756e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss improved from 0.05657 to 0.05467, saving model to outputs/step3_3_optuna\\trial_13\\models\\fold_7\\best_model.keras\n",
      "306/306 - 10s - 34ms/step - accuracy: 0.9297 - loss: 0.2471 - val_accuracy: 0.9932 - val_loss: 0.0547 - learning_rate: 4.0756e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss improved from 0.05467 to 0.04998, saving model to outputs/step3_3_optuna\\trial_13\\models\\fold_7\\best_model.keras\n",
      "306/306 - 11s - 37ms/step - accuracy: 0.9329 - loss: 0.2438 - val_accuracy: 0.9954 - val_loss: 0.0500 - learning_rate: 4.0756e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 0.04998 to 0.04586, saving model to outputs/step3_3_optuna\\trial_13\\models\\fold_7\\best_model.keras\n",
      "306/306 - 9s - 30ms/step - accuracy: 0.9348 - loss: 0.2306 - val_accuracy: 0.9964 - val_loss: 0.0459 - learning_rate: 4.0756e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.04586\n",
      "306/306 - 9s - 28ms/step - accuracy: 0.9374 - loss: 0.2212 - val_accuracy: 0.9946 - val_loss: 0.0523 - learning_rate: 4.0756e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.04586\n",
      "306/306 - 9s - 30ms/step - accuracy: 0.9397 - loss: 0.2195 - val_accuracy: 0.9959 - val_loss: 0.0476 - learning_rate: 4.0756e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.04586\n",
      "306/306 - 10s - 34ms/step - accuracy: 0.9376 - loss: 0.2206 - val_accuracy: 0.9960 - val_loss: 0.0467 - learning_rate: 4.0756e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss improved from 0.04586 to 0.04505, saving model to outputs/step3_3_optuna\\trial_13\\models\\fold_7\\best_model.keras\n",
      "306/306 - 10s - 34ms/step - accuracy: 0.9425 - loss: 0.2025 - val_accuracy: 0.9969 - val_loss: 0.0450 - learning_rate: 4.0756e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.04505\n",
      "306/306 - 10s - 33ms/step - accuracy: 0.9428 - loss: 0.2021 - val_accuracy: 0.9924 - val_loss: 0.0548 - learning_rate: 4.0756e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.04505\n",
      "306/306 - 11s - 37ms/step - accuracy: 0.9448 - loss: 0.1947 - val_accuracy: 0.9926 - val_loss: 0.0549 - learning_rate: 4.0756e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss improved from 0.04505 to 0.04396, saving model to outputs/step3_3_optuna\\trial_13\\models\\fold_7\\best_model.keras\n",
      "306/306 - 9s - 30ms/step - accuracy: 0.9406 - loss: 0.2086 - val_accuracy: 0.9971 - val_loss: 0.0440 - learning_rate: 4.0756e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss improved from 0.04396 to 0.04124, saving model to outputs/step3_3_optuna\\trial_13\\models\\fold_7\\best_model.keras\n",
      "306/306 - 10s - 33ms/step - accuracy: 0.9436 - loss: 0.1907 - val_accuracy: 0.9975 - val_loss: 0.0412 - learning_rate: 4.0756e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.04124\n",
      "306/306 - 9s - 28ms/step - accuracy: 0.9482 - loss: 0.1823 - val_accuracy: 0.9958 - val_loss: 0.0466 - learning_rate: 4.0756e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.04124\n",
      "306/306 - 9s - 30ms/step - accuracy: 0.9459 - loss: 0.1954 - val_accuracy: 0.9946 - val_loss: 0.0528 - learning_rate: 4.0756e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.04124\n",
      "306/306 - 10s - 34ms/step - accuracy: 0.9460 - loss: 0.1920 - val_accuracy: 0.9961 - val_loss: 0.0467 - learning_rate: 4.0756e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.04124\n",
      "306/306 - 7s - 24ms/step - accuracy: 0.9495 - loss: 0.1820 - val_accuracy: 0.9973 - val_loss: 0.0422 - learning_rate: 4.0756e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss improved from 0.04124 to 0.04042, saving model to outputs/step3_3_optuna\\trial_13\\models\\fold_7\\best_model.keras\n",
      "306/306 - 7s - 24ms/step - accuracy: 0.9512 - loss: 0.1700 - val_accuracy: 0.9982 - val_loss: 0.0404 - learning_rate: 4.0756e-04\n",
      "Restoring model weights from the end of the best epoch: 30.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 18:51:41,950] Trial 13 finished with value: 0.6127454329353714 and parameters: {'WINDOW_SIZE': 45, 'STRIDE': 1, 'gru1_units': 96, 'gru2_units': 0, 'dense_units': 32, 'dropout_rate': 0.5544602679953237, 'l2': 2.3957218762154602e-05, 'cf_filters': 80, 'cf_kernel': 3, 'cf_pool': 3, 'cf_drop': 0.1, 'cf_l2': 0.0002254827586861925, 'learning_rate': 0.0004075554026066012, 'batch_size': 96}. Best is trial 10 with value: 0.6558324397414802.\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.6127 ---\n",
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n",
      "학습 데이터가 33515개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 1.22935, saving model to outputs/step3_3_optuna\\trial_14\\models\\fold_7\\best_model.keras\n",
      "350/350 - 13s - 36ms/step - accuracy: 0.3293 - loss: 2.0039 - val_accuracy: 0.6553 - val_loss: 1.2293 - learning_rate: 3.1822e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 1.22935 to 0.75770, saving model to outputs/step3_3_optuna\\trial_14\\models\\fold_7\\best_model.keras\n",
      "350/350 - 9s - 27ms/step - accuracy: 0.5234 - loss: 1.4120 - val_accuracy: 0.7720 - val_loss: 0.7577 - learning_rate: 3.1822e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 0.75770 to 0.45655, saving model to outputs/step3_3_optuna\\trial_14\\models\\fold_7\\best_model.keras\n",
      "350/350 - 8s - 23ms/step - accuracy: 0.6460 - loss: 1.0680 - val_accuracy: 0.8779 - val_loss: 0.4566 - learning_rate: 3.1822e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 0.45655 to 0.28926, saving model to outputs/step3_3_optuna\\trial_14\\models\\fold_7\\best_model.keras\n",
      "350/350 - 11s - 32ms/step - accuracy: 0.7291 - loss: 0.8233 - val_accuracy: 0.9200 - val_loss: 0.2893 - learning_rate: 3.1822e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 0.28926 to 0.22681, saving model to outputs/step3_3_optuna\\trial_14\\models\\fold_7\\best_model.keras\n",
      "350/350 - 9s - 27ms/step - accuracy: 0.7844 - loss: 0.6618 - val_accuracy: 0.9377 - val_loss: 0.2268 - learning_rate: 3.1822e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 0.22681 to 0.17250, saving model to outputs/step3_3_optuna\\trial_14\\models\\fold_7\\best_model.keras\n",
      "350/350 - 10s - 30ms/step - accuracy: 0.8191 - loss: 0.5586 - val_accuracy: 0.9524 - val_loss: 0.1725 - learning_rate: 3.1822e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.17250 to 0.14183, saving model to outputs/step3_3_optuna\\trial_14\\models\\fold_7\\best_model.keras\n",
      "350/350 - 8s - 23ms/step - accuracy: 0.8408 - loss: 0.5014 - val_accuracy: 0.9602 - val_loss: 0.1418 - learning_rate: 3.1822e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 0.14183 to 0.12173, saving model to outputs/step3_3_optuna\\trial_14\\models\\fold_7\\best_model.keras\n",
      "350/350 - 9s - 26ms/step - accuracy: 0.8607 - loss: 0.4422 - val_accuracy: 0.9663 - val_loss: 0.1217 - learning_rate: 3.1822e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.12173 to 0.10065, saving model to outputs/step3_3_optuna\\trial_14\\models\\fold_7\\best_model.keras\n",
      "350/350 - 10s - 29ms/step - accuracy: 0.8781 - loss: 0.4024 - val_accuracy: 0.9741 - val_loss: 0.1007 - learning_rate: 3.1822e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 0.10065 to 0.09114, saving model to outputs/step3_3_optuna\\trial_14\\models\\fold_7\\best_model.keras\n",
      "350/350 - 8s - 23ms/step - accuracy: 0.8908 - loss: 0.3645 - val_accuracy: 0.9776 - val_loss: 0.0911 - learning_rate: 3.1822e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 0.09114 to 0.07636, saving model to outputs/step3_3_optuna\\trial_14\\models\\fold_7\\best_model.keras\n",
      "350/350 - 10s - 30ms/step - accuracy: 0.8949 - loss: 0.3488 - val_accuracy: 0.9854 - val_loss: 0.0764 - learning_rate: 3.1822e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 0.07636 to 0.06626, saving model to outputs/step3_3_optuna\\trial_14\\models\\fold_7\\best_model.keras\n",
      "350/350 - 9s - 26ms/step - accuracy: 0.9029 - loss: 0.3300 - val_accuracy: 0.9893 - val_loss: 0.0663 - learning_rate: 3.1822e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.06626\n",
      "350/350 - 9s - 26ms/step - accuracy: 0.9087 - loss: 0.3097 - val_accuracy: 0.9847 - val_loss: 0.0728 - learning_rate: 3.1822e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 0.06626 to 0.05551, saving model to outputs/step3_3_optuna\\trial_14\\models\\fold_7\\best_model.keras\n",
      "350/350 - 10s - 29ms/step - accuracy: 0.9151 - loss: 0.2899 - val_accuracy: 0.9915 - val_loss: 0.0555 - learning_rate: 3.1822e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.05551\n",
      "350/350 - 10s - 29ms/step - accuracy: 0.9186 - loss: 0.2814 - val_accuracy: 0.9903 - val_loss: 0.0577 - learning_rate: 3.1822e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.05551\n",
      "350/350 - 10s - 29ms/step - accuracy: 0.9233 - loss: 0.2716 - val_accuracy: 0.9906 - val_loss: 0.0573 - learning_rate: 3.1822e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 0.05551 to 0.04877, saving model to outputs/step3_3_optuna\\trial_14\\models\\fold_7\\best_model.keras\n",
      "350/350 - 10s - 30ms/step - accuracy: 0.9269 - loss: 0.2616 - val_accuracy: 0.9939 - val_loss: 0.0488 - learning_rate: 3.1822e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss improved from 0.04877 to 0.04776, saving model to outputs/step3_3_optuna\\trial_14\\models\\fold_7\\best_model.keras\n",
      "350/350 - 10s - 29ms/step - accuracy: 0.9287 - loss: 0.2483 - val_accuracy: 0.9940 - val_loss: 0.0478 - learning_rate: 3.1822e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.04776\n",
      "350/350 - 11s - 32ms/step - accuracy: 0.9352 - loss: 0.2350 - val_accuracy: 0.9924 - val_loss: 0.0512 - learning_rate: 3.1822e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.04776\n",
      "350/350 - 9s - 25ms/step - accuracy: 0.9352 - loss: 0.2319 - val_accuracy: 0.9925 - val_loss: 0.0484 - learning_rate: 3.1822e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss improved from 0.04776 to 0.04417, saving model to outputs/step3_3_optuna\\trial_14\\models\\fold_7\\best_model.keras\n",
      "350/350 - 10s - 30ms/step - accuracy: 0.9390 - loss: 0.2190 - val_accuracy: 0.9946 - val_loss: 0.0442 - learning_rate: 3.1822e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.04417\n",
      "350/350 - 10s - 29ms/step - accuracy: 0.9366 - loss: 0.2195 - val_accuracy: 0.9942 - val_loss: 0.0468 - learning_rate: 3.1822e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss improved from 0.04417 to 0.04412, saving model to outputs/step3_3_optuna\\trial_14\\models\\fold_7\\best_model.keras\n",
      "350/350 - 9s - 27ms/step - accuracy: 0.9401 - loss: 0.2096 - val_accuracy: 0.9948 - val_loss: 0.0441 - learning_rate: 3.1822e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.04412\n",
      "350/350 - 10s - 29ms/step - accuracy: 0.9411 - loss: 0.2100 - val_accuracy: 0.9942 - val_loss: 0.0447 - learning_rate: 3.1822e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.04412\n",
      "350/350 - 10s - 29ms/step - accuracy: 0.9425 - loss: 0.2080 - val_accuracy: 0.9946 - val_loss: 0.0447 - learning_rate: 3.1822e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss improved from 0.04412 to 0.03803, saving model to outputs/step3_3_optuna\\trial_14\\models\\fold_7\\best_model.keras\n",
      "350/350 - 8s - 23ms/step - accuracy: 0.9428 - loss: 0.2040 - val_accuracy: 0.9969 - val_loss: 0.0380 - learning_rate: 3.1822e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.03803\n",
      "350/350 - 10s - 29ms/step - accuracy: 0.9434 - loss: 0.1984 - val_accuracy: 0.9950 - val_loss: 0.0430 - learning_rate: 3.1822e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.03803\n",
      "350/350 - 8s - 23ms/step - accuracy: 0.9452 - loss: 0.1943 - val_accuracy: 0.9961 - val_loss: 0.0396 - learning_rate: 3.1822e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.03803\n",
      "350/350 - 8s - 23ms/step - accuracy: 0.9462 - loss: 0.1924 - val_accuracy: 0.9956 - val_loss: 0.0423 - learning_rate: 3.1822e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss improved from 0.03803 to 0.03714, saving model to outputs/step3_3_optuna\\trial_14\\models\\fold_7\\best_model.keras\n",
      "350/350 - 10s - 30ms/step - accuracy: 0.9471 - loss: 0.1872 - val_accuracy: 0.9971 - val_loss: 0.0371 - learning_rate: 3.1822e-04\n",
      "Restoring model weights from the end of the best epoch: 30.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 18:56:44,512] Trial 14 finished with value: 0.5521345770855248 and parameters: {'WINDOW_SIZE': 30, 'STRIDE': 1, 'gru1_units': 160, 'gru2_units': 0, 'dense_units': 32, 'dropout_rate': 0.5567002257022075, 'l2': 3.0934168373081176e-05, 'cf_filters': 80, 'cf_kernel': 3, 'cf_pool': 3, 'cf_drop': 0.1, 'cf_l2': 0.0001514684517661583, 'learning_rate': 0.0003182226945569508, 'batch_size': 96}. Best is trial 10 with value: 0.6558324397414802.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.5521 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n",
      "학습 데이터가 9972개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 1.46809, saving model to outputs/step3_3_optuna\\trial_15\\models\\fold_7\\best_model.keras\n",
      "104/104 - 6s - 59ms/step - accuracy: 0.2717 - loss: 2.1743 - val_accuracy: 0.5662 - val_loss: 1.4681 - learning_rate: 4.8287e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 1.46809 to 0.94516, saving model to outputs/step3_3_optuna\\trial_15\\models\\fold_7\\best_model.keras\n",
      "104/104 - 5s - 47ms/step - accuracy: 0.4942 - loss: 1.5431 - val_accuracy: 0.7089 - val_loss: 0.9452 - learning_rate: 4.8287e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 0.94516 to 0.69728, saving model to outputs/step3_3_optuna\\trial_15\\models\\fold_7\\best_model.keras\n",
      "104/104 - 5s - 48ms/step - accuracy: 0.6036 - loss: 1.2119 - val_accuracy: 0.8077 - val_loss: 0.6973 - learning_rate: 4.8287e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 0.69728 to 0.54401, saving model to outputs/step3_3_optuna\\trial_15\\models\\fold_7\\best_model.keras\n",
      "104/104 - 3s - 25ms/step - accuracy: 0.6686 - loss: 1.0054 - val_accuracy: 0.8447 - val_loss: 0.5440 - learning_rate: 4.8287e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 0.54401 to 0.44006, saving model to outputs/step3_3_optuna\\trial_15\\models\\fold_7\\best_model.keras\n",
      "104/104 - 2s - 23ms/step - accuracy: 0.7197 - loss: 0.8521 - val_accuracy: 0.8727 - val_loss: 0.4401 - learning_rate: 4.8287e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 0.44006 to 0.37055, saving model to outputs/step3_3_optuna\\trial_15\\models\\fold_7\\best_model.keras\n",
      "104/104 - 2s - 23ms/step - accuracy: 0.7620 - loss: 0.7421 - val_accuracy: 0.8962 - val_loss: 0.3705 - learning_rate: 4.8287e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.37055 to 0.30533, saving model to outputs/step3_3_optuna\\trial_15\\models\\fold_7\\best_model.keras\n",
      "104/104 - 3s - 27ms/step - accuracy: 0.7907 - loss: 0.6550 - val_accuracy: 0.9156 - val_loss: 0.3053 - learning_rate: 4.8287e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 0.30533 to 0.24819, saving model to outputs/step3_3_optuna\\trial_15\\models\\fold_7\\best_model.keras\n",
      "104/104 - 5s - 47ms/step - accuracy: 0.8197 - loss: 0.5716 - val_accuracy: 0.9315 - val_loss: 0.2482 - learning_rate: 4.8287e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.24819 to 0.21449, saving model to outputs/step3_3_optuna\\trial_15\\models\\fold_7\\best_model.keras\n",
      "104/104 - 2s - 23ms/step - accuracy: 0.8431 - loss: 0.5105 - val_accuracy: 0.9390 - val_loss: 0.2145 - learning_rate: 4.8287e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 0.21449 to 0.19483, saving model to outputs/step3_3_optuna\\trial_15\\models\\fold_7\\best_model.keras\n",
      "104/104 - 3s - 25ms/step - accuracy: 0.8567 - loss: 0.4686 - val_accuracy: 0.9468 - val_loss: 0.1948 - learning_rate: 4.8287e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 0.19483 to 0.16803, saving model to outputs/step3_3_optuna\\trial_15\\models\\fold_7\\best_model.keras\n",
      "104/104 - 2s - 23ms/step - accuracy: 0.8735 - loss: 0.4261 - val_accuracy: 0.9523 - val_loss: 0.1680 - learning_rate: 4.8287e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 0.16803 to 0.14557, saving model to outputs/step3_3_optuna\\trial_15\\models\\fold_7\\best_model.keras\n",
      "104/104 - 2s - 23ms/step - accuracy: 0.8869 - loss: 0.3838 - val_accuracy: 0.9657 - val_loss: 0.1456 - learning_rate: 4.8287e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 0.14557 to 0.13237, saving model to outputs/step3_3_optuna\\trial_15\\models\\fold_7\\best_model.keras\n",
      "104/104 - 3s - 25ms/step - accuracy: 0.8995 - loss: 0.3517 - val_accuracy: 0.9686 - val_loss: 0.1324 - learning_rate: 4.8287e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 0.13237 to 0.12143, saving model to outputs/step3_3_optuna\\trial_15\\models\\fold_7\\best_model.keras\n",
      "104/104 - 3s - 25ms/step - accuracy: 0.9044 - loss: 0.3330 - val_accuracy: 0.9726 - val_loss: 0.1214 - learning_rate: 4.8287e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss improved from 0.12143 to 0.12127, saving model to outputs/step3_3_optuna\\trial_15\\models\\fold_7\\best_model.keras\n",
      "104/104 - 2s - 23ms/step - accuracy: 0.9121 - loss: 0.3159 - val_accuracy: 0.9695 - val_loss: 0.1213 - learning_rate: 4.8287e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss improved from 0.12127 to 0.10341, saving model to outputs/step3_3_optuna\\trial_15\\models\\fold_7\\best_model.keras\n",
      "104/104 - 2s - 23ms/step - accuracy: 0.9210 - loss: 0.2848 - val_accuracy: 0.9765 - val_loss: 0.1034 - learning_rate: 4.8287e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 0.10341 to 0.10203, saving model to outputs/step3_3_optuna\\trial_15\\models\\fold_7\\best_model.keras\n",
      "104/104 - 3s - 25ms/step - accuracy: 0.9221 - loss: 0.2854 - val_accuracy: 0.9761 - val_loss: 0.1020 - learning_rate: 4.8287e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss improved from 0.10203 to 0.09615, saving model to outputs/step3_3_optuna\\trial_15\\models\\fold_7\\best_model.keras\n",
      "104/104 - 3s - 25ms/step - accuracy: 0.9316 - loss: 0.2524 - val_accuracy: 0.9804 - val_loss: 0.0961 - learning_rate: 4.8287e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss improved from 0.09615 to 0.09398, saving model to outputs/step3_3_optuna\\trial_15\\models\\fold_7\\best_model.keras\n",
      "104/104 - 3s - 25ms/step - accuracy: 0.9326 - loss: 0.2468 - val_accuracy: 0.9799 - val_loss: 0.0940 - learning_rate: 4.8287e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss improved from 0.09398 to 0.07802, saving model to outputs/step3_3_optuna\\trial_15\\models\\fold_7\\best_model.keras\n",
      "104/104 - 3s - 25ms/step - accuracy: 0.9428 - loss: 0.2197 - val_accuracy: 0.9831 - val_loss: 0.0780 - learning_rate: 4.8287e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.07802\n",
      "104/104 - 5s - 47ms/step - accuracy: 0.9372 - loss: 0.2234 - val_accuracy: 0.9835 - val_loss: 0.0791 - learning_rate: 4.8287e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.07802\n",
      "104/104 - 2s - 22ms/step - accuracy: 0.9418 - loss: 0.2188 - val_accuracy: 0.9801 - val_loss: 0.0833 - learning_rate: 4.8287e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.07802\n",
      "104/104 - 2s - 23ms/step - accuracy: 0.9399 - loss: 0.2242 - val_accuracy: 0.9804 - val_loss: 0.0907 - learning_rate: 4.8287e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss improved from 0.07802 to 0.07350, saving model to outputs/step3_3_optuna\\trial_15\\models\\fold_7\\best_model.keras\n",
      "104/104 - 3s - 26ms/step - accuracy: 0.9485 - loss: 0.2055 - val_accuracy: 0.9857 - val_loss: 0.0735 - learning_rate: 4.8287e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss improved from 0.07350 to 0.06504, saving model to outputs/step3_3_optuna\\trial_15\\models\\fold_7\\best_model.keras\n",
      "104/104 - 3s - 25ms/step - accuracy: 0.9552 - loss: 0.1852 - val_accuracy: 0.9879 - val_loss: 0.0650 - learning_rate: 4.8287e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.06504\n",
      "104/104 - 3s - 24ms/step - accuracy: 0.9491 - loss: 0.1963 - val_accuracy: 0.9855 - val_loss: 0.0726 - learning_rate: 4.8287e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss improved from 0.06504 to 0.06347, saving model to outputs/step3_3_optuna\\trial_15\\models\\fold_7\\best_model.keras\n",
      "104/104 - 3s - 26ms/step - accuracy: 0.9554 - loss: 0.1855 - val_accuracy: 0.9884 - val_loss: 0.0635 - learning_rate: 4.8287e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss improved from 0.06347 to 0.05646, saving model to outputs/step3_3_optuna\\trial_15\\models\\fold_7\\best_model.keras\n",
      "104/104 - 3s - 25ms/step - accuracy: 0.9553 - loss: 0.1803 - val_accuracy: 0.9898 - val_loss: 0.0565 - learning_rate: 4.8287e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.05646\n",
      "104/104 - 3s - 25ms/step - accuracy: 0.9566 - loss: 0.1847 - val_accuracy: 0.9894 - val_loss: 0.0654 - learning_rate: 4.8287e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.05646\n",
      "104/104 - 3s - 27ms/step - accuracy: 0.9581 - loss: 0.1737 - val_accuracy: 0.9835 - val_loss: 0.0731 - learning_rate: 4.8287e-04\n",
      "Restoring model weights from the end of the best epoch: 28.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 18:58:19,476] Trial 15 finished with value: 0.6942727509197062 and parameters: {'WINDOW_SIZE': 45, 'STRIDE': 3, 'gru1_units': 96, 'gru2_units': 0, 'dense_units': 32, 'dropout_rate': 0.4263588477541502, 'l2': 1.1329840327518632e-05, 'cf_filters': 80, 'cf_kernel': 3, 'cf_pool': 3, 'cf_drop': 0.1, 'cf_l2': 0.00018797202596489932, 'learning_rate': 0.0004828676750191711, 'batch_size': 96}. Best is trial 15 with value: 0.6942727509197062.\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.6943 ---\n",
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n",
      "학습 데이터가 11356개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 1.61999, saving model to outputs/step3_3_optuna\\trial_16\\models\\fold_7\\best_model.keras\n",
      "119/119 - 10s - 81ms/step - accuracy: 0.2401 - loss: 2.2392 - val_accuracy: 0.5260 - val_loss: 1.6200 - learning_rate: 3.3441e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 1.61999 to 1.08481, saving model to outputs/step3_3_optuna\\trial_16\\models\\fold_7\\best_model.keras\n",
      "119/119 - 5s - 38ms/step - accuracy: 0.4476 - loss: 1.6645 - val_accuracy: 0.6797 - val_loss: 1.0848 - learning_rate: 3.3441e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 1.08481 to 0.84604, saving model to outputs/step3_3_optuna\\trial_16\\models\\fold_7\\best_model.keras\n",
      "119/119 - 2s - 20ms/step - accuracy: 0.5592 - loss: 1.3479 - val_accuracy: 0.7402 - val_loss: 0.8460 - learning_rate: 3.3441e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 0.84604 to 0.64217, saving model to outputs/step3_3_optuna\\trial_16\\models\\fold_7\\best_model.keras\n",
      "119/119 - 3s - 21ms/step - accuracy: 0.6361 - loss: 1.1288 - val_accuracy: 0.8175 - val_loss: 0.6422 - learning_rate: 3.3441e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 0.64217 to 0.54839, saving model to outputs/step3_3_optuna\\trial_16\\models\\fold_7\\best_model.keras\n",
      "119/119 - 2s - 21ms/step - accuracy: 0.6892 - loss: 0.9639 - val_accuracy: 0.8433 - val_loss: 0.5484 - learning_rate: 3.3441e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 0.54839 to 0.45143, saving model to outputs/step3_3_optuna\\trial_16\\models\\fold_7\\best_model.keras\n",
      "119/119 - 2s - 20ms/step - accuracy: 0.7273 - loss: 0.8430 - val_accuracy: 0.8713 - val_loss: 0.4514 - learning_rate: 3.3441e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.45143 to 0.37109, saving model to outputs/step3_3_optuna\\trial_16\\models\\fold_7\\best_model.keras\n",
      "119/119 - 3s - 22ms/step - accuracy: 0.7628 - loss: 0.7577 - val_accuracy: 0.8897 - val_loss: 0.3711 - learning_rate: 3.3441e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 0.37109 to 0.33480, saving model to outputs/step3_3_optuna\\trial_16\\models\\fold_7\\best_model.keras\n",
      "119/119 - 3s - 23ms/step - accuracy: 0.7849 - loss: 0.6789 - val_accuracy: 0.8976 - val_loss: 0.3348 - learning_rate: 3.3441e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.33480 to 0.30801, saving model to outputs/step3_3_optuna\\trial_16\\models\\fold_7\\best_model.keras\n",
      "119/119 - 3s - 23ms/step - accuracy: 0.8021 - loss: 0.6263 - val_accuracy: 0.9030 - val_loss: 0.3080 - learning_rate: 3.3441e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 0.30801 to 0.25539, saving model to outputs/step3_3_optuna\\trial_16\\models\\fold_7\\best_model.keras\n",
      "119/119 - 3s - 22ms/step - accuracy: 0.8217 - loss: 0.5716 - val_accuracy: 0.9264 - val_loss: 0.2554 - learning_rate: 3.3441e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 0.25539 to 0.23657, saving model to outputs/step3_3_optuna\\trial_16\\models\\fold_7\\best_model.keras\n",
      "119/119 - 2s - 21ms/step - accuracy: 0.8315 - loss: 0.5292 - val_accuracy: 0.9276 - val_loss: 0.2366 - learning_rate: 3.3441e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 0.23657 to 0.19324, saving model to outputs/step3_3_optuna\\trial_16\\models\\fold_7\\best_model.keras\n",
      "119/119 - 2s - 20ms/step - accuracy: 0.8403 - loss: 0.5041 - val_accuracy: 0.9451 - val_loss: 0.1932 - learning_rate: 3.3441e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.19324\n",
      "119/119 - 3s - 23ms/step - accuracy: 0.8603 - loss: 0.4520 - val_accuracy: 0.9407 - val_loss: 0.1998 - learning_rate: 3.3441e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 0.19324 to 0.18793, saving model to outputs/step3_3_optuna\\trial_16\\models\\fold_7\\best_model.keras\n",
      "119/119 - 2s - 20ms/step - accuracy: 0.8675 - loss: 0.4388 - val_accuracy: 0.9385 - val_loss: 0.1879 - learning_rate: 3.3441e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss improved from 0.18793 to 0.18149, saving model to outputs/step3_3_optuna\\trial_16\\models\\fold_7\\best_model.keras\n",
      "119/119 - 3s - 22ms/step - accuracy: 0.8700 - loss: 0.4168 - val_accuracy: 0.9419 - val_loss: 0.1815 - learning_rate: 3.3441e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss improved from 0.18149 to 0.15707, saving model to outputs/step3_3_optuna\\trial_16\\models\\fold_7\\best_model.keras\n",
      "119/119 - 3s - 22ms/step - accuracy: 0.8838 - loss: 0.3912 - val_accuracy: 0.9520 - val_loss: 0.1571 - learning_rate: 3.3441e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 0.15707 to 0.14582, saving model to outputs/step3_3_optuna\\trial_16\\models\\fold_7\\best_model.keras\n",
      "119/119 - 3s - 22ms/step - accuracy: 0.8830 - loss: 0.3787 - val_accuracy: 0.9579 - val_loss: 0.1458 - learning_rate: 3.3441e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.14582\n",
      "119/119 - 2s - 20ms/step - accuracy: 0.8952 - loss: 0.3511 - val_accuracy: 0.9521 - val_loss: 0.1599 - learning_rate: 3.3441e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss improved from 0.14582 to 0.12569, saving model to outputs/step3_3_optuna\\trial_16\\models\\fold_7\\best_model.keras\n",
      "119/119 - 3s - 22ms/step - accuracy: 0.9014 - loss: 0.3355 - val_accuracy: 0.9647 - val_loss: 0.1257 - learning_rate: 3.3441e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss improved from 0.12569 to 0.11208, saving model to outputs/step3_3_optuna\\trial_16\\models\\fold_7\\best_model.keras\n",
      "119/119 - 3s - 22ms/step - accuracy: 0.9074 - loss: 0.3196 - val_accuracy: 0.9710 - val_loss: 0.1121 - learning_rate: 3.3441e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.11208\n",
      "119/119 - 3s - 21ms/step - accuracy: 0.9104 - loss: 0.3029 - val_accuracy: 0.9704 - val_loss: 0.1121 - learning_rate: 3.3441e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss improved from 0.11208 to 0.09941, saving model to outputs/step3_3_optuna\\trial_16\\models\\fold_7\\best_model.keras\n",
      "119/119 - 3s - 24ms/step - accuracy: 0.9143 - loss: 0.2974 - val_accuracy: 0.9722 - val_loss: 0.0994 - learning_rate: 3.3441e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.09941\n",
      "119/119 - 5s - 41ms/step - accuracy: 0.9148 - loss: 0.2854 - val_accuracy: 0.9713 - val_loss: 0.1042 - learning_rate: 3.3441e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss improved from 0.09941 to 0.09434, saving model to outputs/step3_3_optuna\\trial_16\\models\\fold_7\\best_model.keras\n",
      "119/119 - 3s - 24ms/step - accuracy: 0.9244 - loss: 0.2672 - val_accuracy: 0.9746 - val_loss: 0.0943 - learning_rate: 3.3441e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss improved from 0.09434 to 0.08092, saving model to outputs/step3_3_optuna\\trial_16\\models\\fold_7\\best_model.keras\n",
      "119/119 - 5s - 42ms/step - accuracy: 0.9224 - loss: 0.2648 - val_accuracy: 0.9792 - val_loss: 0.0809 - learning_rate: 3.3441e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.08092\n",
      "119/119 - 3s - 23ms/step - accuracy: 0.9302 - loss: 0.2456 - val_accuracy: 0.9792 - val_loss: 0.0848 - learning_rate: 3.3441e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.08092\n",
      "119/119 - 3s - 22ms/step - accuracy: 0.9349 - loss: 0.2368 - val_accuracy: 0.9767 - val_loss: 0.0941 - learning_rate: 3.3441e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.08092\n",
      "119/119 - 2s - 20ms/step - accuracy: 0.9340 - loss: 0.2311 - val_accuracy: 0.9808 - val_loss: 0.0824 - learning_rate: 3.3441e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.08092\n",
      "119/119 - 2s - 20ms/step - accuracy: 0.9391 - loss: 0.2262 - val_accuracy: 0.9777 - val_loss: 0.0853 - learning_rate: 3.3441e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss improved from 0.08092 to 0.06967, saving model to outputs/step3_3_optuna\\trial_16\\models\\fold_7\\best_model.keras\n",
      "119/119 - 3s - 23ms/step - accuracy: 0.9354 - loss: 0.2219 - val_accuracy: 0.9837 - val_loss: 0.0697 - learning_rate: 3.3441e-04\n",
      "Restoring model weights from the end of the best epoch: 30.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 18:59:56,109] Trial 16 finished with value: 0.582387617700227 and parameters: {'WINDOW_SIZE': 30, 'STRIDE': 3, 'gru1_units': 96, 'gru2_units': 0, 'dense_units': 32, 'dropout_rate': 0.42013341366437545, 'l2': 1.035746951716247e-05, 'cf_filters': 80, 'cf_kernel': 3, 'cf_pool': 3, 'cf_drop': 0.1, 'cf_l2': 0.0001615775070330629, 'learning_rate': 0.00033441249879476696, 'batch_size': 96}. Best is trial 15 with value: 0.6942727509197062.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.5824 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n",
      "학습 데이터가 9972개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 1.09339, saving model to outputs/step3_3_optuna\\trial_17\\models\\fold_7\\best_model.keras\n",
      "156/156 - 7s - 45ms/step - accuracy: 0.3548 - loss: 1.9579 - val_accuracy: 0.6816 - val_loss: 1.0934 - learning_rate: 5.2064e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 1.09339 to 0.67831, saving model to outputs/step3_3_optuna\\trial_17\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 30ms/step - accuracy: 0.6104 - loss: 1.1976 - val_accuracy: 0.8074 - val_loss: 0.6783 - learning_rate: 5.2064e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 0.67831 to 0.49574, saving model to outputs/step3_3_optuna\\trial_17\\models\\fold_7\\best_model.keras\n",
      "156/156 - 3s - 21ms/step - accuracy: 0.7231 - loss: 0.8696 - val_accuracy: 0.8565 - val_loss: 0.4957 - learning_rate: 5.2064e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 0.49574 to 0.39270, saving model to outputs/step3_3_optuna\\trial_17\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.7821 - loss: 0.7029 - val_accuracy: 0.8860 - val_loss: 0.3927 - learning_rate: 5.2064e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 0.39270 to 0.27863, saving model to outputs/step3_3_optuna\\trial_17\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.8278 - loss: 0.5650 - val_accuracy: 0.9226 - val_loss: 0.2786 - learning_rate: 5.2064e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 0.27863 to 0.22945, saving model to outputs/step3_3_optuna\\trial_17\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.8566 - loss: 0.4815 - val_accuracy: 0.9342 - val_loss: 0.2294 - learning_rate: 5.2064e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.22945 to 0.22465, saving model to outputs/step3_3_optuna\\trial_17\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.8810 - loss: 0.4086 - val_accuracy: 0.9344 - val_loss: 0.2246 - learning_rate: 5.2064e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 0.22465 to 0.17772, saving model to outputs/step3_3_optuna\\trial_17\\models\\fold_7\\best_model.keras\n",
      "156/156 - 3s - 21ms/step - accuracy: 0.8904 - loss: 0.3634 - val_accuracy: 0.9528 - val_loss: 0.1777 - learning_rate: 5.2064e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.17772 to 0.13601, saving model to outputs/step3_3_optuna\\trial_17\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9125 - loss: 0.3159 - val_accuracy: 0.9680 - val_loss: 0.1360 - learning_rate: 5.2064e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.13601\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9250 - loss: 0.2894 - val_accuracy: 0.9668 - val_loss: 0.1421 - learning_rate: 5.2064e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 0.13601 to 0.11432, saving model to outputs/step3_3_optuna\\trial_17\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9275 - loss: 0.2611 - val_accuracy: 0.9763 - val_loss: 0.1143 - learning_rate: 5.2064e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 0.11432 to 0.10828, saving model to outputs/step3_3_optuna\\trial_17\\models\\fold_7\\best_model.keras\n",
      "156/156 - 3s - 21ms/step - accuracy: 0.9369 - loss: 0.2465 - val_accuracy: 0.9795 - val_loss: 0.1083 - learning_rate: 5.2064e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 0.10828 to 0.09731, saving model to outputs/step3_3_optuna\\trial_17\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9433 - loss: 0.2234 - val_accuracy: 0.9835 - val_loss: 0.0973 - learning_rate: 5.2064e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 0.09731 to 0.09652, saving model to outputs/step3_3_optuna\\trial_17\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9442 - loss: 0.2142 - val_accuracy: 0.9821 - val_loss: 0.0965 - learning_rate: 5.2064e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss improved from 0.09652 to 0.08525, saving model to outputs/step3_3_optuna\\trial_17\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9493 - loss: 0.2036 - val_accuracy: 0.9876 - val_loss: 0.0852 - learning_rate: 5.2064e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss improved from 0.08525 to 0.07731, saving model to outputs/step3_3_optuna\\trial_17\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9572 - loss: 0.1730 - val_accuracy: 0.9891 - val_loss: 0.0773 - learning_rate: 5.2064e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 0.07731 to 0.07264, saving model to outputs/step3_3_optuna\\trial_17\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9575 - loss: 0.1825 - val_accuracy: 0.9913 - val_loss: 0.0726 - learning_rate: 5.2064e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss improved from 0.07264 to 0.07225, saving model to outputs/step3_3_optuna\\trial_17\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9600 - loss: 0.1684 - val_accuracy: 0.9903 - val_loss: 0.0722 - learning_rate: 5.2064e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.07225\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9596 - loss: 0.1760 - val_accuracy: 0.9821 - val_loss: 0.0890 - learning_rate: 5.2064e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss improved from 0.07225 to 0.06443, saving model to outputs/step3_3_optuna\\trial_17\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 34ms/step - accuracy: 0.9663 - loss: 0.1501 - val_accuracy: 0.9913 - val_loss: 0.0644 - learning_rate: 5.2064e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.06443\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9643 - loss: 0.1572 - val_accuracy: 0.9894 - val_loss: 0.0730 - learning_rate: 5.2064e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.06443\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9642 - loss: 0.1517 - val_accuracy: 0.9903 - val_loss: 0.0662 - learning_rate: 5.2064e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.06443\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9677 - loss: 0.1542 - val_accuracy: 0.9876 - val_loss: 0.0746 - learning_rate: 5.2064e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss improved from 0.06443 to 0.05797, saving model to outputs/step3_3_optuna\\trial_17\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9733 - loss: 0.1261 - val_accuracy: 0.9937 - val_loss: 0.0580 - learning_rate: 5.2064e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.05797\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9714 - loss: 0.1328 - val_accuracy: 0.9911 - val_loss: 0.0641 - learning_rate: 5.2064e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.05797\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9714 - loss: 0.1377 - val_accuracy: 0.9913 - val_loss: 0.0616 - learning_rate: 5.2064e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.05797\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9720 - loss: 0.1285 - val_accuracy: 0.9930 - val_loss: 0.0597 - learning_rate: 5.2064e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.05797\n",
      "156/156 - 6s - 37ms/step - accuracy: 0.9715 - loss: 0.1303 - val_accuracy: 0.9881 - val_loss: 0.0765 - learning_rate: 5.2064e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss improved from 0.05797 to 0.05183, saving model to outputs/step3_3_optuna\\trial_17\\models\\fold_7\\best_model.keras\n",
      "156/156 - 3s - 21ms/step - accuracy: 0.9724 - loss: 0.1302 - val_accuracy: 0.9949 - val_loss: 0.0518 - learning_rate: 5.2064e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.05183\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9781 - loss: 0.1135 - val_accuracy: 0.9939 - val_loss: 0.0562 - learning_rate: 5.2064e-04\n",
      "Restoring model weights from the end of the best epoch: 29.\n",
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.7097 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 19:02:31,206] Trial 17 finished with value: 0.7097061527275369 and parameters: {'WINDOW_SIZE': 45, 'STRIDE': 3, 'gru1_units': 96, 'gru2_units': 0, 'dense_units': 32, 'dropout_rate': 0.3120723281148321, 'l2': 0.00011998609361862717, 'cf_filters': 80, 'cf_kernel': 3, 'cf_pool': 3, 'cf_drop': 0.1, 'cf_l2': 7.830972062877116e-05, 'learning_rate': 0.0005206367517536681, 'batch_size': 64}. Best is trial 17 with value: 0.7097061527275369.\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n",
      "학습 데이터가 11356개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 1.17297, saving model to outputs/step3_3_optuna\\trial_18\\models\\fold_7\\best_model.keras\n",
      "178/178 - 9s - 51ms/step - accuracy: 0.3890 - loss: 1.8235 - val_accuracy: 0.6438 - val_loss: 1.1730 - learning_rate: 5.0439e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 1.17297 to 0.62600, saving model to outputs/step3_3_optuna\\trial_18\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 31ms/step - accuracy: 0.6151 - loss: 1.2002 - val_accuracy: 0.8295 - val_loss: 0.6260 - learning_rate: 5.0439e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 0.62600 to 0.41037, saving model to outputs/step3_3_optuna\\trial_18\\models\\fold_7\\best_model.keras\n",
      "178/178 - 10s - 57ms/step - accuracy: 0.7479 - loss: 0.8090 - val_accuracy: 0.8886 - val_loss: 0.4104 - learning_rate: 5.0439e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 0.41037 to 0.36800, saving model to outputs/step3_3_optuna\\trial_18\\models\\fold_7\\best_model.keras\n",
      "178/178 - 11s - 59ms/step - accuracy: 0.8207 - loss: 0.5857 - val_accuracy: 0.8915 - val_loss: 0.3680 - learning_rate: 5.0439e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 0.36800 to 0.26297, saving model to outputs/step3_3_optuna\\trial_18\\models\\fold_7\\best_model.keras\n",
      "178/178 - 10s - 58ms/step - accuracy: 0.8477 - loss: 0.5046 - val_accuracy: 0.9219 - val_loss: 0.2630 - learning_rate: 5.0439e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 0.26297 to 0.21928, saving model to outputs/step3_3_optuna\\trial_18\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 30ms/step - accuracy: 0.8728 - loss: 0.4297 - val_accuracy: 0.9428 - val_loss: 0.2193 - learning_rate: 5.0439e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.21928 to 0.21187, saving model to outputs/step3_3_optuna\\trial_18\\models\\fold_7\\best_model.keras\n",
      "178/178 - 11s - 59ms/step - accuracy: 0.8853 - loss: 0.3857 - val_accuracy: 0.9436 - val_loss: 0.2119 - learning_rate: 5.0439e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 0.21187 to 0.19351, saving model to outputs/step3_3_optuna\\trial_18\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 30ms/step - accuracy: 0.9061 - loss: 0.3289 - val_accuracy: 0.9472 - val_loss: 0.1935 - learning_rate: 5.0439e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.19351 to 0.15420, saving model to outputs/step3_3_optuna\\trial_18\\models\\fold_7\\best_model.keras\n",
      "178/178 - 11s - 59ms/step - accuracy: 0.9214 - loss: 0.2920 - val_accuracy: 0.9630 - val_loss: 0.1542 - learning_rate: 5.0439e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 0.15420 to 0.14298, saving model to outputs/step3_3_optuna\\trial_18\\models\\fold_7\\best_model.keras\n",
      "178/178 - 10s - 58ms/step - accuracy: 0.9262 - loss: 0.2746 - val_accuracy: 0.9665 - val_loss: 0.1430 - learning_rate: 5.0439e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 0.14298 to 0.12728, saving model to outputs/step3_3_optuna\\trial_18\\models\\fold_7\\best_model.keras\n",
      "178/178 - 10s - 58ms/step - accuracy: 0.9340 - loss: 0.2504 - val_accuracy: 0.9678 - val_loss: 0.1273 - learning_rate: 5.0439e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 0.12728 to 0.11221, saving model to outputs/step3_3_optuna\\trial_18\\models\\fold_7\\best_model.keras\n",
      "178/178 - 10s - 57ms/step - accuracy: 0.9378 - loss: 0.2344 - val_accuracy: 0.9750 - val_loss: 0.1122 - learning_rate: 5.0439e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 0.11221 to 0.09528, saving model to outputs/step3_3_optuna\\trial_18\\models\\fold_7\\best_model.keras\n",
      "178/178 - 10s - 57ms/step - accuracy: 0.9480 - loss: 0.2108 - val_accuracy: 0.9801 - val_loss: 0.0953 - learning_rate: 5.0439e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.09528\n",
      "178/178 - 10s - 58ms/step - accuracy: 0.9456 - loss: 0.2126 - val_accuracy: 0.9746 - val_loss: 0.1124 - learning_rate: 5.0439e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss improved from 0.09528 to 0.08964, saving model to outputs/step3_3_optuna\\trial_18\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 31ms/step - accuracy: 0.9547 - loss: 0.1920 - val_accuracy: 0.9813 - val_loss: 0.0896 - learning_rate: 5.0439e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.08964\n",
      "178/178 - 10s - 56ms/step - accuracy: 0.9597 - loss: 0.1732 - val_accuracy: 0.9767 - val_loss: 0.1035 - learning_rate: 5.0439e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.08964\n",
      "178/178 - 10s - 58ms/step - accuracy: 0.9595 - loss: 0.1722 - val_accuracy: 0.9781 - val_loss: 0.0987 - learning_rate: 5.0439e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss improved from 0.08964 to 0.07586, saving model to outputs/step3_3_optuna\\trial_18\\models\\fold_7\\best_model.keras\n",
      "178/178 - 11s - 60ms/step - accuracy: 0.9632 - loss: 0.1571 - val_accuracy: 0.9871 - val_loss: 0.0759 - learning_rate: 5.0439e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.07586\n",
      "178/178 - 5s - 30ms/step - accuracy: 0.9647 - loss: 0.1598 - val_accuracy: 0.9868 - val_loss: 0.0798 - learning_rate: 5.0439e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss improved from 0.07586 to 0.05926, saving model to outputs/step3_3_optuna\\trial_18\\models\\fold_7\\best_model.keras\n",
      "178/178 - 10s - 57ms/step - accuracy: 0.9676 - loss: 0.1446 - val_accuracy: 0.9913 - val_loss: 0.0593 - learning_rate: 5.0439e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.05926\n",
      "178/178 - 10s - 57ms/step - accuracy: 0.9690 - loss: 0.1410 - val_accuracy: 0.9855 - val_loss: 0.0793 - learning_rate: 5.0439e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.05926\n",
      "178/178 - 10s - 59ms/step - accuracy: 0.9677 - loss: 0.1417 - val_accuracy: 0.9859 - val_loss: 0.0785 - learning_rate: 5.0439e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.05926\n",
      "178/178 - 10s - 57ms/step - accuracy: 0.9688 - loss: 0.1380 - val_accuracy: 0.9873 - val_loss: 0.0742 - learning_rate: 5.0439e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss improved from 0.05926 to 0.05903, saving model to outputs/step3_3_optuna\\trial_18\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 29ms/step - accuracy: 0.9731 - loss: 0.1260 - val_accuracy: 0.9924 - val_loss: 0.0590 - learning_rate: 5.0439e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.05903\n",
      "178/178 - 10s - 59ms/step - accuracy: 0.9748 - loss: 0.1207 - val_accuracy: 0.9847 - val_loss: 0.0833 - learning_rate: 5.0439e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss improved from 0.05903 to 0.05823, saving model to outputs/step3_3_optuna\\trial_18\\models\\fold_7\\best_model.keras\n",
      "178/178 - 10s - 58ms/step - accuracy: 0.9739 - loss: 0.1290 - val_accuracy: 0.9933 - val_loss: 0.0582 - learning_rate: 5.0439e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.05823\n",
      "178/178 - 5s - 30ms/step - accuracy: 0.9773 - loss: 0.1199 - val_accuracy: 0.9888 - val_loss: 0.0692 - learning_rate: 5.0439e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.05823\n",
      "178/178 - 10s - 57ms/step - accuracy: 0.9786 - loss: 0.1101 - val_accuracy: 0.9894 - val_loss: 0.0687 - learning_rate: 5.0439e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.05823\n",
      "178/178 - 10s - 59ms/step - accuracy: 0.9781 - loss: 0.1089 - val_accuracy: 0.9912 - val_loss: 0.0663 - learning_rate: 5.0439e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss improved from 0.05823 to 0.05627, saving model to outputs/step3_3_optuna\\trial_18\\models\\fold_7\\best_model.keras\n",
      "178/178 - 10s - 58ms/step - accuracy: 0.9792 - loss: 0.1054 - val_accuracy: 0.9940 - val_loss: 0.0563 - learning_rate: 5.0439e-04\n",
      "Restoring model weights from the end of the best epoch: 30.\n",
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.6140 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 19:07:14,918] Trial 18 finished with value: 0.6139599014777813 and parameters: {'WINDOW_SIZE': 30, 'STRIDE': 3, 'gru1_units': 256, 'gru2_units': 0, 'dense_units': 32, 'dropout_rate': 0.3102336506831863, 'l2': 0.000124599621924529, 'cf_filters': 80, 'cf_kernel': 7, 'cf_pool': 3, 'cf_drop': 0.0, 'cf_l2': 7.041587923958417e-05, 'learning_rate': 0.0005043853866968061, 'batch_size': 64}. Best is trial 17 with value: 0.7097061527275369.\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n",
      "학습 데이터가 9972개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 1.13747, saving model to outputs/step3_3_optuna\\trial_19\\models\\fold_7\\best_model.keras\n",
      "156/156 - 9s - 57ms/step - accuracy: 0.3805 - loss: 1.9393 - val_accuracy: 0.6606 - val_loss: 1.1375 - learning_rate: 5.1913e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 1.13747 to 0.64092, saving model to outputs/step3_3_optuna\\trial_19\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 30ms/step - accuracy: 0.6127 - loss: 1.2119 - val_accuracy: 0.8272 - val_loss: 0.6409 - learning_rate: 5.1913e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 0.64092 to 0.48654, saving model to outputs/step3_3_optuna\\trial_19\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.7268 - loss: 0.8682 - val_accuracy: 0.8619 - val_loss: 0.4865 - learning_rate: 5.1913e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 0.48654 to 0.35851, saving model to outputs/step3_3_optuna\\trial_19\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 31ms/step - accuracy: 0.7859 - loss: 0.6971 - val_accuracy: 0.8931 - val_loss: 0.3585 - learning_rate: 5.1913e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 0.35851 to 0.24935, saving model to outputs/step3_3_optuna\\trial_19\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 30ms/step - accuracy: 0.8348 - loss: 0.5418 - val_accuracy: 0.9398 - val_loss: 0.2493 - learning_rate: 5.1913e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 0.24935 to 0.21881, saving model to outputs/step3_3_optuna\\trial_19\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.8692 - loss: 0.4581 - val_accuracy: 0.9448 - val_loss: 0.2188 - learning_rate: 5.1913e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.21881 to 0.18089, saving model to outputs/step3_3_optuna\\trial_19\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 31ms/step - accuracy: 0.8872 - loss: 0.3992 - val_accuracy: 0.9576 - val_loss: 0.1809 - learning_rate: 5.1913e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 0.18089 to 0.15150, saving model to outputs/step3_3_optuna\\trial_19\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 30ms/step - accuracy: 0.8971 - loss: 0.3574 - val_accuracy: 0.9659 - val_loss: 0.1515 - learning_rate: 5.1913e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.15150 to 0.13537, saving model to outputs/step3_3_optuna\\trial_19\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9111 - loss: 0.3270 - val_accuracy: 0.9715 - val_loss: 0.1354 - learning_rate: 5.1913e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.13537\n",
      "156/156 - 6s - 36ms/step - accuracy: 0.9258 - loss: 0.2856 - val_accuracy: 0.9668 - val_loss: 0.1521 - learning_rate: 5.1913e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 0.13537 to 0.11732, saving model to outputs/step3_3_optuna\\trial_19\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 31ms/step - accuracy: 0.9305 - loss: 0.2716 - val_accuracy: 0.9782 - val_loss: 0.1173 - learning_rate: 5.1913e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 0.11732 to 0.11442, saving model to outputs/step3_3_optuna\\trial_19\\models\\fold_7\\best_model.keras\n",
      "156/156 - 6s - 36ms/step - accuracy: 0.9342 - loss: 0.2541 - val_accuracy: 0.9813 - val_loss: 0.1144 - learning_rate: 5.1913e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 0.11442 to 0.09608, saving model to outputs/step3_3_optuna\\trial_19\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9474 - loss: 0.2247 - val_accuracy: 0.9833 - val_loss: 0.0961 - learning_rate: 5.1913e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.09608\n",
      "156/156 - 5s - 30ms/step - accuracy: 0.9504 - loss: 0.2163 - val_accuracy: 0.9819 - val_loss: 0.1042 - learning_rate: 5.1913e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.09608\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9527 - loss: 0.2017 - val_accuracy: 0.9794 - val_loss: 0.1087 - learning_rate: 5.1913e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.09608\n",
      "156/156 - 5s - 30ms/step - accuracy: 0.9560 - loss: 0.1959 - val_accuracy: 0.9789 - val_loss: 0.1071 - learning_rate: 5.1913e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 0.09608 to 0.08714, saving model to outputs/step3_3_optuna\\trial_19\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 34ms/step - accuracy: 0.9563 - loss: 0.1949 - val_accuracy: 0.9869 - val_loss: 0.0871 - learning_rate: 5.1913e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.08714\n",
      "156/156 - 5s - 30ms/step - accuracy: 0.9600 - loss: 0.1809 - val_accuracy: 0.9833 - val_loss: 0.1017 - learning_rate: 5.1913e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.08714\n",
      "156/156 - 6s - 36ms/step - accuracy: 0.9606 - loss: 0.1804 - val_accuracy: 0.9864 - val_loss: 0.0872 - learning_rate: 5.1913e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss improved from 0.08714 to 0.08235, saving model to outputs/step3_3_optuna\\trial_19\\models\\fold_7\\best_model.keras\n",
      "156/156 - 4s - 28ms/step - accuracy: 0.9630 - loss: 0.1819 - val_accuracy: 0.9877 - val_loss: 0.0824 - learning_rate: 5.1913e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss improved from 0.08235 to 0.08147, saving model to outputs/step3_3_optuna\\trial_19\\models\\fold_7\\best_model.keras\n",
      "156/156 - 4s - 27ms/step - accuracy: 0.9672 - loss: 0.1589 - val_accuracy: 0.9877 - val_loss: 0.0815 - learning_rate: 5.1913e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss improved from 0.08147 to 0.07426, saving model to outputs/step3_3_optuna\\trial_19\\models\\fold_7\\best_model.keras\n",
      "156/156 - 4s - 27ms/step - accuracy: 0.9697 - loss: 0.1515 - val_accuracy: 0.9913 - val_loss: 0.0743 - learning_rate: 5.1913e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.07426\n",
      "156/156 - 6s - 36ms/step - accuracy: 0.9684 - loss: 0.1547 - val_accuracy: 0.9891 - val_loss: 0.0798 - learning_rate: 5.1913e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.07426\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9688 - loss: 0.1560 - val_accuracy: 0.9884 - val_loss: 0.0834 - learning_rate: 5.1913e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.07426\n",
      "156/156 - 5s - 30ms/step - accuracy: 0.9703 - loss: 0.1541 - val_accuracy: 0.9833 - val_loss: 0.0960 - learning_rate: 5.1913e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss improved from 0.07426 to 0.07364, saving model to outputs/step3_3_optuna\\trial_19\\models\\fold_7\\best_model.keras\n",
      "156/156 - 6s - 36ms/step - accuracy: 0.9718 - loss: 0.1482 - val_accuracy: 0.9910 - val_loss: 0.0736 - learning_rate: 5.1913e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss improved from 0.07364 to 0.06968, saving model to outputs/step3_3_optuna\\trial_19\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 30ms/step - accuracy: 0.9742 - loss: 0.1374 - val_accuracy: 0.9922 - val_loss: 0.0697 - learning_rate: 5.1913e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss improved from 0.06968 to 0.06523, saving model to outputs/step3_3_optuna\\trial_19\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9771 - loss: 0.1286 - val_accuracy: 0.9947 - val_loss: 0.0652 - learning_rate: 5.1913e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss improved from 0.06523 to 0.06111, saving model to outputs/step3_3_optuna\\trial_19\\models\\fold_7\\best_model.keras\n",
      "156/156 - 6s - 36ms/step - accuracy: 0.9759 - loss: 0.1319 - val_accuracy: 0.9961 - val_loss: 0.0611 - learning_rate: 5.1913e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.06111\n",
      "156/156 - 5s - 32ms/step - accuracy: 0.9760 - loss: 0.1290 - val_accuracy: 0.9942 - val_loss: 0.0651 - learning_rate: 5.1913e-04\n",
      "Restoring model weights from the end of the best epoch: 29.\n",
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.6343 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 19:09:55,103] Trial 19 finished with value: 0.6342866742274119 and parameters: {'WINDOW_SIZE': 45, 'STRIDE': 3, 'gru1_units': 160, 'gru2_units': 0, 'dense_units': 32, 'dropout_rate': 0.3003399127841966, 'l2': 0.00015938621094136309, 'cf_filters': 80, 'cf_kernel': 5, 'cf_pool': 3, 'cf_drop': 0.2, 'cf_l2': 0.00014730691438657317, 'learning_rate': 0.0005191317434894338, 'batch_size': 64}. Best is trial 17 with value: 0.7097061527275369.\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n",
      "학습 데이터가 9972개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 2.30365, saving model to outputs/step3_3_optuna\\trial_20\\models\\fold_7\\best_model.keras\n",
      "156/156 - 12s - 77ms/step - accuracy: 0.1297 - loss: 2.6286 - val_accuracy: 0.2775 - val_loss: 2.3036 - learning_rate: 1.0258e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 2.30365 to 1.97158, saving model to outputs/step3_3_optuna\\trial_20\\models\\fold_7\\best_model.keras\n",
      "156/156 - 10s - 62ms/step - accuracy: 0.2422 - loss: 2.3250 - val_accuracy: 0.3755 - val_loss: 1.9716 - learning_rate: 1.0258e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 1.97158 to 1.75641, saving model to outputs/step3_3_optuna\\trial_20\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 31ms/step - accuracy: 0.3197 - loss: 2.1039 - val_accuracy: 0.4413 - val_loss: 1.7564 - learning_rate: 1.0258e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 1.75641 to 1.55785, saving model to outputs/step3_3_optuna\\trial_20\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.3721 - loss: 1.9456 - val_accuracy: 0.5410 - val_loss: 1.5578 - learning_rate: 1.0258e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 1.55785 to 1.37378, saving model to outputs/step3_3_optuna\\trial_20\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.4116 - loss: 1.8053 - val_accuracy: 0.5981 - val_loss: 1.3738 - learning_rate: 1.0258e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 1.37378 to 1.22415, saving model to outputs/step3_3_optuna\\trial_20\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.4517 - loss: 1.6837 - val_accuracy: 0.6794 - val_loss: 1.2242 - learning_rate: 1.0258e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 1.22415 to 1.09482, saving model to outputs/step3_3_optuna\\trial_20\\models\\fold_7\\best_model.keras\n",
      "156/156 - 10s - 64ms/step - accuracy: 0.4871 - loss: 1.5595 - val_accuracy: 0.7229 - val_loss: 1.0948 - learning_rate: 1.0258e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 1.09482 to 0.93551, saving model to outputs/step3_3_optuna\\trial_20\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.5290 - loss: 1.4381 - val_accuracy: 0.7627 - val_loss: 0.9355 - learning_rate: 1.0258e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.93551 to 0.82826, saving model to outputs/step3_3_optuna\\trial_20\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 31ms/step - accuracy: 0.5702 - loss: 1.3463 - val_accuracy: 0.7876 - val_loss: 0.8283 - learning_rate: 1.0258e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 0.82826 to 0.73974, saving model to outputs/step3_3_optuna\\trial_20\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.5993 - loss: 1.2491 - val_accuracy: 0.8045 - val_loss: 0.7397 - learning_rate: 1.0258e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 0.73974 to 0.65270, saving model to outputs/step3_3_optuna\\trial_20\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 31ms/step - accuracy: 0.6294 - loss: 1.1518 - val_accuracy: 0.8302 - val_loss: 0.6527 - learning_rate: 1.0258e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 0.65270 to 0.59816, saving model to outputs/step3_3_optuna\\trial_20\\models\\fold_7\\best_model.keras\n",
      "156/156 - 6s - 36ms/step - accuracy: 0.6619 - loss: 1.0738 - val_accuracy: 0.8420 - val_loss: 0.5982 - learning_rate: 1.0258e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 0.59816 to 0.55508, saving model to outputs/step3_3_optuna\\trial_20\\models\\fold_7\\best_model.keras\n",
      "156/156 - 10s - 66ms/step - accuracy: 0.6737 - loss: 1.0350 - val_accuracy: 0.8558 - val_loss: 0.5551 - learning_rate: 1.0258e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 0.55508 to 0.50275, saving model to outputs/step3_3_optuna\\trial_20\\models\\fold_7\\best_model.keras\n",
      "156/156 - 10s - 63ms/step - accuracy: 0.6974 - loss: 0.9666 - val_accuracy: 0.8720 - val_loss: 0.5028 - learning_rate: 1.0258e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss improved from 0.50275 to 0.47994, saving model to outputs/step3_3_optuna\\trial_20\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.7131 - loss: 0.9201 - val_accuracy: 0.8686 - val_loss: 0.4799 - learning_rate: 1.0258e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss improved from 0.47994 to 0.45094, saving model to outputs/step3_3_optuna\\trial_20\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.7277 - loss: 0.8696 - val_accuracy: 0.8800 - val_loss: 0.4509 - learning_rate: 1.0258e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 0.45094 to 0.41281, saving model to outputs/step3_3_optuna\\trial_20\\models\\fold_7\\best_model.keras\n",
      "156/156 - 6s - 35ms/step - accuracy: 0.7394 - loss: 0.8321 - val_accuracy: 0.8887 - val_loss: 0.4128 - learning_rate: 1.0258e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss improved from 0.41281 to 0.39469, saving model to outputs/step3_3_optuna\\trial_20\\models\\fold_7\\best_model.keras\n",
      "156/156 - 10s - 63ms/step - accuracy: 0.7490 - loss: 0.8022 - val_accuracy: 0.8918 - val_loss: 0.3947 - learning_rate: 1.0258e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss improved from 0.39469 to 0.38008, saving model to outputs/step3_3_optuna\\trial_20\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.7580 - loss: 0.7766 - val_accuracy: 0.8959 - val_loss: 0.3801 - learning_rate: 1.0258e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss improved from 0.38008 to 0.36034, saving model to outputs/step3_3_optuna\\trial_20\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 31ms/step - accuracy: 0.7722 - loss: 0.7357 - val_accuracy: 0.9001 - val_loss: 0.3603 - learning_rate: 1.0258e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss improved from 0.36034 to 0.34346, saving model to outputs/step3_3_optuna\\trial_20\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.7738 - loss: 0.7262 - val_accuracy: 0.9059 - val_loss: 0.3435 - learning_rate: 1.0258e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss improved from 0.34346 to 0.32282, saving model to outputs/step3_3_optuna\\trial_20\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.7875 - loss: 0.6849 - val_accuracy: 0.9110 - val_loss: 0.3228 - learning_rate: 1.0258e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss improved from 0.32282 to 0.30943, saving model to outputs/step3_3_optuna\\trial_20\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.7960 - loss: 0.6647 - val_accuracy: 0.9151 - val_loss: 0.3094 - learning_rate: 1.0258e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss improved from 0.30943 to 0.29388, saving model to outputs/step3_3_optuna\\trial_20\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.8003 - loss: 0.6404 - val_accuracy: 0.9184 - val_loss: 0.2939 - learning_rate: 1.0258e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss improved from 0.29388 to 0.27735, saving model to outputs/step3_3_optuna\\trial_20\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.8053 - loss: 0.6220 - val_accuracy: 0.9219 - val_loss: 0.2773 - learning_rate: 1.0258e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss improved from 0.27735 to 0.26800, saving model to outputs/step3_3_optuna\\trial_20\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.8215 - loss: 0.5874 - val_accuracy: 0.9274 - val_loss: 0.2680 - learning_rate: 1.0258e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss improved from 0.26800 to 0.25368, saving model to outputs/step3_3_optuna\\trial_20\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.8239 - loss: 0.5829 - val_accuracy: 0.9305 - val_loss: 0.2537 - learning_rate: 1.0258e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.25368\n",
      "156/156 - 10s - 65ms/step - accuracy: 0.8308 - loss: 0.5666 - val_accuracy: 0.9315 - val_loss: 0.2567 - learning_rate: 1.0258e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss improved from 0.25368 to 0.23634, saving model to outputs/step3_3_optuna\\trial_20\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 31ms/step - accuracy: 0.8359 - loss: 0.5464 - val_accuracy: 0.9390 - val_loss: 0.2363 - learning_rate: 1.0258e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss improved from 0.23634 to 0.22232, saving model to outputs/step3_3_optuna\\trial_20\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.8396 - loss: 0.5320 - val_accuracy: 0.9431 - val_loss: 0.2223 - learning_rate: 1.0258e-04\n",
      "Restoring model weights from the end of the best epoch: 30.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 19:13:12,445] Trial 20 finished with value: 0.5302422535791432 and parameters: {'WINDOW_SIZE': 45, 'STRIDE': 3, 'gru1_units': 96, 'gru2_units': 96, 'dense_units': 32, 'dropout_rate': 0.426842888444458, 'l2': 9.16686758209201e-05, 'cf_filters': 80, 'cf_kernel': 3, 'cf_pool': 3, 'cf_drop': 0.1, 'cf_l2': 8.203411815031724e-05, 'learning_rate': 0.00010258169292126736, 'batch_size': 64}. Best is trial 17 with value: 0.7097061527275369.\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.5302 ---\n",
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n",
      "학습 데이터가 9972개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 1.38524, saving model to outputs/step3_3_optuna\\trial_21\\models\\fold_7\\best_model.keras\n",
      "156/156 - 7s - 47ms/step - accuracy: 0.2811 - loss: 2.1357 - val_accuracy: 0.5834 - val_loss: 1.3852 - learning_rate: 5.0349e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 1.38524 to 0.85423, saving model to outputs/step3_3_optuna\\trial_21\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 29ms/step - accuracy: 0.5136 - loss: 1.4652 - val_accuracy: 0.7759 - val_loss: 0.8542 - learning_rate: 5.0349e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 0.85423 to 0.61094, saving model to outputs/step3_3_optuna\\trial_21\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.6363 - loss: 1.1251 - val_accuracy: 0.8343 - val_loss: 0.6109 - learning_rate: 5.0349e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 0.61094 to 0.47359, saving model to outputs/step3_3_optuna\\trial_21\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.6953 - loss: 0.9281 - val_accuracy: 0.8645 - val_loss: 0.4736 - learning_rate: 5.0349e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 0.47359 to 0.36400, saving model to outputs/step3_3_optuna\\trial_21\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 34ms/step - accuracy: 0.7552 - loss: 0.7712 - val_accuracy: 0.8988 - val_loss: 0.3640 - learning_rate: 5.0349e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 0.36400 to 0.29228, saving model to outputs/step3_3_optuna\\trial_21\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.7911 - loss: 0.6710 - val_accuracy: 0.9192 - val_loss: 0.2923 - learning_rate: 5.0349e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.29228 to 0.23314, saving model to outputs/step3_3_optuna\\trial_21\\models\\fold_7\\best_model.keras\n",
      "156/156 - 3s - 19ms/step - accuracy: 0.8081 - loss: 0.5992 - val_accuracy: 0.9349 - val_loss: 0.2331 - learning_rate: 5.0349e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 0.23314 to 0.20373, saving model to outputs/step3_3_optuna\\trial_21\\models\\fold_7\\best_model.keras\n",
      "156/156 - 3s - 19ms/step - accuracy: 0.8356 - loss: 0.5292 - val_accuracy: 0.9461 - val_loss: 0.2037 - learning_rate: 5.0349e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.20373 to 0.17455, saving model to outputs/step3_3_optuna\\trial_21\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 34ms/step - accuracy: 0.8575 - loss: 0.4816 - val_accuracy: 0.9535 - val_loss: 0.1745 - learning_rate: 5.0349e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.17455\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.8722 - loss: 0.4299 - val_accuracy: 0.9484 - val_loss: 0.1836 - learning_rate: 5.0349e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 0.17455 to 0.14743, saving model to outputs/step3_3_optuna\\trial_21\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.8801 - loss: 0.3997 - val_accuracy: 0.9616 - val_loss: 0.1474 - learning_rate: 5.0349e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 0.14743 to 0.14583, saving model to outputs/step3_3_optuna\\trial_21\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 34ms/step - accuracy: 0.8968 - loss: 0.3688 - val_accuracy: 0.9623 - val_loss: 0.1458 - learning_rate: 5.0349e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 0.14583 to 0.10381, saving model to outputs/step3_3_optuna\\trial_21\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 32ms/step - accuracy: 0.9057 - loss: 0.3434 - val_accuracy: 0.9756 - val_loss: 0.1038 - learning_rate: 5.0349e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.10381\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9065 - loss: 0.3345 - val_accuracy: 0.9741 - val_loss: 0.1078 - learning_rate: 5.0349e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss improved from 0.10381 to 0.10329, saving model to outputs/step3_3_optuna\\trial_21\\models\\fold_7\\best_model.keras\n",
      "156/156 - 3s - 19ms/step - accuracy: 0.9189 - loss: 0.3017 - val_accuracy: 0.9787 - val_loss: 0.1033 - learning_rate: 5.0349e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.10329\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9225 - loss: 0.2874 - val_accuracy: 0.9746 - val_loss: 0.1139 - learning_rate: 5.0349e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 0.10329 to 0.08188, saving model to outputs/step3_3_optuna\\trial_21\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 34ms/step - accuracy: 0.9248 - loss: 0.2733 - val_accuracy: 0.9840 - val_loss: 0.0819 - learning_rate: 5.0349e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.08188\n",
      "156/156 - 3s - 18ms/step - accuracy: 0.9300 - loss: 0.2648 - val_accuracy: 0.9755 - val_loss: 0.1043 - learning_rate: 5.0349e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.08188\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9362 - loss: 0.2421 - val_accuracy: 0.9797 - val_loss: 0.0863 - learning_rate: 5.0349e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.08188\n",
      "156/156 - 3s - 18ms/step - accuracy: 0.9349 - loss: 0.2423 - val_accuracy: 0.9818 - val_loss: 0.0903 - learning_rate: 5.0349e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss improved from 0.08188 to 0.07830, saving model to outputs/step3_3_optuna\\trial_21\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 34ms/step - accuracy: 0.9425 - loss: 0.2281 - val_accuracy: 0.9841 - val_loss: 0.0783 - learning_rate: 5.0349e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss improved from 0.07830 to 0.06974, saving model to outputs/step3_3_optuna\\trial_21\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9428 - loss: 0.2251 - val_accuracy: 0.9865 - val_loss: 0.0697 - learning_rate: 5.0349e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.06974\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9475 - loss: 0.2128 - val_accuracy: 0.9864 - val_loss: 0.0721 - learning_rate: 5.0349e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss improved from 0.06974 to 0.06135, saving model to outputs/step3_3_optuna\\trial_21\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 34ms/step - accuracy: 0.9524 - loss: 0.1921 - val_accuracy: 0.9913 - val_loss: 0.0614 - learning_rate: 5.0349e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.06135\n",
      "156/156 - 3s - 18ms/step - accuracy: 0.9516 - loss: 0.1946 - val_accuracy: 0.9879 - val_loss: 0.0721 - learning_rate: 5.0349e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.06135\n",
      "156/156 - 3s - 18ms/step - accuracy: 0.9481 - loss: 0.2096 - val_accuracy: 0.9899 - val_loss: 0.0615 - learning_rate: 5.0349e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.06135\n",
      "156/156 - 3s - 18ms/step - accuracy: 0.9526 - loss: 0.1945 - val_accuracy: 0.9894 - val_loss: 0.0692 - learning_rate: 5.0349e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss improved from 0.06135 to 0.06018, saving model to outputs/step3_3_optuna\\trial_21\\models\\fold_7\\best_model.keras\n",
      "156/156 - 3s - 19ms/step - accuracy: 0.9575 - loss: 0.1792 - val_accuracy: 0.9905 - val_loss: 0.0602 - learning_rate: 5.0349e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss improved from 0.06018 to 0.05742, saving model to outputs/step3_3_optuna\\trial_21\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9583 - loss: 0.1711 - val_accuracy: 0.9913 - val_loss: 0.0574 - learning_rate: 5.0349e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss improved from 0.05742 to 0.05055, saving model to outputs/step3_3_optuna\\trial_21\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9585 - loss: 0.1695 - val_accuracy: 0.9939 - val_loss: 0.0506 - learning_rate: 5.0349e-04\n",
      "Restoring model weights from the end of the best epoch: 30.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 19:15:38,235] Trial 21 finished with value: 0.6795939525008772 and parameters: {'WINDOW_SIZE': 45, 'STRIDE': 3, 'gru1_units': 96, 'gru2_units': 0, 'dense_units': 32, 'dropout_rate': 0.4705153500103331, 'l2': 1.5642058893595252e-05, 'cf_filters': 80, 'cf_kernel': 3, 'cf_pool': 3, 'cf_drop': 0.1, 'cf_l2': 0.00020166474415944335, 'learning_rate': 0.0005034919392354749, 'batch_size': 64}. Best is trial 17 with value: 0.7097061527275369.\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.6796 ---\n",
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n",
      "학습 데이터가 9972개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 1.41259, saving model to outputs/step3_3_optuna\\trial_22\\models\\fold_7\\best_model.keras\n",
      "156/156 - 7s - 48ms/step - accuracy: 0.2710 - loss: 2.1533 - val_accuracy: 0.5889 - val_loss: 1.4126 - learning_rate: 5.0438e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 1.41259 to 0.88018, saving model to outputs/step3_3_optuna\\trial_22\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 30ms/step - accuracy: 0.4993 - loss: 1.4892 - val_accuracy: 0.7680 - val_loss: 0.8802 - learning_rate: 5.0438e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 0.88018 to 0.63384, saving model to outputs/step3_3_optuna\\trial_22\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 34ms/step - accuracy: 0.6220 - loss: 1.1480 - val_accuracy: 0.8236 - val_loss: 0.6338 - learning_rate: 5.0438e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 0.63384 to 0.47818, saving model to outputs/step3_3_optuna\\trial_22\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 34ms/step - accuracy: 0.6873 - loss: 0.9495 - val_accuracy: 0.8662 - val_loss: 0.4782 - learning_rate: 5.0438e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 0.47818 to 0.37454, saving model to outputs/step3_3_optuna\\trial_22\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 32ms/step - accuracy: 0.7460 - loss: 0.7935 - val_accuracy: 0.8955 - val_loss: 0.3745 - learning_rate: 5.0438e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 0.37454 to 0.30772, saving model to outputs/step3_3_optuna\\trial_22\\models\\fold_7\\best_model.keras\n",
      "156/156 - 3s - 18ms/step - accuracy: 0.7821 - loss: 0.6938 - val_accuracy: 0.9100 - val_loss: 0.3077 - learning_rate: 5.0438e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.30772 to 0.24882, saving model to outputs/step3_3_optuna\\trial_22\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.8062 - loss: 0.6141 - val_accuracy: 0.9320 - val_loss: 0.2488 - learning_rate: 5.0438e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 0.24882 to 0.21572, saving model to outputs/step3_3_optuna\\trial_22\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.8301 - loss: 0.5400 - val_accuracy: 0.9385 - val_loss: 0.2157 - learning_rate: 5.0438e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.21572 to 0.17843, saving model to outputs/step3_3_optuna\\trial_22\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 34ms/step - accuracy: 0.8511 - loss: 0.5001 - val_accuracy: 0.9552 - val_loss: 0.1784 - learning_rate: 5.0438e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 0.17843 to 0.16712, saving model to outputs/step3_3_optuna\\trial_22\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 32ms/step - accuracy: 0.8682 - loss: 0.4408 - val_accuracy: 0.9545 - val_loss: 0.1671 - learning_rate: 5.0438e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.16712\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.8828 - loss: 0.4023 - val_accuracy: 0.9530 - val_loss: 0.1686 - learning_rate: 5.0438e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 0.16712 to 0.14818, saving model to outputs/step3_3_optuna\\trial_22\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.8891 - loss: 0.3857 - val_accuracy: 0.9654 - val_loss: 0.1482 - learning_rate: 5.0438e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 0.14818 to 0.10767, saving model to outputs/step3_3_optuna\\trial_22\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.8980 - loss: 0.3483 - val_accuracy: 0.9755 - val_loss: 0.1077 - learning_rate: 5.0438e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.10767\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9032 - loss: 0.3397 - val_accuracy: 0.9673 - val_loss: 0.1236 - learning_rate: 5.0438e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.10767\n",
      "156/156 - 5s - 32ms/step - accuracy: 0.9148 - loss: 0.3070 - val_accuracy: 0.9770 - val_loss: 0.1120 - learning_rate: 5.0438e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss improved from 0.10767 to 0.10672, saving model to outputs/step3_3_optuna\\trial_22\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9206 - loss: 0.2903 - val_accuracy: 0.9749 - val_loss: 0.1067 - learning_rate: 5.0438e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 0.10672 to 0.08617, saving model to outputs/step3_3_optuna\\trial_22\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 34ms/step - accuracy: 0.9227 - loss: 0.2837 - val_accuracy: 0.9809 - val_loss: 0.0862 - learning_rate: 5.0438e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.08617\n",
      "156/156 - 5s - 32ms/step - accuracy: 0.9254 - loss: 0.2720 - val_accuracy: 0.9814 - val_loss: 0.0899 - learning_rate: 5.0438e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss improved from 0.08617 to 0.08200, saving model to outputs/step3_3_optuna\\trial_22\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9285 - loss: 0.2548 - val_accuracy: 0.9845 - val_loss: 0.0820 - learning_rate: 5.0438e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss improved from 0.08200 to 0.07805, saving model to outputs/step3_3_optuna\\trial_22\\models\\fold_7\\best_model.keras\n",
      "156/156 - 3s - 19ms/step - accuracy: 0.9320 - loss: 0.2495 - val_accuracy: 0.9877 - val_loss: 0.0781 - learning_rate: 5.0438e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.07805\n",
      "156/156 - 3s - 18ms/step - accuracy: 0.9333 - loss: 0.2473 - val_accuracy: 0.9833 - val_loss: 0.0839 - learning_rate: 5.0438e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss improved from 0.07805 to 0.07058, saving model to outputs/step3_3_optuna\\trial_22\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9349 - loss: 0.2326 - val_accuracy: 0.9886 - val_loss: 0.0706 - learning_rate: 5.0438e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.07058\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9373 - loss: 0.2296 - val_accuracy: 0.9879 - val_loss: 0.0729 - learning_rate: 5.0438e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.07058\n",
      "156/156 - 5s - 34ms/step - accuracy: 0.9470 - loss: 0.2005 - val_accuracy: 0.9872 - val_loss: 0.0711 - learning_rate: 5.0438e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.07058\n",
      "156/156 - 5s - 32ms/step - accuracy: 0.9477 - loss: 0.2140 - val_accuracy: 0.9787 - val_loss: 0.0985 - learning_rate: 5.0438e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss improved from 0.07058 to 0.06900, saving model to outputs/step3_3_optuna\\trial_22\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 35ms/step - accuracy: 0.9494 - loss: 0.1969 - val_accuracy: 0.9884 - val_loss: 0.0690 - learning_rate: 5.0438e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss improved from 0.06900 to 0.06300, saving model to outputs/step3_3_optuna\\trial_22\\models\\fold_7\\best_model.keras\n",
      "156/156 - 3s - 20ms/step - accuracy: 0.9483 - loss: 0.2041 - val_accuracy: 0.9901 - val_loss: 0.0630 - learning_rate: 5.0438e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.06300\n",
      "156/156 - 5s - 32ms/step - accuracy: 0.9512 - loss: 0.1910 - val_accuracy: 0.9816 - val_loss: 0.0914 - learning_rate: 5.0438e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.06300\n",
      "156/156 - 5s - 32ms/step - accuracy: 0.9517 - loss: 0.1887 - val_accuracy: 0.9889 - val_loss: 0.0661 - learning_rate: 5.0438e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss improved from 0.06300 to 0.05780, saving model to outputs/step3_3_optuna\\trial_22\\models\\fold_7\\best_model.keras\n",
      "156/156 - 3s - 18ms/step - accuracy: 0.9534 - loss: 0.1807 - val_accuracy: 0.9928 - val_loss: 0.0578 - learning_rate: 5.0438e-04\n",
      "Restoring model weights from the end of the best epoch: 30.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 19:18:13,217] Trial 22 finished with value: 0.7596586896056158 and parameters: {'WINDOW_SIZE': 45, 'STRIDE': 3, 'gru1_units': 96, 'gru2_units': 0, 'dense_units': 32, 'dropout_rate': 0.48251104946192314, 'l2': 1.56850614158258e-05, 'cf_filters': 80, 'cf_kernel': 3, 'cf_pool': 3, 'cf_drop': 0.1, 'cf_l2': 0.00019154698595805827, 'learning_rate': 0.0005043822289191313, 'batch_size': 64}. Best is trial 22 with value: 0.7596586896056158.\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.7597 ---\n",
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n",
      "학습 데이터가 9972개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 1.16213, saving model to outputs/step3_3_optuna\\trial_23\\models\\fold_7\\best_model.keras\n",
      "156/156 - 7s - 48ms/step - accuracy: 0.3248 - loss: 1.9971 - val_accuracy: 0.6494 - val_loss: 1.1621 - learning_rate: 6.9016e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 1.16213 to 0.68527, saving model to outputs/step3_3_optuna\\trial_23\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 29ms/step - accuracy: 0.5758 - loss: 1.2785 - val_accuracy: 0.7996 - val_loss: 0.6853 - learning_rate: 6.9016e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 0.68527 to 0.46932, saving model to outputs/step3_3_optuna\\trial_23\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 32ms/step - accuracy: 0.6916 - loss: 0.9405 - val_accuracy: 0.8708 - val_loss: 0.4693 - learning_rate: 6.9016e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 0.46932 to 0.40483, saving model to outputs/step3_3_optuna\\trial_23\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.7577 - loss: 0.7499 - val_accuracy: 0.8747 - val_loss: 0.4048 - learning_rate: 6.9016e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 0.40483 to 0.26020, saving model to outputs/step3_3_optuna\\trial_23\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.8136 - loss: 0.5943 - val_accuracy: 0.9257 - val_loss: 0.2602 - learning_rate: 6.9016e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 0.26020 to 0.20961, saving model to outputs/step3_3_optuna\\trial_23\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 35ms/step - accuracy: 0.8399 - loss: 0.5198 - val_accuracy: 0.9371 - val_loss: 0.2096 - learning_rate: 6.9016e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.20961 to 0.19317, saving model to outputs/step3_3_optuna\\trial_23\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 32ms/step - accuracy: 0.8688 - loss: 0.4351 - val_accuracy: 0.9420 - val_loss: 0.1932 - learning_rate: 6.9016e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 0.19317 to 0.13612, saving model to outputs/step3_3_optuna\\trial_23\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.8836 - loss: 0.3946 - val_accuracy: 0.9622 - val_loss: 0.1361 - learning_rate: 6.9016e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.13612 to 0.13235, saving model to outputs/step3_3_optuna\\trial_23\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9006 - loss: 0.3405 - val_accuracy: 0.9589 - val_loss: 0.1323 - learning_rate: 6.9016e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 0.13235 to 0.11465, saving model to outputs/step3_3_optuna\\trial_23\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9073 - loss: 0.3190 - val_accuracy: 0.9681 - val_loss: 0.1146 - learning_rate: 6.9016e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.11465\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9159 - loss: 0.2903 - val_accuracy: 0.9610 - val_loss: 0.1261 - learning_rate: 6.9016e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.11465\n",
      "156/156 - 3s - 18ms/step - accuracy: 0.9230 - loss: 0.2775 - val_accuracy: 0.9680 - val_loss: 0.1161 - learning_rate: 6.9016e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 0.11465 to 0.07933, saving model to outputs/step3_3_optuna\\trial_23\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 34ms/step - accuracy: 0.9299 - loss: 0.2419 - val_accuracy: 0.9813 - val_loss: 0.0793 - learning_rate: 6.9016e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 0.07933 to 0.06657, saving model to outputs/step3_3_optuna\\trial_23\\models\\fold_7\\best_model.keras\n",
      "156/156 - 3s - 18ms/step - accuracy: 0.9352 - loss: 0.2365 - val_accuracy: 0.9862 - val_loss: 0.0666 - learning_rate: 6.9016e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.06657\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9447 - loss: 0.2119 - val_accuracy: 0.9847 - val_loss: 0.0683 - learning_rate: 6.9016e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.06657\n",
      "156/156 - 3s - 18ms/step - accuracy: 0.9444 - loss: 0.2062 - val_accuracy: 0.9845 - val_loss: 0.0698 - learning_rate: 6.9016e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 0.06657 to 0.06418, saving model to outputs/step3_3_optuna\\trial_23\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 34ms/step - accuracy: 0.9472 - loss: 0.1988 - val_accuracy: 0.9859 - val_loss: 0.0642 - learning_rate: 6.9016e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss improved from 0.06418 to 0.05270, saving model to outputs/step3_3_optuna\\trial_23\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9533 - loss: 0.1863 - val_accuracy: 0.9920 - val_loss: 0.0527 - learning_rate: 6.9016e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.05270\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9549 - loss: 0.1772 - val_accuracy: 0.9876 - val_loss: 0.0703 - learning_rate: 6.9016e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.05270\n",
      "156/156 - 3s - 19ms/step - accuracy: 0.9565 - loss: 0.1690 - val_accuracy: 0.9893 - val_loss: 0.0550 - learning_rate: 6.9016e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.05270\n",
      "156/156 - 3s - 18ms/step - accuracy: 0.9572 - loss: 0.1773 - val_accuracy: 0.9860 - val_loss: 0.0676 - learning_rate: 6.9016e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.05270\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9616 - loss: 0.1617 - val_accuracy: 0.9841 - val_loss: 0.0717 - learning_rate: 6.9016e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.05270\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9633 - loss: 0.1499 - val_accuracy: 0.9879 - val_loss: 0.0618 - learning_rate: 6.9016e-04\n",
      "Epoch 23: early stopping\n",
      "Restoring model weights from the end of the best epoch: 18.\n",
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.6182 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 19:20:11,527] Trial 23 finished with value: 0.6182480289778847 and parameters: {'WINDOW_SIZE': 45, 'STRIDE': 3, 'gru1_units': 96, 'gru2_units': 0, 'dense_units': 32, 'dropout_rate': 0.42929109767619794, 'l2': 1.0037915319602108e-05, 'cf_filters': 80, 'cf_kernel': 3, 'cf_pool': 3, 'cf_drop': 0.1, 'cf_l2': 0.00011703466268324364, 'learning_rate': 0.0006901596787193833, 'batch_size': 64}. Best is trial 22 with value: 0.7596586896056158.\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n",
      "학습 데이터가 9972개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 1.50148, saving model to outputs/step3_3_optuna\\trial_24\\models\\fold_7\\best_model.keras\n",
      "156/156 - 7s - 47ms/step - accuracy: 0.2699 - loss: 2.1677 - val_accuracy: 0.5178 - val_loss: 1.5015 - learning_rate: 4.8275e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 1.50148 to 0.92929, saving model to outputs/step3_3_optuna\\trial_24\\models\\fold_7\\best_model.keras\n",
      "156/156 - 3s - 18ms/step - accuracy: 0.4714 - loss: 1.5500 - val_accuracy: 0.7610 - val_loss: 0.9293 - learning_rate: 4.8275e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 0.92929 to 0.66392, saving model to outputs/step3_3_optuna\\trial_24\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.6037 - loss: 1.1960 - val_accuracy: 0.8267 - val_loss: 0.6639 - learning_rate: 4.8275e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 0.66392 to 0.49242, saving model to outputs/step3_3_optuna\\trial_24\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.6636 - loss: 1.0007 - val_accuracy: 0.8636 - val_loss: 0.4924 - learning_rate: 4.8275e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 0.49242 to 0.39765, saving model to outputs/step3_3_optuna\\trial_24\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 34ms/step - accuracy: 0.7235 - loss: 0.8372 - val_accuracy: 0.8817 - val_loss: 0.3976 - learning_rate: 4.8275e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 0.39765 to 0.30647, saving model to outputs/step3_3_optuna\\trial_24\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 32ms/step - accuracy: 0.7640 - loss: 0.7249 - val_accuracy: 0.9141 - val_loss: 0.3065 - learning_rate: 4.8275e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.30647 to 0.26452, saving model to outputs/step3_3_optuna\\trial_24\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.7922 - loss: 0.6518 - val_accuracy: 0.9221 - val_loss: 0.2645 - learning_rate: 4.8275e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 0.26452 to 0.21428, saving model to outputs/step3_3_optuna\\trial_24\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.8148 - loss: 0.5770 - val_accuracy: 0.9407 - val_loss: 0.2143 - learning_rate: 4.8275e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.21428 to 0.18931, saving model to outputs/step3_3_optuna\\trial_24\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 34ms/step - accuracy: 0.8380 - loss: 0.5273 - val_accuracy: 0.9455 - val_loss: 0.1893 - learning_rate: 4.8275e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 0.18931 to 0.17251, saving model to outputs/step3_3_optuna\\trial_24\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 32ms/step - accuracy: 0.8537 - loss: 0.4734 - val_accuracy: 0.9495 - val_loss: 0.1725 - learning_rate: 4.8275e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 0.17251 to 0.15944, saving model to outputs/step3_3_optuna\\trial_24\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.8624 - loss: 0.4358 - val_accuracy: 0.9524 - val_loss: 0.1594 - learning_rate: 4.8275e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 0.15944 to 0.13760, saving model to outputs/step3_3_optuna\\trial_24\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.8794 - loss: 0.4027 - val_accuracy: 0.9618 - val_loss: 0.1376 - learning_rate: 4.8275e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 0.13760 to 0.11339, saving model to outputs/step3_3_optuna\\trial_24\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.8924 - loss: 0.3717 - val_accuracy: 0.9705 - val_loss: 0.1134 - learning_rate: 4.8275e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 0.11339 to 0.10643, saving model to outputs/step3_3_optuna\\trial_24\\models\\fold_7\\best_model.keras\n",
      "156/156 - 3s - 18ms/step - accuracy: 0.8946 - loss: 0.3584 - val_accuracy: 0.9722 - val_loss: 0.1064 - learning_rate: 4.8275e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.10643\n",
      "156/156 - 3s - 18ms/step - accuracy: 0.9030 - loss: 0.3271 - val_accuracy: 0.9683 - val_loss: 0.1156 - learning_rate: 4.8275e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss improved from 0.10643 to 0.09562, saving model to outputs/step3_3_optuna\\trial_24\\models\\fold_7\\best_model.keras\n",
      "156/156 - 3s - 19ms/step - accuracy: 0.9112 - loss: 0.3070 - val_accuracy: 0.9749 - val_loss: 0.0956 - learning_rate: 4.8275e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 0.09562 to 0.08107, saving model to outputs/step3_3_optuna\\trial_24\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 32ms/step - accuracy: 0.9128 - loss: 0.3043 - val_accuracy: 0.9807 - val_loss: 0.0811 - learning_rate: 4.8275e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.08107\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9223 - loss: 0.2842 - val_accuracy: 0.9722 - val_loss: 0.0982 - learning_rate: 4.8275e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.08107\n",
      "156/156 - 5s - 34ms/step - accuracy: 0.9217 - loss: 0.2710 - val_accuracy: 0.9792 - val_loss: 0.0829 - learning_rate: 4.8275e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss improved from 0.08107 to 0.06573, saving model to outputs/step3_3_optuna\\trial_24\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9245 - loss: 0.2748 - val_accuracy: 0.9864 - val_loss: 0.0657 - learning_rate: 4.8275e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss improved from 0.06573 to 0.06383, saving model to outputs/step3_3_optuna\\trial_24\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9287 - loss: 0.2535 - val_accuracy: 0.9869 - val_loss: 0.0638 - learning_rate: 4.8275e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.06383\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9341 - loss: 0.2430 - val_accuracy: 0.9867 - val_loss: 0.0679 - learning_rate: 4.8275e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.06383\n",
      "156/156 - 3s - 18ms/step - accuracy: 0.9327 - loss: 0.2497 - val_accuracy: 0.9874 - val_loss: 0.0651 - learning_rate: 4.8275e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss improved from 0.06383 to 0.05142, saving model to outputs/step3_3_optuna\\trial_24\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 34ms/step - accuracy: 0.9380 - loss: 0.2202 - val_accuracy: 0.9903 - val_loss: 0.0514 - learning_rate: 4.8275e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.05142\n",
      "156/156 - 5s - 32ms/step - accuracy: 0.9427 - loss: 0.2189 - val_accuracy: 0.9794 - val_loss: 0.0831 - learning_rate: 4.8275e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.05142\n",
      "156/156 - 5s - 34ms/step - accuracy: 0.9435 - loss: 0.2101 - val_accuracy: 0.9884 - val_loss: 0.0565 - learning_rate: 4.8275e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss improved from 0.05142 to 0.05104, saving model to outputs/step3_3_optuna\\trial_24\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 32ms/step - accuracy: 0.9463 - loss: 0.2044 - val_accuracy: 0.9905 - val_loss: 0.0510 - learning_rate: 4.8275e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.05104\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9500 - loss: 0.1912 - val_accuracy: 0.9756 - val_loss: 0.0934 - learning_rate: 4.8275e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss improved from 0.05104 to 0.04625, saving model to outputs/step3_3_optuna\\trial_24\\models\\fold_7\\best_model.keras\n",
      "156/156 - 3s - 18ms/step - accuracy: 0.9447 - loss: 0.2041 - val_accuracy: 0.9928 - val_loss: 0.0462 - learning_rate: 4.8275e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.04625\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9507 - loss: 0.1803 - val_accuracy: 0.9901 - val_loss: 0.0508 - learning_rate: 4.8275e-04\n",
      "Restoring model weights from the end of the best epoch: 29.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 19:22:43,647] Trial 24 finished with value: 0.7208248885087254 and parameters: {'WINDOW_SIZE': 45, 'STRIDE': 3, 'gru1_units': 96, 'gru2_units': 0, 'dense_units': 32, 'dropout_rate': 0.49696584345137185, 'l2': 3.269051459996175e-05, 'cf_filters': 80, 'cf_kernel': 3, 'cf_pool': 3, 'cf_drop': 0.1, 'cf_l2': 5.31996671454695e-05, 'learning_rate': 0.0004827485657710206, 'batch_size': 64}. Best is trial 22 with value: 0.7596586896056158.\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.7208 ---\n",
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n",
      "학습 데이터가 6924개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 1.57442, saving model to outputs/step3_3_optuna\\trial_25\\models\\fold_7\\best_model.keras\n",
      "109/109 - 6s - 54ms/step - accuracy: 0.2569 - loss: 2.2115 - val_accuracy: 0.5196 - val_loss: 1.5744 - learning_rate: 5.7665e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 1.57442 to 1.10971, saving model to outputs/step3_3_optuna\\trial_25\\models\\fold_7\\best_model.keras\n",
      "109/109 - 2s - 19ms/step - accuracy: 0.4284 - loss: 1.6966 - val_accuracy: 0.6532 - val_loss: 1.1097 - learning_rate: 5.7665e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 1.10971 to 0.86418, saving model to outputs/step3_3_optuna\\trial_25\\models\\fold_7\\best_model.keras\n",
      "109/109 - 3s - 24ms/step - accuracy: 0.5269 - loss: 1.4041 - val_accuracy: 0.7484 - val_loss: 0.8642 - learning_rate: 5.7665e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 0.86418 to 0.69467, saving model to outputs/step3_3_optuna\\trial_25\\models\\fold_7\\best_model.keras\n",
      "109/109 - 3s - 24ms/step - accuracy: 0.5956 - loss: 1.2070 - val_accuracy: 0.8034 - val_loss: 0.6947 - learning_rate: 5.7665e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 0.69467 to 0.57209, saving model to outputs/step3_3_optuna\\trial_25\\models\\fold_7\\best_model.keras\n",
      "109/109 - 3s - 23ms/step - accuracy: 0.6488 - loss: 1.0601 - val_accuracy: 0.8292 - val_loss: 0.5721 - learning_rate: 5.7665e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 0.57209 to 0.50514, saving model to outputs/step3_3_optuna\\trial_25\\models\\fold_7\\best_model.keras\n",
      "109/109 - 3s - 24ms/step - accuracy: 0.6901 - loss: 0.9442 - val_accuracy: 0.8436 - val_loss: 0.5051 - learning_rate: 5.7665e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.50514 to 0.40152, saving model to outputs/step3_3_optuna\\trial_25\\models\\fold_7\\best_model.keras\n",
      "109/109 - 2s - 14ms/step - accuracy: 0.7171 - loss: 0.8612 - val_accuracy: 0.8726 - val_loss: 0.4015 - learning_rate: 5.7665e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.40152\n",
      "109/109 - 3s - 23ms/step - accuracy: 0.7392 - loss: 0.7979 - val_accuracy: 0.8738 - val_loss: 0.4080 - learning_rate: 5.7665e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.40152 to 0.32859, saving model to outputs/step3_3_optuna\\trial_25\\models\\fold_7\\best_model.keras\n",
      "109/109 - 3s - 26ms/step - accuracy: 0.7572 - loss: 0.7500 - val_accuracy: 0.9055 - val_loss: 0.3286 - learning_rate: 5.7665e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 0.32859 to 0.28856, saving model to outputs/step3_3_optuna\\trial_25\\models\\fold_7\\best_model.keras\n",
      "109/109 - 2s - 23ms/step - accuracy: 0.7744 - loss: 0.6836 - val_accuracy: 0.9134 - val_loss: 0.2886 - learning_rate: 5.7665e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 0.28856 to 0.26500, saving model to outputs/step3_3_optuna\\trial_25\\models\\fold_7\\best_model.keras\n",
      "109/109 - 3s - 24ms/step - accuracy: 0.7984 - loss: 0.6232 - val_accuracy: 0.9141 - val_loss: 0.2650 - learning_rate: 5.7665e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 0.26500 to 0.23353, saving model to outputs/step3_3_optuna\\trial_25\\models\\fold_7\\best_model.keras\n",
      "109/109 - 2s - 23ms/step - accuracy: 0.8114 - loss: 0.5969 - val_accuracy: 0.9313 - val_loss: 0.2335 - learning_rate: 5.7665e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.23353\n",
      "109/109 - 1s - 14ms/step - accuracy: 0.8196 - loss: 0.5845 - val_accuracy: 0.9261 - val_loss: 0.2465 - learning_rate: 5.7665e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 0.23353 to 0.20471, saving model to outputs/step3_3_optuna\\trial_25\\models\\fold_7\\best_model.keras\n",
      "109/109 - 3s - 25ms/step - accuracy: 0.8250 - loss: 0.5518 - val_accuracy: 0.9399 - val_loss: 0.2047 - learning_rate: 5.7665e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss improved from 0.20471 to 0.18907, saving model to outputs/step3_3_optuna\\trial_25\\models\\fold_7\\best_model.keras\n",
      "109/109 - 3s - 24ms/step - accuracy: 0.8488 - loss: 0.4989 - val_accuracy: 0.9435 - val_loss: 0.1891 - learning_rate: 5.7665e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss improved from 0.18907 to 0.18104, saving model to outputs/step3_3_optuna\\trial_25\\models\\fold_7\\best_model.keras\n",
      "109/109 - 3s - 24ms/step - accuracy: 0.8543 - loss: 0.4834 - val_accuracy: 0.9455 - val_loss: 0.1810 - learning_rate: 5.7665e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 0.18104 to 0.16956, saving model to outputs/step3_3_optuna\\trial_25\\models\\fold_7\\best_model.keras\n",
      "109/109 - 3s - 24ms/step - accuracy: 0.8663 - loss: 0.4392 - val_accuracy: 0.9492 - val_loss: 0.1696 - learning_rate: 5.7665e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss improved from 0.16956 to 0.16077, saving model to outputs/step3_3_optuna\\trial_25\\models\\fold_7\\best_model.keras\n",
      "109/109 - 3s - 24ms/step - accuracy: 0.8641 - loss: 0.4458 - val_accuracy: 0.9521 - val_loss: 0.1608 - learning_rate: 5.7665e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss improved from 0.16077 to 0.13762, saving model to outputs/step3_3_optuna\\trial_25\\models\\fold_7\\best_model.keras\n",
      "109/109 - 3s - 24ms/step - accuracy: 0.8767 - loss: 0.4011 - val_accuracy: 0.9588 - val_loss: 0.1376 - learning_rate: 5.7665e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss improved from 0.13762 to 0.13175, saving model to outputs/step3_3_optuna\\trial_25\\models\\fold_7\\best_model.keras\n",
      "109/109 - 3s - 24ms/step - accuracy: 0.8804 - loss: 0.3968 - val_accuracy: 0.9627 - val_loss: 0.1317 - learning_rate: 5.7665e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.13175\n",
      "109/109 - 3s - 24ms/step - accuracy: 0.8860 - loss: 0.3841 - val_accuracy: 0.9583 - val_loss: 0.1378 - learning_rate: 5.7665e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.13175\n",
      "109/109 - 3s - 24ms/step - accuracy: 0.8803 - loss: 0.3852 - val_accuracy: 0.9578 - val_loss: 0.1433 - learning_rate: 5.7665e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss improved from 0.13175 to 0.10714, saving model to outputs/step3_3_optuna\\trial_25\\models\\fold_7\\best_model.keras\n",
      "109/109 - 3s - 24ms/step - accuracy: 0.8911 - loss: 0.3615 - val_accuracy: 0.9732 - val_loss: 0.1071 - learning_rate: 5.7665e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.10714\n",
      "109/109 - 3s - 24ms/step - accuracy: 0.8928 - loss: 0.3572 - val_accuracy: 0.9681 - val_loss: 0.1161 - learning_rate: 5.7665e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.10714\n",
      "109/109 - 3s - 24ms/step - accuracy: 0.9009 - loss: 0.3377 - val_accuracy: 0.9612 - val_loss: 0.1243 - learning_rate: 5.7665e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.10714\n",
      "109/109 - 3s - 24ms/step - accuracy: 0.9034 - loss: 0.3372 - val_accuracy: 0.9713 - val_loss: 0.1089 - learning_rate: 5.7665e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss improved from 0.10714 to 0.10466, saving model to outputs/step3_3_optuna\\trial_25\\models\\fold_7\\best_model.keras\n",
      "109/109 - 3s - 24ms/step - accuracy: 0.9077 - loss: 0.3224 - val_accuracy: 0.9696 - val_loss: 0.1047 - learning_rate: 5.7665e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss improved from 0.10466 to 0.09185, saving model to outputs/step3_3_optuna\\trial_25\\models\\fold_7\\best_model.keras\n",
      "109/109 - 3s - 24ms/step - accuracy: 0.9154 - loss: 0.2988 - val_accuracy: 0.9769 - val_loss: 0.0919 - learning_rate: 5.7665e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss improved from 0.09185 to 0.09008, saving model to outputs/step3_3_optuna\\trial_25\\models\\fold_7\\best_model.keras\n",
      "109/109 - 3s - 24ms/step - accuracy: 0.9120 - loss: 0.3080 - val_accuracy: 0.9767 - val_loss: 0.0901 - learning_rate: 5.7665e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.09008\n",
      "109/109 - 1s - 14ms/step - accuracy: 0.9132 - loss: 0.2975 - val_accuracy: 0.9745 - val_loss: 0.0972 - learning_rate: 5.7665e-04\n",
      "Restoring model weights from the end of the best epoch: 29.\n",
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.6172 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 19:24:07,172] Trial 25 finished with value: 0.6172319639356718 and parameters: {'WINDOW_SIZE': 30, 'STRIDE': 5, 'gru1_units': 96, 'gru2_units': 0, 'dense_units': 32, 'dropout_rate': 0.4980629683135822, 'l2': 6.469096665316209e-05, 'cf_filters': 80, 'cf_kernel': 3, 'cf_pool': 3, 'cf_drop': 0.1, 'cf_l2': 5.0287907055085035e-05, 'learning_rate': 0.0005766548851413582, 'batch_size': 64}. Best is trial 22 with value: 0.7596586896056158.\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n",
      "학습 데이터가 9972개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 1.48847, saving model to outputs/step3_3_optuna\\trial_26\\models\\fold_7\\best_model.keras\n",
      "156/156 - 8s - 52ms/step - accuracy: 0.2576 - loss: 2.1803 - val_accuracy: 0.5325 - val_loss: 1.4885 - learning_rate: 4.6965e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 1.48847 to 0.96877, saving model to outputs/step3_3_optuna\\trial_26\\models\\fold_7\\best_model.keras\n",
      "156/156 - 4s - 23ms/step - accuracy: 0.4637 - loss: 1.5721 - val_accuracy: 0.7529 - val_loss: 0.9688 - learning_rate: 4.6965e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 0.96877 to 0.67931, saving model to outputs/step3_3_optuna\\trial_26\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.5933 - loss: 1.2212 - val_accuracy: 0.8175 - val_loss: 0.6793 - learning_rate: 4.6965e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 0.67931 to 0.51947, saving model to outputs/step3_3_optuna\\trial_26\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.6566 - loss: 1.0351 - val_accuracy: 0.8584 - val_loss: 0.5195 - learning_rate: 4.6965e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 0.51947 to 0.43306, saving model to outputs/step3_3_optuna\\trial_26\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.7071 - loss: 0.8779 - val_accuracy: 0.8682 - val_loss: 0.4331 - learning_rate: 4.6965e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 0.43306 to 0.30846, saving model to outputs/step3_3_optuna\\trial_26\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.7530 - loss: 0.7549 - val_accuracy: 0.9182 - val_loss: 0.3085 - learning_rate: 4.6965e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.30846 to 0.25944, saving model to outputs/step3_3_optuna\\trial_26\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.7818 - loss: 0.6801 - val_accuracy: 0.9264 - val_loss: 0.2594 - learning_rate: 4.6965e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 0.25944 to 0.22320, saving model to outputs/step3_3_optuna\\trial_26\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.8048 - loss: 0.6017 - val_accuracy: 0.9393 - val_loss: 0.2232 - learning_rate: 4.6965e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.22320 to 0.18688, saving model to outputs/step3_3_optuna\\trial_26\\models\\fold_7\\best_model.keras\n",
      "156/156 - 4s - 23ms/step - accuracy: 0.8259 - loss: 0.5483 - val_accuracy: 0.9489 - val_loss: 0.1869 - learning_rate: 4.6965e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.18688\n",
      "156/156 - 4s - 23ms/step - accuracy: 0.8439 - loss: 0.4887 - val_accuracy: 0.9487 - val_loss: 0.1921 - learning_rate: 4.6965e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 0.18688 to 0.15941, saving model to outputs/step3_3_optuna\\trial_26\\models\\fold_7\\best_model.keras\n",
      "156/156 - 4s - 23ms/step - accuracy: 0.8610 - loss: 0.4486 - val_accuracy: 0.9548 - val_loss: 0.1594 - learning_rate: 4.6965e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 0.15941 to 0.15415, saving model to outputs/step3_3_optuna\\trial_26\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.8740 - loss: 0.4080 - val_accuracy: 0.9560 - val_loss: 0.1541 - learning_rate: 4.6965e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 0.15415 to 0.12969, saving model to outputs/step3_3_optuna\\trial_26\\models\\fold_7\\best_model.keras\n",
      "156/156 - 4s - 23ms/step - accuracy: 0.8804 - loss: 0.3868 - val_accuracy: 0.9639 - val_loss: 0.1297 - learning_rate: 4.6965e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 0.12969 to 0.10557, saving model to outputs/step3_3_optuna\\trial_26\\models\\fold_7\\best_model.keras\n",
      "156/156 - 4s - 23ms/step - accuracy: 0.8870 - loss: 0.3671 - val_accuracy: 0.9710 - val_loss: 0.1056 - learning_rate: 4.6965e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss improved from 0.10557 to 0.10226, saving model to outputs/step3_3_optuna\\trial_26\\models\\fold_7\\best_model.keras\n",
      "156/156 - 4s - 23ms/step - accuracy: 0.8929 - loss: 0.3504 - val_accuracy: 0.9719 - val_loss: 0.1023 - learning_rate: 4.6965e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.10226\n",
      "156/156 - 4s - 23ms/step - accuracy: 0.9051 - loss: 0.3168 - val_accuracy: 0.9685 - val_loss: 0.1117 - learning_rate: 4.6965e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 0.10226 to 0.08222, saving model to outputs/step3_3_optuna\\trial_26\\models\\fold_7\\best_model.keras\n",
      "156/156 - 6s - 38ms/step - accuracy: 0.9062 - loss: 0.3192 - val_accuracy: 0.9794 - val_loss: 0.0822 - learning_rate: 4.6965e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.08222\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9171 - loss: 0.2853 - val_accuracy: 0.9755 - val_loss: 0.0872 - learning_rate: 4.6965e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.08222\n",
      "156/156 - 5s - 29ms/step - accuracy: 0.9131 - loss: 0.2936 - val_accuracy: 0.9772 - val_loss: 0.0926 - learning_rate: 4.6965e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.08222\n",
      "156/156 - 6s - 37ms/step - accuracy: 0.9192 - loss: 0.2840 - val_accuracy: 0.9777 - val_loss: 0.0849 - learning_rate: 4.6965e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss improved from 0.08222 to 0.06863, saving model to outputs/step3_3_optuna\\trial_26\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 29ms/step - accuracy: 0.9175 - loss: 0.2652 - val_accuracy: 0.9838 - val_loss: 0.0686 - learning_rate: 4.6965e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss improved from 0.06863 to 0.05917, saving model to outputs/step3_3_optuna\\trial_26\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9234 - loss: 0.2641 - val_accuracy: 0.9881 - val_loss: 0.0592 - learning_rate: 4.6965e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.05917\n",
      "156/156 - 4s - 23ms/step - accuracy: 0.9315 - loss: 0.2419 - val_accuracy: 0.9789 - val_loss: 0.0824 - learning_rate: 4.6965e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.05917\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9401 - loss: 0.2095 - val_accuracy: 0.9836 - val_loss: 0.0807 - learning_rate: 4.6965e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.05917\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9355 - loss: 0.2286 - val_accuracy: 0.9838 - val_loss: 0.0670 - learning_rate: 4.6965e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.05917\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9343 - loss: 0.2284 - val_accuracy: 0.9826 - val_loss: 0.0676 - learning_rate: 4.6965e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss improved from 0.05917 to 0.05452, saving model to outputs/step3_3_optuna\\trial_26\\models\\fold_7\\best_model.keras\n",
      "156/156 - 4s - 23ms/step - accuracy: 0.9410 - loss: 0.2123 - val_accuracy: 0.9888 - val_loss: 0.0545 - learning_rate: 4.6965e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.05452\n",
      "156/156 - 6s - 37ms/step - accuracy: 0.9461 - loss: 0.1998 - val_accuracy: 0.9845 - val_loss: 0.0638 - learning_rate: 4.6965e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.05452\n",
      "156/156 - 4s - 29ms/step - accuracy: 0.9465 - loss: 0.1829 - val_accuracy: 0.9870 - val_loss: 0.0666 - learning_rate: 4.6965e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss improved from 0.05452 to 0.04637, saving model to outputs/step3_3_optuna\\trial_26\\models\\fold_7\\best_model.keras\n",
      "156/156 - 4s - 23ms/step - accuracy: 0.9438 - loss: 0.1997 - val_accuracy: 0.9903 - val_loss: 0.0464 - learning_rate: 4.6965e-04\n",
      "Restoring model weights from the end of the best epoch: 30.\n",
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.6094 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 19:26:35,328] Trial 26 finished with value: 0.6093703330235563 and parameters: {'WINDOW_SIZE': 45, 'STRIDE': 3, 'gru1_units': 96, 'gru2_units': 0, 'dense_units': 32, 'dropout_rate': 0.5136385422079895, 'l2': 3.1221483636146964e-05, 'cf_filters': 80, 'cf_kernel': 3, 'cf_pool': 2, 'cf_drop': 0.1, 'cf_l2': 2.5529539572536972e-05, 'learning_rate': 0.0004696471440437873, 'batch_size': 64}. Best is trial 22 with value: 0.7596586896056158.\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n",
      "학습 데이터가 9972개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 1.70488, saving model to outputs/step3_3_optuna\\trial_27\\models\\fold_7\\best_model.keras\n",
      "156/156 - 8s - 50ms/step - accuracy: 0.2608 - loss: 2.2313 - val_accuracy: 0.5107 - val_loss: 1.7049 - learning_rate: 3.2293e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 1.70488 to 1.30979, saving model to outputs/step3_3_optuna\\trial_27\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.4051 - loss: 1.7983 - val_accuracy: 0.6175 - val_loss: 1.3098 - learning_rate: 3.2293e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 1.30979 to 0.97987, saving model to outputs/step3_3_optuna\\trial_27\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 30ms/step - accuracy: 0.5050 - loss: 1.5063 - val_accuracy: 0.7164 - val_loss: 0.9799 - learning_rate: 3.2293e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 0.97987 to 0.67961, saving model to outputs/step3_3_optuna\\trial_27\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.5883 - loss: 1.2372 - val_accuracy: 0.8227 - val_loss: 0.6796 - learning_rate: 3.2293e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 0.67961 to 0.50390, saving model to outputs/step3_3_optuna\\trial_27\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.6695 - loss: 1.0204 - val_accuracy: 0.8514 - val_loss: 0.5039 - learning_rate: 3.2293e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 0.50390 to 0.39871, saving model to outputs/step3_3_optuna\\trial_27\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.7250 - loss: 0.8569 - val_accuracy: 0.8867 - val_loss: 0.3987 - learning_rate: 3.2293e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.39871 to 0.32965, saving model to outputs/step3_3_optuna\\trial_27\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.7611 - loss: 0.7441 - val_accuracy: 0.9105 - val_loss: 0.3296 - learning_rate: 3.2293e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 0.32965 to 0.29410, saving model to outputs/step3_3_optuna\\trial_27\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.7939 - loss: 0.6627 - val_accuracy: 0.9190 - val_loss: 0.2941 - learning_rate: 3.2293e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.29410 to 0.25179, saving model to outputs/step3_3_optuna\\trial_27\\models\\fold_7\\best_model.keras\n",
      "156/156 - 4s - 25ms/step - accuracy: 0.8124 - loss: 0.6018 - val_accuracy: 0.9284 - val_loss: 0.2518 - learning_rate: 3.2293e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 0.25179 to 0.21353, saving model to outputs/step3_3_optuna\\trial_27\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.8334 - loss: 0.5379 - val_accuracy: 0.9475 - val_loss: 0.2135 - learning_rate: 3.2293e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 0.21353 to 0.20437, saving model to outputs/step3_3_optuna\\trial_27\\models\\fold_7\\best_model.keras\n",
      "156/156 - 6s - 37ms/step - accuracy: 0.8505 - loss: 0.4872 - val_accuracy: 0.9472 - val_loss: 0.2044 - learning_rate: 3.2293e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 0.20437 to 0.16421, saving model to outputs/step3_3_optuna\\trial_27\\models\\fold_7\\best_model.keras\n",
      "156/156 - 4s - 25ms/step - accuracy: 0.8612 - loss: 0.4677 - val_accuracy: 0.9586 - val_loss: 0.1642 - learning_rate: 3.2293e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 0.16421 to 0.15685, saving model to outputs/step3_3_optuna\\trial_27\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.8749 - loss: 0.4204 - val_accuracy: 0.9627 - val_loss: 0.1569 - learning_rate: 3.2293e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 0.15685 to 0.15506, saving model to outputs/step3_3_optuna\\trial_27\\models\\fold_7\\best_model.keras\n",
      "156/156 - 6s - 36ms/step - accuracy: 0.8787 - loss: 0.4170 - val_accuracy: 0.9599 - val_loss: 0.1551 - learning_rate: 3.2293e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss improved from 0.15506 to 0.13310, saving model to outputs/step3_3_optuna\\trial_27\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 30ms/step - accuracy: 0.8884 - loss: 0.3861 - val_accuracy: 0.9680 - val_loss: 0.1331 - learning_rate: 3.2293e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss improved from 0.13310 to 0.12985, saving model to outputs/step3_3_optuna\\trial_27\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.8950 - loss: 0.3583 - val_accuracy: 0.9705 - val_loss: 0.1298 - learning_rate: 3.2293e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 0.12985 to 0.10485, saving model to outputs/step3_3_optuna\\trial_27\\models\\fold_7\\best_model.keras\n",
      "156/156 - 4s - 25ms/step - accuracy: 0.9032 - loss: 0.3433 - val_accuracy: 0.9761 - val_loss: 0.1049 - learning_rate: 3.2293e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss improved from 0.10485 to 0.10401, saving model to outputs/step3_3_optuna\\trial_27\\models\\fold_7\\best_model.keras\n",
      "156/156 - 4s - 25ms/step - accuracy: 0.9086 - loss: 0.3259 - val_accuracy: 0.9802 - val_loss: 0.1040 - learning_rate: 3.2293e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss improved from 0.10401 to 0.09711, saving model to outputs/step3_3_optuna\\trial_27\\models\\fold_7\\best_model.keras\n",
      "156/156 - 4s - 25ms/step - accuracy: 0.9093 - loss: 0.3184 - val_accuracy: 0.9813 - val_loss: 0.0971 - learning_rate: 3.2293e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.09711\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9172 - loss: 0.3049 - val_accuracy: 0.9773 - val_loss: 0.1110 - learning_rate: 3.2293e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss improved from 0.09711 to 0.08998, saving model to outputs/step3_3_optuna\\trial_27\\models\\fold_7\\best_model.keras\n",
      "156/156 - 6s - 37ms/step - accuracy: 0.9221 - loss: 0.2904 - val_accuracy: 0.9845 - val_loss: 0.0900 - learning_rate: 3.2293e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss improved from 0.08998 to 0.08745, saving model to outputs/step3_3_optuna\\trial_27\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9262 - loss: 0.2812 - val_accuracy: 0.9853 - val_loss: 0.0874 - learning_rate: 3.2293e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss improved from 0.08745 to 0.08742, saving model to outputs/step3_3_optuna\\trial_27\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 30ms/step - accuracy: 0.9322 - loss: 0.2694 - val_accuracy: 0.9830 - val_loss: 0.0874 - learning_rate: 3.2293e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.08742\n",
      "156/156 - 6s - 36ms/step - accuracy: 0.9332 - loss: 0.2519 - val_accuracy: 0.9840 - val_loss: 0.0904 - learning_rate: 3.2293e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.08742\n",
      "156/156 - 5s - 30ms/step - accuracy: 0.9374 - loss: 0.2442 - val_accuracy: 0.9758 - val_loss: 0.1028 - learning_rate: 3.2293e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss improved from 0.08742 to 0.08677, saving model to outputs/step3_3_optuna\\trial_27\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9337 - loss: 0.2547 - val_accuracy: 0.9836 - val_loss: 0.0868 - learning_rate: 3.2293e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss improved from 0.08677 to 0.08092, saving model to outputs/step3_3_optuna\\trial_27\\models\\fold_7\\best_model.keras\n",
      "156/156 - 4s - 25ms/step - accuracy: 0.9415 - loss: 0.2349 - val_accuracy: 0.9845 - val_loss: 0.0809 - learning_rate: 3.2293e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.08092\n",
      "156/156 - 4s - 25ms/step - accuracy: 0.9442 - loss: 0.2252 - val_accuracy: 0.9831 - val_loss: 0.0877 - learning_rate: 3.2293e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.08092\n",
      "156/156 - 4s - 25ms/step - accuracy: 0.9433 - loss: 0.2203 - val_accuracy: 0.9819 - val_loss: 0.0920 - learning_rate: 3.2293e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss improved from 0.08092 to 0.06936, saving model to outputs/step3_3_optuna\\trial_27\\models\\fold_7\\best_model.keras\n",
      "156/156 - 4s - 25ms/step - accuracy: 0.9424 - loss: 0.2257 - val_accuracy: 0.9879 - val_loss: 0.0694 - learning_rate: 3.2293e-04\n",
      "Restoring model weights from the end of the best epoch: 30.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 19:29:08,679] Trial 27 finished with value: 0.632202584810301 and parameters: {'WINDOW_SIZE': 45, 'STRIDE': 3, 'gru1_units': 160, 'gru2_units': 0, 'dense_units': 32, 'dropout_rate': 0.4762633756768084, 'l2': 0.0001418887645553152, 'cf_filters': 80, 'cf_kernel': 3, 'cf_pool': 3, 'cf_drop': 0.1, 'cf_l2': 4.928320607793579e-05, 'learning_rate': 0.00032293083586185426, 'batch_size': 64}. Best is trial 22 with value: 0.7596586896056158.\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.6322 ---\n",
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n",
      "학습 데이터가 6924개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 1.24138, saving model to outputs/step3_3_optuna\\trial_28\\models\\fold_7\\best_model.keras\n",
      "109/109 - 10s - 92ms/step - accuracy: 0.3595 - loss: 1.9474 - val_accuracy: 0.5908 - val_loss: 1.2414 - learning_rate: 5.4886e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 1.24138 to 0.83971, saving model to outputs/step3_3_optuna\\trial_28\\models\\fold_7\\best_model.keras\n",
      "109/109 - 4s - 40ms/step - accuracy: 0.5797 - loss: 1.3263 - val_accuracy: 0.7455 - val_loss: 0.8397 - learning_rate: 5.4886e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 0.83971 to 0.66061, saving model to outputs/step3_3_optuna\\trial_28\\models\\fold_7\\best_model.keras\n",
      "109/109 - 5s - 42ms/step - accuracy: 0.6905 - loss: 1.0199 - val_accuracy: 0.7884 - val_loss: 0.6606 - learning_rate: 5.4886e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 0.66061 to 0.53523, saving model to outputs/step3_3_optuna\\trial_28\\models\\fold_7\\best_model.keras\n",
      "109/109 - 5s - 48ms/step - accuracy: 0.7488 - loss: 0.8302 - val_accuracy: 0.8306 - val_loss: 0.5352 - learning_rate: 5.4886e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 0.53523 to 0.52376, saving model to outputs/step3_3_optuna\\trial_28\\models\\fold_7\\best_model.keras\n",
      "109/109 - 4s - 40ms/step - accuracy: 0.7896 - loss: 0.7122 - val_accuracy: 0.8355 - val_loss: 0.5238 - learning_rate: 5.4886e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 0.52376 to 0.38416, saving model to outputs/step3_3_optuna\\trial_28\\models\\fold_7\\best_model.keras\n",
      "109/109 - 5s - 43ms/step - accuracy: 0.8169 - loss: 0.6205 - val_accuracy: 0.8864 - val_loss: 0.3842 - learning_rate: 5.4886e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.38416 to 0.33835, saving model to outputs/step3_3_optuna\\trial_28\\models\\fold_7\\best_model.keras\n",
      "109/109 - 5s - 47ms/step - accuracy: 0.8367 - loss: 0.5496 - val_accuracy: 0.8957 - val_loss: 0.3383 - learning_rate: 5.4886e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 0.33835 to 0.32594, saving model to outputs/step3_3_optuna\\trial_28\\models\\fold_7\\best_model.keras\n",
      "109/109 - 5s - 47ms/step - accuracy: 0.8531 - loss: 0.5074 - val_accuracy: 0.9023 - val_loss: 0.3259 - learning_rate: 5.4886e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.32594 to 0.26122, saving model to outputs/step3_3_optuna\\trial_28\\models\\fold_7\\best_model.keras\n",
      "109/109 - 6s - 52ms/step - accuracy: 0.8670 - loss: 0.4622 - val_accuracy: 0.9269 - val_loss: 0.2612 - learning_rate: 5.4886e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.26122\n",
      "109/109 - 5s - 42ms/step - accuracy: 0.8788 - loss: 0.4222 - val_accuracy: 0.9315 - val_loss: 0.2650 - learning_rate: 5.4886e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 0.26122 to 0.24432, saving model to outputs/step3_3_optuna\\trial_28\\models\\fold_7\\best_model.keras\n",
      "109/109 - 4s - 36ms/step - accuracy: 0.8860 - loss: 0.4132 - val_accuracy: 0.9330 - val_loss: 0.2443 - learning_rate: 5.4886e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 0.24432 to 0.19028, saving model to outputs/step3_3_optuna\\trial_28\\models\\fold_7\\best_model.keras\n",
      "109/109 - 6s - 52ms/step - accuracy: 0.8869 - loss: 0.4028 - val_accuracy: 0.9548 - val_loss: 0.1903 - learning_rate: 5.4886e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.19028\n",
      "109/109 - 5s - 42ms/step - accuracy: 0.9060 - loss: 0.3457 - val_accuracy: 0.9389 - val_loss: 0.2225 - learning_rate: 5.4886e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 0.19028 to 0.18270, saving model to outputs/step3_3_optuna\\trial_28\\models\\fold_7\\best_model.keras\n",
      "109/109 - 5s - 48ms/step - accuracy: 0.9096 - loss: 0.3330 - val_accuracy: 0.9561 - val_loss: 0.1827 - learning_rate: 5.4886e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss improved from 0.18270 to 0.16121, saving model to outputs/step3_3_optuna\\trial_28\\models\\fold_7\\best_model.keras\n",
      "109/109 - 4s - 40ms/step - accuracy: 0.9157 - loss: 0.3104 - val_accuracy: 0.9671 - val_loss: 0.1612 - learning_rate: 5.4886e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.16121\n",
      "109/109 - 5s - 46ms/step - accuracy: 0.9172 - loss: 0.3030 - val_accuracy: 0.9627 - val_loss: 0.1613 - learning_rate: 5.4886e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 0.16121 to 0.14213, saving model to outputs/step3_3_optuna\\trial_28\\models\\fold_7\\best_model.keras\n",
      "109/109 - 5s - 49ms/step - accuracy: 0.9240 - loss: 0.2997 - val_accuracy: 0.9725 - val_loss: 0.1421 - learning_rate: 5.4886e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.14213\n",
      "109/109 - 5s - 46ms/step - accuracy: 0.9399 - loss: 0.2450 - val_accuracy: 0.9647 - val_loss: 0.1577 - learning_rate: 5.4886e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.14213\n",
      "109/109 - 5s - 42ms/step - accuracy: 0.9324 - loss: 0.2541 - val_accuracy: 0.9624 - val_loss: 0.1546 - learning_rate: 5.4886e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss improved from 0.14213 to 0.13231, saving model to outputs/step3_3_optuna\\trial_28\\models\\fold_7\\best_model.keras\n",
      "109/109 - 5s - 48ms/step - accuracy: 0.9352 - loss: 0.2525 - val_accuracy: 0.9757 - val_loss: 0.1323 - learning_rate: 5.4886e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss improved from 0.13231 to 0.12772, saving model to outputs/step3_3_optuna\\trial_28\\models\\fold_7\\best_model.keras\n",
      "109/109 - 4s - 35ms/step - accuracy: 0.9463 - loss: 0.2271 - val_accuracy: 0.9735 - val_loss: 0.1277 - learning_rate: 5.4886e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss improved from 0.12772 to 0.12031, saving model to outputs/step3_3_optuna\\trial_28\\models\\fold_7\\best_model.keras\n",
      "109/109 - 5s - 47ms/step - accuracy: 0.9513 - loss: 0.2095 - val_accuracy: 0.9786 - val_loss: 0.1203 - learning_rate: 5.4886e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss improved from 0.12031 to 0.10899, saving model to outputs/step3_3_optuna\\trial_28\\models\\fold_7\\best_model.keras\n",
      "109/109 - 5s - 47ms/step - accuracy: 0.9486 - loss: 0.2103 - val_accuracy: 0.9816 - val_loss: 0.1090 - learning_rate: 5.4886e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.10899\n",
      "109/109 - 5s - 47ms/step - accuracy: 0.9547 - loss: 0.1993 - val_accuracy: 0.9796 - val_loss: 0.1147 - learning_rate: 5.4886e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss improved from 0.10899 to 0.10043, saving model to outputs/step3_3_optuna\\trial_28\\models\\fold_7\\best_model.keras\n",
      "109/109 - 4s - 35ms/step - accuracy: 0.9555 - loss: 0.1941 - val_accuracy: 0.9838 - val_loss: 0.1004 - learning_rate: 5.4886e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.10043\n",
      "109/109 - 4s - 39ms/step - accuracy: 0.9591 - loss: 0.1836 - val_accuracy: 0.9831 - val_loss: 0.1035 - learning_rate: 5.4886e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.10043\n",
      "109/109 - 4s - 40ms/step - accuracy: 0.9606 - loss: 0.1843 - val_accuracy: 0.9858 - val_loss: 0.1015 - learning_rate: 5.4886e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss improved from 0.10043 to 0.10031, saving model to outputs/step3_3_optuna\\trial_28\\models\\fold_7\\best_model.keras\n",
      "109/109 - 5s - 43ms/step - accuracy: 0.9653 - loss: 0.1665 - val_accuracy: 0.9843 - val_loss: 0.1003 - learning_rate: 5.4886e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss improved from 0.10031 to 0.09819, saving model to outputs/step3_3_optuna\\trial_28\\models\\fold_7\\best_model.keras\n",
      "109/109 - 4s - 40ms/step - accuracy: 0.9624 - loss: 0.1720 - val_accuracy: 0.9865 - val_loss: 0.0982 - learning_rate: 5.4886e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.09819\n",
      "109/109 - 5s - 47ms/step - accuracy: 0.9614 - loss: 0.1750 - val_accuracy: 0.9850 - val_loss: 0.1001 - learning_rate: 5.4886e-04\n",
      "Restoring model weights from the end of the best epoch: 29.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 19:31:43,494] Trial 28 finished with value: 0.6341442123911322 and parameters: {'WINDOW_SIZE': 30, 'STRIDE': 5, 'gru1_units': 256, 'gru2_units': 96, 'dense_units': 96, 'dropout_rate': 0.3935107156172119, 'l2': 9.634947389379414e-05, 'cf_filters': 80, 'cf_kernel': 5, 'cf_pool': 3, 'cf_drop': 0.2, 'cf_l2': 6.552870067379184e-05, 'learning_rate': 0.0005488613241237499, 'batch_size': 64}. Best is trial 22 with value: 0.7596586896056158.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.6341 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optuna 하이퍼파라미터 탐색:  58%|█████▊    | 29/50 [1:26:30<51:08, 146.12s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n",
      "학습 데이터가 9972개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 1.67683, saving model to outputs/step3_3_optuna\\trial_29\\models\\fold_7\\best_model.keras\n",
      "156/156 - 12s - 78ms/step - accuracy: 0.2541 - loss: 2.1998 - val_accuracy: 0.4276 - val_loss: 1.6768 - learning_rate: 1.9159e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 1.67683 to 1.46160, saving model to outputs/step3_3_optuna\\trial_29\\models\\fold_7\\best_model.keras\n",
      "156/156 - 10s - 62ms/step - accuracy: 0.3489 - loss: 1.8806 - val_accuracy: 0.4922 - val_loss: 1.4616 - learning_rate: 1.9159e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 1.46160 to 1.29292, saving model to outputs/step3_3_optuna\\trial_29\\models\\fold_7\\best_model.keras\n",
      "156/156 - 10s - 66ms/step - accuracy: 0.4163 - loss: 1.7022 - val_accuracy: 0.6213 - val_loss: 1.2929 - learning_rate: 1.9159e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 1.29292 to 1.02847, saving model to outputs/step3_3_optuna\\trial_29\\models\\fold_7\\best_model.keras\n",
      "156/156 - 10s - 66ms/step - accuracy: 0.4811 - loss: 1.5477 - val_accuracy: 0.6949 - val_loss: 1.0285 - learning_rate: 1.9159e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 1.02847 to 0.80095, saving model to outputs/step3_3_optuna\\trial_29\\models\\fold_7\\best_model.keras\n",
      "156/156 - 10s - 66ms/step - accuracy: 0.5453 - loss: 1.3643 - val_accuracy: 0.7689 - val_loss: 0.8010 - learning_rate: 1.9159e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 0.80095 to 0.64746, saving model to outputs/step3_3_optuna\\trial_29\\models\\fold_7\\best_model.keras\n",
      "156/156 - 11s - 71ms/step - accuracy: 0.5994 - loss: 1.1961 - val_accuracy: 0.8408 - val_loss: 0.6475 - learning_rate: 1.9159e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.64746 to 0.51915, saving model to outputs/step3_3_optuna\\trial_29\\models\\fold_7\\best_model.keras\n",
      "156/156 - 10s - 66ms/step - accuracy: 0.6457 - loss: 1.0671 - val_accuracy: 0.8693 - val_loss: 0.5191 - learning_rate: 1.9159e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 0.51915 to 0.42149, saving model to outputs/step3_3_optuna\\trial_29\\models\\fold_7\\best_model.keras\n",
      "156/156 - 10s - 61ms/step - accuracy: 0.6974 - loss: 0.9359 - val_accuracy: 0.8924 - val_loss: 0.4215 - learning_rate: 1.9159e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.42149 to 0.34003, saving model to outputs/step3_3_optuna\\trial_29\\models\\fold_7\\best_model.keras\n",
      "156/156 - 8s - 53ms/step - accuracy: 0.7226 - loss: 0.8273 - val_accuracy: 0.9136 - val_loss: 0.3400 - learning_rate: 1.9159e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 0.34003 to 0.29157, saving model to outputs/step3_3_optuna\\trial_29\\models\\fold_7\\best_model.keras\n",
      "156/156 - 10s - 66ms/step - accuracy: 0.7540 - loss: 0.7478 - val_accuracy: 0.9201 - val_loss: 0.2916 - learning_rate: 1.9159e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 0.29157 to 0.28208, saving model to outputs/step3_3_optuna\\trial_29\\models\\fold_7\\best_model.keras\n",
      "156/156 - 10s - 66ms/step - accuracy: 0.7722 - loss: 0.6971 - val_accuracy: 0.9250 - val_loss: 0.2821 - learning_rate: 1.9159e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 0.28208 to 0.22583, saving model to outputs/step3_3_optuna\\trial_29\\models\\fold_7\\best_model.keras\n",
      "156/156 - 8s - 53ms/step - accuracy: 0.7973 - loss: 0.6395 - val_accuracy: 0.9393 - val_loss: 0.2258 - learning_rate: 1.9159e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 0.22583 to 0.20502, saving model to outputs/step3_3_optuna\\trial_29\\models\\fold_7\\best_model.keras\n",
      "156/156 - 10s - 66ms/step - accuracy: 0.8130 - loss: 0.5841 - val_accuracy: 0.9444 - val_loss: 0.2050 - learning_rate: 1.9159e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 0.20502 to 0.18634, saving model to outputs/step3_3_optuna\\trial_29\\models\\fold_7\\best_model.keras\n",
      "156/156 - 10s - 66ms/step - accuracy: 0.8290 - loss: 0.5510 - val_accuracy: 0.9506 - val_loss: 0.1863 - learning_rate: 1.9159e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss improved from 0.18634 to 0.17280, saving model to outputs/step3_3_optuna\\trial_29\\models\\fold_7\\best_model.keras\n",
      "156/156 - 10s - 66ms/step - accuracy: 0.8446 - loss: 0.5032 - val_accuracy: 0.9526 - val_loss: 0.1728 - learning_rate: 1.9159e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss improved from 0.17280 to 0.15457, saving model to outputs/step3_3_optuna\\trial_29\\models\\fold_7\\best_model.keras\n",
      "156/156 - 8s - 53ms/step - accuracy: 0.8533 - loss: 0.4649 - val_accuracy: 0.9584 - val_loss: 0.1546 - learning_rate: 1.9159e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 0.15457 to 0.15057, saving model to outputs/step3_3_optuna\\trial_29\\models\\fold_7\\best_model.keras\n",
      "156/156 - 8s - 53ms/step - accuracy: 0.8630 - loss: 0.4445 - val_accuracy: 0.9589 - val_loss: 0.1506 - learning_rate: 1.9159e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.15057\n",
      "156/156 - 10s - 66ms/step - accuracy: 0.8666 - loss: 0.4246 - val_accuracy: 0.9560 - val_loss: 0.1521 - learning_rate: 1.9159e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss improved from 0.15057 to 0.13478, saving model to outputs/step3_3_optuna\\trial_29\\models\\fold_7\\best_model.keras\n",
      "156/156 - 10s - 67ms/step - accuracy: 0.8807 - loss: 0.3870 - val_accuracy: 0.9640 - val_loss: 0.1348 - learning_rate: 1.9159e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss improved from 0.13478 to 0.11818, saving model to outputs/step3_3_optuna\\trial_29\\models\\fold_7\\best_model.keras\n",
      "156/156 - 10s - 66ms/step - accuracy: 0.8860 - loss: 0.3861 - val_accuracy: 0.9693 - val_loss: 0.1182 - learning_rate: 1.9159e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.11818\n",
      "156/156 - 10s - 65ms/step - accuracy: 0.8893 - loss: 0.3603 - val_accuracy: 0.9674 - val_loss: 0.1212 - learning_rate: 1.9159e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss improved from 0.11818 to 0.10938, saving model to outputs/step3_3_optuna\\trial_29\\models\\fold_7\\best_model.keras\n",
      "156/156 - 8s - 52ms/step - accuracy: 0.8980 - loss: 0.3414 - val_accuracy: 0.9715 - val_loss: 0.1094 - learning_rate: 1.9159e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss improved from 0.10938 to 0.10237, saving model to outputs/step3_3_optuna\\trial_29\\models\\fold_7\\best_model.keras\n",
      "156/156 - 9s - 58ms/step - accuracy: 0.9010 - loss: 0.3266 - val_accuracy: 0.9772 - val_loss: 0.1024 - learning_rate: 1.9159e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss improved from 0.10237 to 0.09476, saving model to outputs/step3_3_optuna\\trial_29\\models\\fold_7\\best_model.keras\n",
      "156/156 - 8s - 53ms/step - accuracy: 0.9007 - loss: 0.3341 - val_accuracy: 0.9748 - val_loss: 0.0948 - learning_rate: 1.9159e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss improved from 0.09476 to 0.08845, saving model to outputs/step3_3_optuna\\trial_29\\models\\fold_7\\best_model.keras\n",
      "156/156 - 10s - 66ms/step - accuracy: 0.9119 - loss: 0.3000 - val_accuracy: 0.9775 - val_loss: 0.0884 - learning_rate: 1.9159e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss improved from 0.08845 to 0.08800, saving model to outputs/step3_3_optuna\\trial_29\\models\\fold_7\\best_model.keras\n",
      "156/156 - 8s - 53ms/step - accuracy: 0.9176 - loss: 0.2783 - val_accuracy: 0.9792 - val_loss: 0.0880 - learning_rate: 1.9159e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss improved from 0.08800 to 0.07589, saving model to outputs/step3_3_optuna\\trial_29\\models\\fold_7\\best_model.keras\n",
      "156/156 - 11s - 72ms/step - accuracy: 0.9134 - loss: 0.2861 - val_accuracy: 0.9843 - val_loss: 0.0759 - learning_rate: 1.9159e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.07589\n",
      "156/156 - 9s - 60ms/step - accuracy: 0.9228 - loss: 0.2769 - val_accuracy: 0.9801 - val_loss: 0.0796 - learning_rate: 1.9159e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss improved from 0.07589 to 0.07415, saving model to outputs/step3_3_optuna\\trial_29\\models\\fold_7\\best_model.keras\n",
      "156/156 - 8s - 52ms/step - accuracy: 0.9219 - loss: 0.2597 - val_accuracy: 0.9821 - val_loss: 0.0742 - learning_rate: 1.9159e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss improved from 0.07415 to 0.06872, saving model to outputs/step3_3_optuna\\trial_29\\models\\fold_7\\best_model.keras\n",
      "156/156 - 8s - 53ms/step - accuracy: 0.9221 - loss: 0.2580 - val_accuracy: 0.9860 - val_loss: 0.0687 - learning_rate: 1.9159e-04\n",
      "Restoring model weights from the end of the best epoch: 30.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 19:36:42,777] Trial 29 finished with value: 0.5474262757017179 and parameters: {'WINDOW_SIZE': 45, 'STRIDE': 3, 'gru1_units': 256, 'gru2_units': 0, 'dense_units': 32, 'dropout_rate': 0.5346923107549384, 'l2': 3.199308789722636e-05, 'cf_filters': 80, 'cf_kernel': 7, 'cf_pool': 2, 'cf_drop': 0.0, 'cf_l2': 0.00010124197117238488, 'learning_rate': 0.00019159261731588754, 'batch_size': 64}. Best is trial 22 with value: 0.7596586896056158.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.5474 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n",
      "학습 데이터가 9972개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 1.74786, saving model to outputs/step3_3_optuna\\trial_30\\models\\fold_7\\best_model.keras\n",
      "156/156 - 11s - 70ms/step - accuracy: 0.2387 - loss: 2.2992 - val_accuracy: 0.4476 - val_loss: 1.7479 - learning_rate: 4.5367e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 1.74786 to 1.26626, saving model to outputs/step3_3_optuna\\trial_30\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 32ms/step - accuracy: 0.4055 - loss: 1.8020 - val_accuracy: 0.6494 - val_loss: 1.2663 - learning_rate: 4.5367e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 1.26626 to 0.89322, saving model to outputs/step3_3_optuna\\trial_30\\models\\fold_7\\best_model.keras\n",
      "156/156 - 4s - 27ms/step - accuracy: 0.5401 - loss: 1.4525 - val_accuracy: 0.7559 - val_loss: 0.8932 - learning_rate: 4.5367e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 0.89322 to 0.63863, saving model to outputs/step3_3_optuna\\trial_30\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 34ms/step - accuracy: 0.6350 - loss: 1.1587 - val_accuracy: 0.8253 - val_loss: 0.6386 - learning_rate: 4.5367e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 0.63863 to 0.50897, saving model to outputs/step3_3_optuna\\trial_30\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.7015 - loss: 0.9595 - val_accuracy: 0.8626 - val_loss: 0.5090 - learning_rate: 4.5367e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 0.50897 to 0.43603, saving model to outputs/step3_3_optuna\\trial_30\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.7443 - loss: 0.8375 - val_accuracy: 0.8858 - val_loss: 0.4360 - learning_rate: 4.5367e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.43603 to 0.36319, saving model to outputs/step3_3_optuna\\trial_30\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.7765 - loss: 0.7379 - val_accuracy: 0.9057 - val_loss: 0.3632 - learning_rate: 4.5367e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 0.36319 to 0.31563, saving model to outputs/step3_3_optuna\\trial_30\\models\\fold_7\\best_model.keras\n",
      "156/156 - 6s - 36ms/step - accuracy: 0.8098 - loss: 0.6679 - val_accuracy: 0.9182 - val_loss: 0.3156 - learning_rate: 4.5367e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.31563 to 0.27723, saving model to outputs/step3_3_optuna\\trial_30\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 31ms/step - accuracy: 0.8301 - loss: 0.5922 - val_accuracy: 0.9310 - val_loss: 0.2772 - learning_rate: 4.5367e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 0.27723 to 0.26205, saving model to outputs/step3_3_optuna\\trial_30\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 34ms/step - accuracy: 0.8527 - loss: 0.5391 - val_accuracy: 0.9345 - val_loss: 0.2620 - learning_rate: 4.5367e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 0.26205 to 0.24673, saving model to outputs/step3_3_optuna\\trial_30\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 29ms/step - accuracy: 0.8500 - loss: 0.5337 - val_accuracy: 0.9403 - val_loss: 0.2467 - learning_rate: 4.5367e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 0.24673 to 0.24656, saving model to outputs/step3_3_optuna\\trial_30\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.8693 - loss: 0.4936 - val_accuracy: 0.9402 - val_loss: 0.2466 - learning_rate: 4.5367e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 0.24656 to 0.23474, saving model to outputs/step3_3_optuna\\trial_30\\models\\fold_7\\best_model.keras\n",
      "156/156 - 6s - 36ms/step - accuracy: 0.8808 - loss: 0.4525 - val_accuracy: 0.9400 - val_loss: 0.2347 - learning_rate: 4.5367e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 0.23474 to 0.18762, saving model to outputs/step3_3_optuna\\trial_30\\models\\fold_7\\best_model.keras\n",
      "156/156 - 4s - 27ms/step - accuracy: 0.8899 - loss: 0.4184 - val_accuracy: 0.9581 - val_loss: 0.1876 - learning_rate: 4.5367e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.18762\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.8944 - loss: 0.4134 - val_accuracy: 0.9560 - val_loss: 0.1914 - learning_rate: 4.5367e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss improved from 0.18762 to 0.16158, saving model to outputs/step3_3_optuna\\trial_30\\models\\fold_7\\best_model.keras\n",
      "156/156 - 6s - 37ms/step - accuracy: 0.9014 - loss: 0.3943 - val_accuracy: 0.9681 - val_loss: 0.1616 - learning_rate: 4.5367e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 0.16158 to 0.15277, saving model to outputs/step3_3_optuna\\trial_30\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 30ms/step - accuracy: 0.9081 - loss: 0.3606 - val_accuracy: 0.9702 - val_loss: 0.1528 - learning_rate: 4.5367e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.15277\n",
      "156/156 - 6s - 36ms/step - accuracy: 0.9043 - loss: 0.3716 - val_accuracy: 0.9712 - val_loss: 0.1547 - learning_rate: 4.5367e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss improved from 0.15277 to 0.14518, saving model to outputs/step3_3_optuna\\trial_30\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 34ms/step - accuracy: 0.9196 - loss: 0.3307 - val_accuracy: 0.9731 - val_loss: 0.1452 - learning_rate: 4.5367e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.14518\n",
      "156/156 - 5s - 32ms/step - accuracy: 0.9263 - loss: 0.3163 - val_accuracy: 0.9743 - val_loss: 0.1472 - learning_rate: 4.5367e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss improved from 0.14518 to 0.14296, saving model to outputs/step3_3_optuna\\trial_30\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 31ms/step - accuracy: 0.9254 - loss: 0.3064 - val_accuracy: 0.9736 - val_loss: 0.1430 - learning_rate: 4.5367e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss improved from 0.14296 to 0.14085, saving model to outputs/step3_3_optuna\\trial_30\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 30ms/step - accuracy: 0.9287 - loss: 0.3057 - val_accuracy: 0.9753 - val_loss: 0.1408 - learning_rate: 4.5367e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss improved from 0.14085 to 0.13699, saving model to outputs/step3_3_optuna\\trial_30\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9310 - loss: 0.2889 - val_accuracy: 0.9744 - val_loss: 0.1370 - learning_rate: 4.5367e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss improved from 0.13699 to 0.13170, saving model to outputs/step3_3_optuna\\trial_30\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9357 - loss: 0.2740 - val_accuracy: 0.9784 - val_loss: 0.1317 - learning_rate: 4.5367e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss improved from 0.13170 to 0.10499, saving model to outputs/step3_3_optuna\\trial_30\\models\\fold_7\\best_model.keras\n",
      "156/156 - 6s - 36ms/step - accuracy: 0.9395 - loss: 0.2631 - val_accuracy: 0.9852 - val_loss: 0.1050 - learning_rate: 4.5367e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.10499\n",
      "156/156 - 5s - 30ms/step - accuracy: 0.9379 - loss: 0.2707 - val_accuracy: 0.9830 - val_loss: 0.1196 - learning_rate: 4.5367e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.10499\n",
      "156/156 - 6s - 37ms/step - accuracy: 0.9425 - loss: 0.2578 - val_accuracy: 0.9826 - val_loss: 0.1195 - learning_rate: 4.5367e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.10499\n",
      "156/156 - 4s - 27ms/step - accuracy: 0.9439 - loss: 0.2567 - val_accuracy: 0.9841 - val_loss: 0.1125 - learning_rate: 4.5367e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.10499\n",
      "156/156 - 6s - 36ms/step - accuracy: 0.9503 - loss: 0.2327 - val_accuracy: 0.9680 - val_loss: 0.1500 - learning_rate: 4.5367e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss improved from 0.10499 to 0.09868, saving model to outputs/step3_3_optuna\\trial_30\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9454 - loss: 0.2386 - val_accuracy: 0.9865 - val_loss: 0.0987 - learning_rate: 4.5367e-04\n",
      "Restoring model weights from the end of the best epoch: 30.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 19:39:28,048] Trial 30 finished with value: 0.5159214128716096 and parameters: {'WINDOW_SIZE': 45, 'STRIDE': 3, 'gru1_units': 96, 'gru2_units': 64, 'dense_units': 32, 'dropout_rate': 0.44443957082394014, 'l2': 0.00018461962207703037, 'cf_filters': 64, 'cf_kernel': 7, 'cf_pool': 3, 'cf_drop': 0.0, 'cf_l2': 7.939837057635317e-05, 'learning_rate': 0.00045367043929696397, 'batch_size': 64}. Best is trial 22 with value: 0.7596586896056158.\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.5159 ---\n",
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n",
      "학습 데이터가 9972개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 1.32832, saving model to outputs/step3_3_optuna\\trial_31\\models\\fold_7\\best_model.keras\n",
      "156/156 - 8s - 50ms/step - accuracy: 0.3029 - loss: 2.0843 - val_accuracy: 0.5979 - val_loss: 1.3283 - learning_rate: 4.5891e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 1.32832 to 0.83167, saving model to outputs/step3_3_optuna\\trial_31\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 29ms/step - accuracy: 0.5353 - loss: 1.3957 - val_accuracy: 0.7663 - val_loss: 0.8317 - learning_rate: 4.5891e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 0.83167 to 0.59713, saving model to outputs/step3_3_optuna\\trial_31\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.6594 - loss: 1.0493 - val_accuracy: 0.8336 - val_loss: 0.5971 - learning_rate: 4.5891e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 0.59713 to 0.46765, saving model to outputs/step3_3_optuna\\trial_31\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.7293 - loss: 0.8524 - val_accuracy: 0.8655 - val_loss: 0.4676 - learning_rate: 4.5891e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 0.46765 to 0.34777, saving model to outputs/step3_3_optuna\\trial_31\\models\\fold_7\\best_model.keras\n",
      "156/156 - 3s - 20ms/step - accuracy: 0.7770 - loss: 0.7076 - val_accuracy: 0.8994 - val_loss: 0.3478 - learning_rate: 4.5891e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 0.34777 to 0.27405, saving model to outputs/step3_3_optuna\\trial_31\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.8084 - loss: 0.6121 - val_accuracy: 0.9284 - val_loss: 0.2740 - learning_rate: 4.5891e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.27405 to 0.22807, saving model to outputs/step3_3_optuna\\trial_31\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.8446 - loss: 0.5187 - val_accuracy: 0.9373 - val_loss: 0.2281 - learning_rate: 4.5891e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 0.22807 to 0.20560, saving model to outputs/step3_3_optuna\\trial_31\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.8560 - loss: 0.4651 - val_accuracy: 0.9434 - val_loss: 0.2056 - learning_rate: 4.5891e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.20560 to 0.17639, saving model to outputs/step3_3_optuna\\trial_31\\models\\fold_7\\best_model.keras\n",
      "156/156 - 3s - 20ms/step - accuracy: 0.8755 - loss: 0.4157 - val_accuracy: 0.9548 - val_loss: 0.1764 - learning_rate: 4.5891e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 0.17639 to 0.14191, saving model to outputs/step3_3_optuna\\trial_31\\models\\fold_7\\best_model.keras\n",
      "156/156 - 3s - 19ms/step - accuracy: 0.8938 - loss: 0.3752 - val_accuracy: 0.9640 - val_loss: 0.1419 - learning_rate: 4.5891e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 0.14191 to 0.12861, saving model to outputs/step3_3_optuna\\trial_31\\models\\fold_7\\best_model.keras\n",
      "156/156 - 3s - 19ms/step - accuracy: 0.9052 - loss: 0.3313 - val_accuracy: 0.9685 - val_loss: 0.1286 - learning_rate: 4.5891e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 0.12861 to 0.12560, saving model to outputs/step3_3_optuna\\trial_31\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9102 - loss: 0.3209 - val_accuracy: 0.9697 - val_loss: 0.1256 - learning_rate: 4.5891e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 0.12560 to 0.09933, saving model to outputs/step3_3_optuna\\trial_31\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9182 - loss: 0.2893 - val_accuracy: 0.9799 - val_loss: 0.0993 - learning_rate: 4.5891e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.09933\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9183 - loss: 0.2849 - val_accuracy: 0.9746 - val_loss: 0.1064 - learning_rate: 4.5891e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.09933\n",
      "156/156 - 3s - 19ms/step - accuracy: 0.9347 - loss: 0.2493 - val_accuracy: 0.9777 - val_loss: 0.1045 - learning_rate: 4.5891e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss improved from 0.09933 to 0.09480, saving model to outputs/step3_3_optuna\\trial_31\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9378 - loss: 0.2316 - val_accuracy: 0.9785 - val_loss: 0.0948 - learning_rate: 4.5891e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 0.09480 to 0.08463, saving model to outputs/step3_3_optuna\\trial_31\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 34ms/step - accuracy: 0.9410 - loss: 0.2242 - val_accuracy: 0.9838 - val_loss: 0.0846 - learning_rate: 4.5891e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss improved from 0.08463 to 0.07804, saving model to outputs/step3_3_optuna\\trial_31\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9444 - loss: 0.2133 - val_accuracy: 0.9862 - val_loss: 0.0780 - learning_rate: 4.5891e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.07804\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9449 - loss: 0.2180 - val_accuracy: 0.9852 - val_loss: 0.0790 - learning_rate: 4.5891e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss improved from 0.07804 to 0.07087, saving model to outputs/step3_3_optuna\\trial_31\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9445 - loss: 0.2127 - val_accuracy: 0.9872 - val_loss: 0.0709 - learning_rate: 4.5891e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss improved from 0.07087 to 0.06940, saving model to outputs/step3_3_optuna\\trial_31\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9534 - loss: 0.1840 - val_accuracy: 0.9879 - val_loss: 0.0694 - learning_rate: 4.5891e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss improved from 0.06940 to 0.06389, saving model to outputs/step3_3_optuna\\trial_31\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9521 - loss: 0.1896 - val_accuracy: 0.9905 - val_loss: 0.0639 - learning_rate: 4.5891e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.06389\n",
      "156/156 - 3s - 19ms/step - accuracy: 0.9525 - loss: 0.1869 - val_accuracy: 0.9819 - val_loss: 0.0887 - learning_rate: 4.5891e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.06389\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9577 - loss: 0.1678 - val_accuracy: 0.9886 - val_loss: 0.0685 - learning_rate: 4.5891e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.06389\n",
      "156/156 - 3s - 19ms/step - accuracy: 0.9636 - loss: 0.1597 - val_accuracy: 0.9860 - val_loss: 0.0754 - learning_rate: 4.5891e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss improved from 0.06389 to 0.05661, saving model to outputs/step3_3_optuna\\trial_31\\models\\fold_7\\best_model.keras\n",
      "156/156 - 3s - 19ms/step - accuracy: 0.9571 - loss: 0.1816 - val_accuracy: 0.9915 - val_loss: 0.0566 - learning_rate: 4.5891e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.05661\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9628 - loss: 0.1482 - val_accuracy: 0.9886 - val_loss: 0.0631 - learning_rate: 4.5891e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.05661\n",
      "156/156 - 3s - 19ms/step - accuracy: 0.9683 - loss: 0.1442 - val_accuracy: 0.9881 - val_loss: 0.0719 - learning_rate: 4.5891e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.05661\n",
      "156/156 - 3s - 19ms/step - accuracy: 0.9631 - loss: 0.1580 - val_accuracy: 0.9894 - val_loss: 0.0702 - learning_rate: 4.5891e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.05661\n",
      "156/156 - 3s - 19ms/step - accuracy: 0.9705 - loss: 0.1334 - val_accuracy: 0.9860 - val_loss: 0.0764 - learning_rate: 4.5891e-04\n",
      "Restoring model weights from the end of the best epoch: 26.\n",
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.6856 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 19:41:52,740] Trial 31 finished with value: 0.6856416206431308 and parameters: {'WINDOW_SIZE': 45, 'STRIDE': 3, 'gru1_units': 96, 'gru2_units': 0, 'dense_units': 32, 'dropout_rate': 0.3990766898966006, 'l2': 1.6539031218124743e-05, 'cf_filters': 80, 'cf_kernel': 3, 'cf_pool': 3, 'cf_drop': 0.1, 'cf_l2': 0.00019452543300741817, 'learning_rate': 0.0004589084865159149, 'batch_size': 64}. Best is trial 22 with value: 0.7596586896056158.\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n",
      "학습 데이터가 9972개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 1.18262, saving model to outputs/step3_3_optuna\\trial_32\\models\\fold_7\\best_model.keras\n",
      "156/156 - 8s - 53ms/step - accuracy: 0.3083 - loss: 2.0389 - val_accuracy: 0.6767 - val_loss: 1.1826 - learning_rate: 6.8457e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 1.18262 to 0.71315, saving model to outputs/step3_3_optuna\\trial_32\\models\\fold_7\\best_model.keras\n",
      "156/156 - 4s - 26ms/step - accuracy: 0.5608 - loss: 1.3076 - val_accuracy: 0.7907 - val_loss: 0.7131 - learning_rate: 6.8457e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 0.71315 to 0.52543, saving model to outputs/step3_3_optuna\\trial_32\\models\\fold_7\\best_model.keras\n",
      "156/156 - 3s - 22ms/step - accuracy: 0.6769 - loss: 0.9891 - val_accuracy: 0.8428 - val_loss: 0.5254 - learning_rate: 6.8457e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 0.52543 to 0.38158, saving model to outputs/step3_3_optuna\\trial_32\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.7419 - loss: 0.7970 - val_accuracy: 0.8846 - val_loss: 0.3816 - learning_rate: 6.8457e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 0.38158 to 0.31409, saving model to outputs/step3_3_optuna\\trial_32\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 34ms/step - accuracy: 0.7981 - loss: 0.6367 - val_accuracy: 0.9034 - val_loss: 0.3141 - learning_rate: 6.8457e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 0.31409 to 0.24038, saving model to outputs/step3_3_optuna\\trial_32\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.8261 - loss: 0.5595 - val_accuracy: 0.9318 - val_loss: 0.2404 - learning_rate: 6.8457e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.24038 to 0.20678, saving model to outputs/step3_3_optuna\\trial_32\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.8492 - loss: 0.4944 - val_accuracy: 0.9414 - val_loss: 0.2068 - learning_rate: 6.8457e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 0.20678 to 0.16136, saving model to outputs/step3_3_optuna\\trial_32\\models\\fold_7\\best_model.keras\n",
      "156/156 - 6s - 37ms/step - accuracy: 0.8672 - loss: 0.4384 - val_accuracy: 0.9594 - val_loss: 0.1614 - learning_rate: 6.8457e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.16136 to 0.12754, saving model to outputs/step3_3_optuna\\trial_32\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.8885 - loss: 0.3876 - val_accuracy: 0.9671 - val_loss: 0.1275 - learning_rate: 6.8457e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 0.12754 to 0.12625, saving model to outputs/step3_3_optuna\\trial_32\\models\\fold_7\\best_model.keras\n",
      "156/156 - 3s - 22ms/step - accuracy: 0.9011 - loss: 0.3467 - val_accuracy: 0.9710 - val_loss: 0.1262 - learning_rate: 6.8457e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.12625\n",
      "156/156 - 3s - 21ms/step - accuracy: 0.9085 - loss: 0.3204 - val_accuracy: 0.9538 - val_loss: 0.1531 - learning_rate: 6.8457e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 0.12625 to 0.11040, saving model to outputs/step3_3_optuna\\trial_32\\models\\fold_7\\best_model.keras\n",
      "156/156 - 6s - 38ms/step - accuracy: 0.9162 - loss: 0.2991 - val_accuracy: 0.9756 - val_loss: 0.1104 - learning_rate: 6.8457e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 0.11040 to 0.10498, saving model to outputs/step3_3_optuna\\trial_32\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 29ms/step - accuracy: 0.9225 - loss: 0.2822 - val_accuracy: 0.9751 - val_loss: 0.1050 - learning_rate: 6.8457e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 0.10498 to 0.09659, saving model to outputs/step3_3_optuna\\trial_32\\models\\fold_7\\best_model.keras\n",
      "156/156 - 6s - 37ms/step - accuracy: 0.9302 - loss: 0.2570 - val_accuracy: 0.9753 - val_loss: 0.0966 - learning_rate: 6.8457e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss improved from 0.09659 to 0.07944, saving model to outputs/step3_3_optuna\\trial_32\\models\\fold_7\\best_model.keras\n",
      "156/156 - 4s - 26ms/step - accuracy: 0.9346 - loss: 0.2429 - val_accuracy: 0.9843 - val_loss: 0.0794 - learning_rate: 6.8457e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.07944\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9348 - loss: 0.2322 - val_accuracy: 0.9816 - val_loss: 0.0887 - learning_rate: 6.8457e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 0.07944 to 0.06586, saving model to outputs/step3_3_optuna\\trial_32\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 34ms/step - accuracy: 0.9424 - loss: 0.2245 - val_accuracy: 0.9857 - val_loss: 0.0659 - learning_rate: 6.8457e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.06586\n",
      "156/156 - 4s - 25ms/step - accuracy: 0.9411 - loss: 0.2174 - val_accuracy: 0.9860 - val_loss: 0.0732 - learning_rate: 6.8457e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.06586\n",
      "156/156 - 5s - 34ms/step - accuracy: 0.9462 - loss: 0.2004 - val_accuracy: 0.9833 - val_loss: 0.0846 - learning_rate: 6.8457e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss improved from 0.06586 to 0.05615, saving model to outputs/step3_3_optuna\\trial_32\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9535 - loss: 0.1874 - val_accuracy: 0.9903 - val_loss: 0.0562 - learning_rate: 6.8457e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.05615\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9534 - loss: 0.1824 - val_accuracy: 0.9864 - val_loss: 0.0751 - learning_rate: 6.8457e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss improved from 0.05615 to 0.05328, saving model to outputs/step3_3_optuna\\trial_32\\models\\fold_7\\best_model.keras\n",
      "156/156 - 4s - 26ms/step - accuracy: 0.9513 - loss: 0.1931 - val_accuracy: 0.9916 - val_loss: 0.0533 - learning_rate: 6.8457e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss improved from 0.05328 to 0.05051, saving model to outputs/step3_3_optuna\\trial_32\\models\\fold_7\\best_model.keras\n",
      "156/156 - 3s - 22ms/step - accuracy: 0.9558 - loss: 0.1833 - val_accuracy: 0.9920 - val_loss: 0.0505 - learning_rate: 6.8457e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.05051\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9606 - loss: 0.1580 - val_accuracy: 0.9876 - val_loss: 0.0679 - learning_rate: 6.8457e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.05051\n",
      "156/156 - 6s - 37ms/step - accuracy: 0.9611 - loss: 0.1587 - val_accuracy: 0.9898 - val_loss: 0.0558 - learning_rate: 6.8457e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.05051\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9633 - loss: 0.1465 - val_accuracy: 0.9908 - val_loss: 0.0540 - learning_rate: 6.8457e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss improved from 0.05051 to 0.04113, saving model to outputs/step3_3_optuna\\trial_32\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 30ms/step - accuracy: 0.9651 - loss: 0.1438 - val_accuracy: 0.9940 - val_loss: 0.0411 - learning_rate: 6.8457e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.04113\n",
      "156/156 - 3s - 21ms/step - accuracy: 0.9622 - loss: 0.1595 - val_accuracy: 0.9836 - val_loss: 0.0769 - learning_rate: 6.8457e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.04113\n",
      "156/156 - 3s - 21ms/step - accuracy: 0.9630 - loss: 0.1468 - val_accuracy: 0.9932 - val_loss: 0.0454 - learning_rate: 6.8457e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.04113\n",
      "156/156 - 6s - 37ms/step - accuracy: 0.9636 - loss: 0.1485 - val_accuracy: 0.9928 - val_loss: 0.0485 - learning_rate: 6.8457e-04\n",
      "Restoring model weights from the end of the best epoch: 27.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 19:44:25,305] Trial 32 finished with value: 0.6071739841829576 and parameters: {'WINDOW_SIZE': 45, 'STRIDE': 3, 'gru1_units': 96, 'gru2_units': 0, 'dense_units': 32, 'dropout_rate': 0.4568201727612381, 'l2': 1.3752264597586139e-05, 'cf_filters': 80, 'cf_kernel': 3, 'cf_pool': 3, 'cf_drop': 0.1, 'cf_l2': 0.00012739538210850953, 'learning_rate': 0.0006845679373349242, 'batch_size': 64}. Best is trial 22 with value: 0.7596586896056158.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.6072 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optuna 하이퍼파라미터 탐색:  66%|██████▌   | 33/50 [1:39:12<47:07, 166.33s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n",
      "학습 데이터가 9972개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 1.03430, saving model to outputs/step3_3_optuna\\trial_33\\models\\fold_7\\best_model.keras\n",
      "156/156 - 8s - 52ms/step - accuracy: 0.3594 - loss: 1.9057 - val_accuracy: 0.6828 - val_loss: 1.0343 - learning_rate: 6.2308e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 1.03430 to 0.60885, saving model to outputs/step3_3_optuna\\trial_33\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 30ms/step - accuracy: 0.6213 - loss: 1.1405 - val_accuracy: 0.8202 - val_loss: 0.6088 - learning_rate: 6.2308e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 0.60885 to 0.46328, saving model to outputs/step3_3_optuna\\trial_33\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 29ms/step - accuracy: 0.7361 - loss: 0.8160 - val_accuracy: 0.8587 - val_loss: 0.4633 - learning_rate: 6.2308e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 0.46328 to 0.35020, saving model to outputs/step3_3_optuna\\trial_33\\models\\fold_7\\best_model.keras\n",
      "156/156 - 6s - 37ms/step - accuracy: 0.7921 - loss: 0.6440 - val_accuracy: 0.8923 - val_loss: 0.3502 - learning_rate: 6.2308e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 0.35020 to 0.21923, saving model to outputs/step3_3_optuna\\trial_33\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 29ms/step - accuracy: 0.8442 - loss: 0.5010 - val_accuracy: 0.9357 - val_loss: 0.2192 - learning_rate: 6.2308e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 0.21923 to 0.19613, saving model to outputs/step3_3_optuna\\trial_33\\models\\fold_7\\best_model.keras\n",
      "156/156 - 6s - 37ms/step - accuracy: 0.8682 - loss: 0.4274 - val_accuracy: 0.9388 - val_loss: 0.1961 - learning_rate: 6.2308e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.19613 to 0.16831, saving model to outputs/step3_3_optuna\\trial_33\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 29ms/step - accuracy: 0.8923 - loss: 0.3544 - val_accuracy: 0.9509 - val_loss: 0.1683 - learning_rate: 6.2308e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 0.16831 to 0.15557, saving model to outputs/step3_3_optuna\\trial_33\\models\\fold_7\\best_model.keras\n",
      "156/156 - 6s - 37ms/step - accuracy: 0.9045 - loss: 0.3103 - val_accuracy: 0.9533 - val_loss: 0.1556 - learning_rate: 6.2308e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.15557 to 0.09916, saving model to outputs/step3_3_optuna\\trial_33\\models\\fold_7\\best_model.keras\n",
      "156/156 - 4s - 25ms/step - accuracy: 0.9213 - loss: 0.2663 - val_accuracy: 0.9755 - val_loss: 0.0992 - learning_rate: 6.2308e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 0.09916 to 0.09842, saving model to outputs/step3_3_optuna\\trial_33\\models\\fold_7\\best_model.keras\n",
      "156/156 - 3s - 21ms/step - accuracy: 0.9305 - loss: 0.2411 - val_accuracy: 0.9765 - val_loss: 0.0984 - learning_rate: 6.2308e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 0.09842 to 0.09171, saving model to outputs/step3_3_optuna\\trial_33\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9370 - loss: 0.2162 - val_accuracy: 0.9773 - val_loss: 0.0917 - learning_rate: 6.2308e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 0.09171 to 0.07131, saving model to outputs/step3_3_optuna\\trial_33\\models\\fold_7\\best_model.keras\n",
      "156/156 - 4s - 25ms/step - accuracy: 0.9391 - loss: 0.2173 - val_accuracy: 0.9847 - val_loss: 0.0713 - learning_rate: 6.2308e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 0.07131 to 0.06072, saving model to outputs/step3_3_optuna\\trial_33\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 29ms/step - accuracy: 0.9446 - loss: 0.1966 - val_accuracy: 0.9884 - val_loss: 0.0607 - learning_rate: 6.2308e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.06072\n",
      "156/156 - 6s - 37ms/step - accuracy: 0.9499 - loss: 0.1828 - val_accuracy: 0.9841 - val_loss: 0.0657 - learning_rate: 6.2308e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss improved from 0.06072 to 0.05443, saving model to outputs/step3_3_optuna\\trial_33\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 29ms/step - accuracy: 0.9566 - loss: 0.1582 - val_accuracy: 0.9908 - val_loss: 0.0544 - learning_rate: 6.2308e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.05443\n",
      "156/156 - 6s - 37ms/step - accuracy: 0.9608 - loss: 0.1460 - val_accuracy: 0.9884 - val_loss: 0.0556 - learning_rate: 6.2308e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 0.05443 to 0.04874, saving model to outputs/step3_3_optuna\\trial_33\\models\\fold_7\\best_model.keras\n",
      "156/156 - 3s - 22ms/step - accuracy: 0.9619 - loss: 0.1469 - val_accuracy: 0.9898 - val_loss: 0.0487 - learning_rate: 6.2308e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.04874\n",
      "156/156 - 4s - 25ms/step - accuracy: 0.9660 - loss: 0.1393 - val_accuracy: 0.9899 - val_loss: 0.0544 - learning_rate: 6.2308e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss improved from 0.04874 to 0.04806, saving model to outputs/step3_3_optuna\\trial_33\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 29ms/step - accuracy: 0.9663 - loss: 0.1325 - val_accuracy: 0.9896 - val_loss: 0.0481 - learning_rate: 6.2308e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss improved from 0.04806 to 0.04065, saving model to outputs/step3_3_optuna\\trial_33\\models\\fold_7\\best_model.keras\n",
      "156/156 - 4s - 26ms/step - accuracy: 0.9684 - loss: 0.1254 - val_accuracy: 0.9911 - val_loss: 0.0407 - learning_rate: 6.2308e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.04065\n",
      "156/156 - 4s - 29ms/step - accuracy: 0.9660 - loss: 0.1271 - val_accuracy: 0.9916 - val_loss: 0.0423 - learning_rate: 6.2308e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.04065\n",
      "156/156 - 4s - 25ms/step - accuracy: 0.9684 - loss: 0.1197 - val_accuracy: 0.9862 - val_loss: 0.0564 - learning_rate: 6.2308e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.04065\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9702 - loss: 0.1214 - val_accuracy: 0.9823 - val_loss: 0.0639 - learning_rate: 6.2308e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss improved from 0.04065 to 0.03790, saving model to outputs/step3_3_optuna\\trial_33\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 30ms/step - accuracy: 0.9718 - loss: 0.1114 - val_accuracy: 0.9930 - val_loss: 0.0379 - learning_rate: 6.2308e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss improved from 0.03790 to 0.03260, saving model to outputs/step3_3_optuna\\trial_33\\models\\fold_7\\best_model.keras\n",
      "156/156 - 4s - 26ms/step - accuracy: 0.9760 - loss: 0.0993 - val_accuracy: 0.9939 - val_loss: 0.0326 - learning_rate: 6.2308e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.03260\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9772 - loss: 0.0949 - val_accuracy: 0.9930 - val_loss: 0.0379 - learning_rate: 6.2308e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.03260\n",
      "156/156 - 5s - 29ms/step - accuracy: 0.9772 - loss: 0.0933 - val_accuracy: 0.9937 - val_loss: 0.0371 - learning_rate: 6.2308e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.03260\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9765 - loss: 0.0950 - val_accuracy: 0.9891 - val_loss: 0.0529 - learning_rate: 6.2308e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.03260\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9786 - loss: 0.0959 - val_accuracy: 0.9927 - val_loss: 0.0366 - learning_rate: 6.2308e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.03260\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9777 - loss: 0.0902 - val_accuracy: 0.9913 - val_loss: 0.0434 - learning_rate: 6.2308e-04\n",
      "Epoch 30: early stopping\n",
      "Restoring model weights from the end of the best epoch: 25.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 19:46:57,361] Trial 33 finished with value: 0.6722468109826013 and parameters: {'WINDOW_SIZE': 45, 'STRIDE': 3, 'gru1_units': 96, 'gru2_units': 0, 'dense_units': 32, 'dropout_rate': 0.3262469730351324, 'l2': 3.673467602351455e-05, 'cf_filters': 80, 'cf_kernel': 3, 'cf_pool': 3, 'cf_drop': 0.1, 'cf_l2': 2.2413343626240327e-05, 'learning_rate': 0.0006230788679665117, 'batch_size': 64}. Best is trial 22 with value: 0.7596586896056158.\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.6722 ---\n",
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n",
      "학습 데이터가 9972개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 1.61393, saving model to outputs/step3_3_optuna\\trial_34\\models\\fold_7\\best_model.keras\n",
      "78/78 - 7s - 96ms/step - accuracy: 0.2668 - loss: 2.2464 - val_accuracy: 0.4738 - val_loss: 1.6139 - learning_rate: 5.3760e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 1.61393 to 1.08628, saving model to outputs/step3_3_optuna\\trial_34\\models\\fold_7\\best_model.keras\n",
      "78/78 - 5s - 67ms/step - accuracy: 0.4510 - loss: 1.6428 - val_accuracy: 0.6693 - val_loss: 1.0863 - learning_rate: 5.3760e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 1.08628 to 0.74908, saving model to outputs/step3_3_optuna\\trial_34\\models\\fold_7\\best_model.keras\n",
      "78/78 - 5s - 58ms/step - accuracy: 0.5881 - loss: 1.2349 - val_accuracy: 0.7827 - val_loss: 0.7491 - learning_rate: 5.3760e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 0.74908 to 0.56887, saving model to outputs/step3_3_optuna\\trial_34\\models\\fold_7\\best_model.keras\n",
      "78/78 - 5s - 66ms/step - accuracy: 0.6928 - loss: 0.9556 - val_accuracy: 0.8294 - val_loss: 0.5689 - learning_rate: 5.3760e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 0.56887 to 0.44245, saving model to outputs/step3_3_optuna\\trial_34\\models\\fold_7\\best_model.keras\n",
      "78/78 - 4s - 47ms/step - accuracy: 0.7455 - loss: 0.7950 - val_accuracy: 0.8657 - val_loss: 0.4424 - learning_rate: 5.3760e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 0.44245 to 0.35712, saving model to outputs/step3_3_optuna\\trial_34\\models\\fold_7\\best_model.keras\n",
      "78/78 - 6s - 74ms/step - accuracy: 0.7888 - loss: 0.6569 - val_accuracy: 0.8923 - val_loss: 0.3571 - learning_rate: 5.3760e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.35712 to 0.30805, saving model to outputs/step3_3_optuna\\trial_34\\models\\fold_7\\best_model.keras\n",
      "78/78 - 4s - 48ms/step - accuracy: 0.8174 - loss: 0.5744 - val_accuracy: 0.9076 - val_loss: 0.3080 - learning_rate: 5.3760e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 0.30805 to 0.26183, saving model to outputs/step3_3_optuna\\trial_34\\models\\fold_7\\best_model.keras\n",
      "78/78 - 5s - 66ms/step - accuracy: 0.8441 - loss: 0.5041 - val_accuracy: 0.9199 - val_loss: 0.2618 - learning_rate: 5.3760e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.26183 to 0.23175, saving model to outputs/step3_3_optuna\\trial_34\\models\\fold_7\\best_model.keras\n",
      "78/78 - 4s - 47ms/step - accuracy: 0.8602 - loss: 0.4506 - val_accuracy: 0.9233 - val_loss: 0.2318 - learning_rate: 5.3760e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 0.23175 to 0.18876, saving model to outputs/step3_3_optuna\\trial_34\\models\\fold_7\\best_model.keras\n",
      "78/78 - 4s - 47ms/step - accuracy: 0.8779 - loss: 0.4081 - val_accuracy: 0.9468 - val_loss: 0.1888 - learning_rate: 5.3760e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.18876\n",
      "78/78 - 6s - 73ms/step - accuracy: 0.8900 - loss: 0.3668 - val_accuracy: 0.9340 - val_loss: 0.2157 - learning_rate: 5.3760e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 0.18876 to 0.15530, saving model to outputs/step3_3_optuna\\trial_34\\models\\fold_7\\best_model.keras\n",
      "78/78 - 4s - 47ms/step - accuracy: 0.9003 - loss: 0.3299 - val_accuracy: 0.9567 - val_loss: 0.1553 - learning_rate: 5.3760e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 0.15530 to 0.13487, saving model to outputs/step3_3_optuna\\trial_34\\models\\fold_7\\best_model.keras\n",
      "78/78 - 5s - 66ms/step - accuracy: 0.9152 - loss: 0.2990 - val_accuracy: 0.9654 - val_loss: 0.1349 - learning_rate: 5.3760e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 0.13487 to 0.12146, saving model to outputs/step3_3_optuna\\trial_34\\models\\fold_7\\best_model.keras\n",
      "78/78 - 5s - 66ms/step - accuracy: 0.9153 - loss: 0.2949 - val_accuracy: 0.9700 - val_loss: 0.1215 - learning_rate: 5.3760e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.12146\n",
      "78/78 - 5s - 65ms/step - accuracy: 0.9288 - loss: 0.2572 - val_accuracy: 0.9628 - val_loss: 0.1271 - learning_rate: 5.3760e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.12146\n",
      "78/78 - 5s - 66ms/step - accuracy: 0.9313 - loss: 0.2427 - val_accuracy: 0.9654 - val_loss: 0.1228 - learning_rate: 5.3760e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 0.12146 to 0.09793, saving model to outputs/step3_3_optuna\\trial_34\\models\\fold_7\\best_model.keras\n",
      "78/78 - 5s - 67ms/step - accuracy: 0.9377 - loss: 0.2262 - val_accuracy: 0.9761 - val_loss: 0.0979 - learning_rate: 5.3760e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss improved from 0.09793 to 0.08217, saving model to outputs/step3_3_optuna\\trial_34\\models\\fold_7\\best_model.keras\n",
      "78/78 - 5s - 66ms/step - accuracy: 0.9449 - loss: 0.2081 - val_accuracy: 0.9804 - val_loss: 0.0822 - learning_rate: 5.3760e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.08217\n",
      "78/78 - 4s - 54ms/step - accuracy: 0.9427 - loss: 0.2089 - val_accuracy: 0.9765 - val_loss: 0.0982 - learning_rate: 5.3760e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.08217\n",
      "78/78 - 5s - 66ms/step - accuracy: 0.9429 - loss: 0.2104 - val_accuracy: 0.9797 - val_loss: 0.0839 - learning_rate: 5.3760e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.08217\n",
      "78/78 - 5s - 59ms/step - accuracy: 0.9539 - loss: 0.1758 - val_accuracy: 0.9731 - val_loss: 0.1034 - learning_rate: 5.3760e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss improved from 0.08217 to 0.07979, saving model to outputs/step3_3_optuna\\trial_34\\models\\fold_7\\best_model.keras\n",
      "78/78 - 5s - 66ms/step - accuracy: 0.9519 - loss: 0.1836 - val_accuracy: 0.9804 - val_loss: 0.0798 - learning_rate: 5.3760e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss improved from 0.07979 to 0.06662, saving model to outputs/step3_3_optuna\\trial_34\\models\\fold_7\\best_model.keras\n",
      "78/78 - 5s - 66ms/step - accuracy: 0.9596 - loss: 0.1620 - val_accuracy: 0.9864 - val_loss: 0.0666 - learning_rate: 5.3760e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.06662\n",
      "78/78 - 4s - 47ms/step - accuracy: 0.9628 - loss: 0.1538 - val_accuracy: 0.9843 - val_loss: 0.0708 - learning_rate: 5.3760e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.06662\n",
      "78/78 - 4s - 47ms/step - accuracy: 0.9578 - loss: 0.1696 - val_accuracy: 0.9843 - val_loss: 0.0771 - learning_rate: 5.3760e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.06662\n",
      "78/78 - 4s - 47ms/step - accuracy: 0.9609 - loss: 0.1527 - val_accuracy: 0.9860 - val_loss: 0.0752 - learning_rate: 5.3760e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.06662\n",
      "78/78 - 4s - 47ms/step - accuracy: 0.9628 - loss: 0.1522 - val_accuracy: 0.9857 - val_loss: 0.0672 - learning_rate: 5.3760e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss improved from 0.06662 to 0.05426, saving model to outputs/step3_3_optuna\\trial_34\\models\\fold_7\\best_model.keras\n",
      "78/78 - 4s - 55ms/step - accuracy: 0.9688 - loss: 0.1347 - val_accuracy: 0.9906 - val_loss: 0.0543 - learning_rate: 5.3760e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.05426\n",
      "78/78 - 5s - 65ms/step - accuracy: 0.9699 - loss: 0.1329 - val_accuracy: 0.9833 - val_loss: 0.0735 - learning_rate: 5.3760e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.05426\n",
      "78/78 - 5s - 59ms/step - accuracy: 0.9692 - loss: 0.1321 - val_accuracy: 0.9906 - val_loss: 0.0547 - learning_rate: 5.3760e-04\n",
      "Restoring model weights from the end of the best epoch: 28.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 19:49:25,361] Trial 34 finished with value: 0.6351362377921409 and parameters: {'WINDOW_SIZE': 45, 'STRIDE': 3, 'gru1_units': 96, 'gru2_units': 0, 'dense_units': 64, 'dropout_rate': 0.49409151107570526, 'l2': 1.7985687592451012e-05, 'cf_filters': 80, 'cf_kernel': 3, 'cf_pool': 2, 'cf_drop': 0.1, 'cf_l2': 0.00018321836060834136, 'learning_rate': 0.0005376029327086378, 'batch_size': 128}. Best is trial 22 with value: 0.7596586896056158.\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.6351 ---\n",
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n",
      "학습 데이터가 11356개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 1.50673, saving model to outputs/step3_3_optuna\\trial_35\\models\\fold_7\\best_model.keras\n",
      "178/178 - 8s - 43ms/step - accuracy: 0.2992 - loss: 2.1159 - val_accuracy: 0.5001 - val_loss: 1.5067 - learning_rate: 3.5709e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 1.50673 to 1.01810, saving model to outputs/step3_3_optuna\\trial_35\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 29ms/step - accuracy: 0.5018 - loss: 1.5163 - val_accuracy: 0.6822 - val_loss: 1.0181 - learning_rate: 3.5709e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 1.01810 to 0.73707, saving model to outputs/step3_3_optuna\\trial_35\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 29ms/step - accuracy: 0.6103 - loss: 1.1796 - val_accuracy: 0.7726 - val_loss: 0.7371 - learning_rate: 3.5709e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 0.73707 to 0.55596, saving model to outputs/step3_3_optuna\\trial_35\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 26ms/step - accuracy: 0.6870 - loss: 0.9820 - val_accuracy: 0.8409 - val_loss: 0.5560 - learning_rate: 3.5709e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 0.55596 to 0.48372, saving model to outputs/step3_3_optuna\\trial_35\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 29ms/step - accuracy: 0.7321 - loss: 0.8409 - val_accuracy: 0.8541 - val_loss: 0.4837 - learning_rate: 3.5709e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 0.48372 to 0.45201, saving model to outputs/step3_3_optuna\\trial_35\\models\\fold_7\\best_model.keras\n",
      "178/178 - 6s - 32ms/step - accuracy: 0.7622 - loss: 0.7566 - val_accuracy: 0.8572 - val_loss: 0.4520 - learning_rate: 3.5709e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.45201 to 0.36486, saving model to outputs/step3_3_optuna\\trial_35\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 29ms/step - accuracy: 0.7884 - loss: 0.6620 - val_accuracy: 0.8807 - val_loss: 0.3649 - learning_rate: 3.5709e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 0.36486 to 0.32754, saving model to outputs/step3_3_optuna\\trial_35\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 29ms/step - accuracy: 0.8100 - loss: 0.6101 - val_accuracy: 0.8994 - val_loss: 0.3275 - learning_rate: 3.5709e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.32754 to 0.30793, saving model to outputs/step3_3_optuna\\trial_35\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 30ms/step - accuracy: 0.8287 - loss: 0.5563 - val_accuracy: 0.8988 - val_loss: 0.3079 - learning_rate: 3.5709e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 0.30793 to 0.27766, saving model to outputs/step3_3_optuna\\trial_35\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 29ms/step - accuracy: 0.8438 - loss: 0.5105 - val_accuracy: 0.9117 - val_loss: 0.2777 - learning_rate: 3.5709e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 0.27766 to 0.27261, saving model to outputs/step3_3_optuna\\trial_35\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 28ms/step - accuracy: 0.8498 - loss: 0.4842 - val_accuracy: 0.9150 - val_loss: 0.2726 - learning_rate: 3.5709e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 0.27261 to 0.22469, saving model to outputs/step3_3_optuna\\trial_35\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 29ms/step - accuracy: 0.8626 - loss: 0.4483 - val_accuracy: 0.9282 - val_loss: 0.2247 - learning_rate: 3.5709e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 0.22469 to 0.21464, saving model to outputs/step3_3_optuna\\trial_35\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 29ms/step - accuracy: 0.8721 - loss: 0.4198 - val_accuracy: 0.9295 - val_loss: 0.2146 - learning_rate: 3.5709e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 0.21464 to 0.18270, saving model to outputs/step3_3_optuna\\trial_35\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 29ms/step - accuracy: 0.8762 - loss: 0.4047 - val_accuracy: 0.9415 - val_loss: 0.1827 - learning_rate: 3.5709e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.18270\n",
      "178/178 - 5s - 26ms/step - accuracy: 0.8886 - loss: 0.3685 - val_accuracy: 0.9430 - val_loss: 0.1846 - learning_rate: 3.5709e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss improved from 0.18270 to 0.15235, saving model to outputs/step3_3_optuna\\trial_35\\models\\fold_7\\best_model.keras\n",
      "178/178 - 6s - 32ms/step - accuracy: 0.8897 - loss: 0.3601 - val_accuracy: 0.9543 - val_loss: 0.1524 - learning_rate: 3.5709e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.15235\n",
      "178/178 - 5s - 29ms/step - accuracy: 0.8968 - loss: 0.3407 - val_accuracy: 0.9511 - val_loss: 0.1598 - learning_rate: 3.5709e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss improved from 0.15235 to 0.13445, saving model to outputs/step3_3_optuna\\trial_35\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 29ms/step - accuracy: 0.9033 - loss: 0.3314 - val_accuracy: 0.9591 - val_loss: 0.1344 - learning_rate: 3.5709e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.13445\n",
      "178/178 - 5s - 28ms/step - accuracy: 0.9083 - loss: 0.3091 - val_accuracy: 0.9505 - val_loss: 0.1589 - learning_rate: 3.5709e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss improved from 0.13445 to 0.12944, saving model to outputs/step3_3_optuna\\trial_35\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 29ms/step - accuracy: 0.9137 - loss: 0.2936 - val_accuracy: 0.9594 - val_loss: 0.1294 - learning_rate: 3.5709e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.12944\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.9133 - loss: 0.2860 - val_accuracy: 0.9588 - val_loss: 0.1360 - learning_rate: 3.5709e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.12944\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.9192 - loss: 0.2779 - val_accuracy: 0.9596 - val_loss: 0.1309 - learning_rate: 3.5709e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss improved from 0.12944 to 0.09818, saving model to outputs/step3_3_optuna\\trial_35\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 24ms/step - accuracy: 0.9241 - loss: 0.2638 - val_accuracy: 0.9728 - val_loss: 0.0982 - learning_rate: 3.5709e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss improved from 0.09818 to 0.09478, saving model to outputs/step3_3_optuna\\trial_35\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 24ms/step - accuracy: 0.9261 - loss: 0.2561 - val_accuracy: 0.9744 - val_loss: 0.0948 - learning_rate: 3.5709e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.09478\n",
      "178/178 - 5s - 29ms/step - accuracy: 0.9308 - loss: 0.2427 - val_accuracy: 0.9698 - val_loss: 0.1082 - learning_rate: 3.5709e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.09478\n",
      "178/178 - 5s - 26ms/step - accuracy: 0.9277 - loss: 0.2375 - val_accuracy: 0.9653 - val_loss: 0.1129 - learning_rate: 3.5709e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.09478\n",
      "178/178 - 4s - 24ms/step - accuracy: 0.9338 - loss: 0.2332 - val_accuracy: 0.9690 - val_loss: 0.1080 - learning_rate: 3.5709e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss improved from 0.09478 to 0.08694, saving model to outputs/step3_3_optuna\\trial_35\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 30ms/step - accuracy: 0.9341 - loss: 0.2308 - val_accuracy: 0.9758 - val_loss: 0.0869 - learning_rate: 3.5709e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.08694\n",
      "178/178 - 5s - 29ms/step - accuracy: 0.9399 - loss: 0.2105 - val_accuracy: 0.9750 - val_loss: 0.0916 - learning_rate: 3.5709e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss improved from 0.08694 to 0.08586, saving model to outputs/step3_3_optuna\\trial_35\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 29ms/step - accuracy: 0.9394 - loss: 0.2183 - val_accuracy: 0.9772 - val_loss: 0.0859 - learning_rate: 3.5709e-04\n",
      "Restoring model weights from the end of the best epoch: 30.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 19:52:03,108] Trial 35 finished with value: 0.6980669169985 and parameters: {'WINDOW_SIZE': 30, 'STRIDE': 3, 'gru1_units': 160, 'gru2_units': 0, 'dense_units': 32, 'dropout_rate': 0.3773318811198012, 'l2': 5.644968181559248e-05, 'cf_filters': 48, 'cf_kernel': 3, 'cf_pool': 3, 'cf_drop': 0.1, 'cf_l2': 4.049909022771875e-05, 'learning_rate': 0.00035709329408003263, 'batch_size': 64}. Best is trial 22 with value: 0.7596586896056158.\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.6981 ---\n",
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n",
      "학습 데이터가 6924개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 1.87117, saving model to outputs/step3_3_optuna\\trial_36\\models\\fold_7\\best_model.keras\n",
      "109/109 - 10s - 92ms/step - accuracy: 0.2250 - loss: 2.3695 - val_accuracy: 0.4021 - val_loss: 1.8712 - learning_rate: 3.5813e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 1.87117 to 1.62528, saving model to outputs/step3_3_optuna\\trial_36\\models\\fold_7\\best_model.keras\n",
      "109/109 - 5s - 41ms/step - accuracy: 0.3744 - loss: 1.9503 - val_accuracy: 0.4590 - val_loss: 1.6253 - learning_rate: 3.5813e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 1.62528 to 1.42368, saving model to outputs/step3_3_optuna\\trial_36\\models\\fold_7\\best_model.keras\n",
      "109/109 - 3s - 29ms/step - accuracy: 0.4321 - loss: 1.7590 - val_accuracy: 0.5707 - val_loss: 1.4237 - learning_rate: 3.5813e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 1.42368 to 1.26731, saving model to outputs/step3_3_optuna\\trial_36\\models\\fold_7\\best_model.keras\n",
      "109/109 - 3s - 29ms/step - accuracy: 0.4806 - loss: 1.6121 - val_accuracy: 0.6502 - val_loss: 1.2673 - learning_rate: 3.5813e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 1.26731 to 0.93964, saving model to outputs/step3_3_optuna\\trial_36\\models\\fold_7\\best_model.keras\n",
      "109/109 - 5s - 47ms/step - accuracy: 0.5478 - loss: 1.4132 - val_accuracy: 0.7405 - val_loss: 0.9396 - learning_rate: 3.5813e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 0.93964 to 0.75373, saving model to outputs/step3_3_optuna\\trial_36\\models\\fold_7\\best_model.keras\n",
      "109/109 - 5s - 47ms/step - accuracy: 0.6059 - loss: 1.2334 - val_accuracy: 0.8022 - val_loss: 0.7537 - learning_rate: 3.5813e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.75373 to 0.64328, saving model to outputs/step3_3_optuna\\trial_36\\models\\fold_7\\best_model.keras\n",
      "109/109 - 5s - 48ms/step - accuracy: 0.6518 - loss: 1.0872 - val_accuracy: 0.8228 - val_loss: 0.6433 - learning_rate: 3.5813e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 0.64328 to 0.57444, saving model to outputs/step3_3_optuna\\trial_36\\models\\fold_7\\best_model.keras\n",
      "109/109 - 5s - 49ms/step - accuracy: 0.6899 - loss: 0.9804 - val_accuracy: 0.8252 - val_loss: 0.5744 - learning_rate: 3.5813e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.57444 to 0.53850, saving model to outputs/step3_3_optuna\\trial_36\\models\\fold_7\\best_model.keras\n",
      "109/109 - 3s - 28ms/step - accuracy: 0.7091 - loss: 0.9271 - val_accuracy: 0.8449 - val_loss: 0.5385 - learning_rate: 3.5813e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 0.53850 to 0.44870, saving model to outputs/step3_3_optuna\\trial_36\\models\\fold_7\\best_model.keras\n",
      "109/109 - 5s - 48ms/step - accuracy: 0.7377 - loss: 0.8451 - val_accuracy: 0.8652 - val_loss: 0.4487 - learning_rate: 3.5813e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 0.44870 to 0.42447, saving model to outputs/step3_3_optuna\\trial_36\\models\\fold_7\\best_model.keras\n",
      "109/109 - 5s - 47ms/step - accuracy: 0.7462 - loss: 0.8117 - val_accuracy: 0.8677 - val_loss: 0.4245 - learning_rate: 3.5813e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 0.42447 to 0.39307, saving model to outputs/step3_3_optuna\\trial_36\\models\\fold_7\\best_model.keras\n",
      "109/109 - 5s - 48ms/step - accuracy: 0.7673 - loss: 0.7486 - val_accuracy: 0.8783 - val_loss: 0.3931 - learning_rate: 3.5813e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 0.39307 to 0.39079, saving model to outputs/step3_3_optuna\\trial_36\\models\\fold_7\\best_model.keras\n",
      "109/109 - 5s - 47ms/step - accuracy: 0.7787 - loss: 0.7194 - val_accuracy: 0.8866 - val_loss: 0.3908 - learning_rate: 3.5813e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 0.39079 to 0.38038, saving model to outputs/step3_3_optuna\\trial_36\\models\\fold_7\\best_model.keras\n",
      "109/109 - 5s - 48ms/step - accuracy: 0.7922 - loss: 0.6665 - val_accuracy: 0.8758 - val_loss: 0.3804 - learning_rate: 3.5813e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss improved from 0.38038 to 0.33748, saving model to outputs/step3_3_optuna\\trial_36\\models\\fold_7\\best_model.keras\n",
      "109/109 - 3s - 29ms/step - accuracy: 0.8040 - loss: 0.6349 - val_accuracy: 0.8967 - val_loss: 0.3375 - learning_rate: 3.5813e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss improved from 0.33748 to 0.29254, saving model to outputs/step3_3_optuna\\trial_36\\models\\fold_7\\best_model.keras\n",
      "109/109 - 5s - 48ms/step - accuracy: 0.8131 - loss: 0.5971 - val_accuracy: 0.9170 - val_loss: 0.2925 - learning_rate: 3.5813e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.29254\n",
      "109/109 - 5s - 47ms/step - accuracy: 0.8284 - loss: 0.5499 - val_accuracy: 0.9038 - val_loss: 0.3242 - learning_rate: 3.5813e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss improved from 0.29254 to 0.29057, saving model to outputs/step3_3_optuna\\trial_36\\models\\fold_7\\best_model.keras\n",
      "109/109 - 5s - 48ms/step - accuracy: 0.8320 - loss: 0.5640 - val_accuracy: 0.9141 - val_loss: 0.2906 - learning_rate: 3.5813e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss improved from 0.29057 to 0.26780, saving model to outputs/step3_3_optuna\\trial_36\\models\\fold_7\\best_model.keras\n",
      "109/109 - 5s - 46ms/step - accuracy: 0.8452 - loss: 0.5250 - val_accuracy: 0.9185 - val_loss: 0.2678 - learning_rate: 3.5813e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss improved from 0.26780 to 0.23305, saving model to outputs/step3_3_optuna\\trial_36\\models\\fold_7\\best_model.keras\n",
      "109/109 - 3s - 29ms/step - accuracy: 0.8473 - loss: 0.5125 - val_accuracy: 0.9308 - val_loss: 0.2331 - learning_rate: 3.5813e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss improved from 0.23305 to 0.23237, saving model to outputs/step3_3_optuna\\trial_36\\models\\fold_7\\best_model.keras\n",
      "109/109 - 5s - 48ms/step - accuracy: 0.8573 - loss: 0.4974 - val_accuracy: 0.9300 - val_loss: 0.2324 - learning_rate: 3.5813e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss improved from 0.23237 to 0.20814, saving model to outputs/step3_3_optuna\\trial_36\\models\\fold_7\\best_model.keras\n",
      "109/109 - 5s - 47ms/step - accuracy: 0.8635 - loss: 0.4743 - val_accuracy: 0.9364 - val_loss: 0.2081 - learning_rate: 3.5813e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss improved from 0.20814 to 0.19942, saving model to outputs/step3_3_optuna\\trial_36\\models\\fold_7\\best_model.keras\n",
      "109/109 - 5s - 47ms/step - accuracy: 0.8694 - loss: 0.4447 - val_accuracy: 0.9381 - val_loss: 0.1994 - learning_rate: 3.5813e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.19942\n",
      "109/109 - 3s - 28ms/step - accuracy: 0.8680 - loss: 0.4442 - val_accuracy: 0.9364 - val_loss: 0.1995 - learning_rate: 3.5813e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss improved from 0.19942 to 0.18646, saving model to outputs/step3_3_optuna\\trial_36\\models\\fold_7\\best_model.keras\n",
      "109/109 - 5s - 49ms/step - accuracy: 0.8738 - loss: 0.4172 - val_accuracy: 0.9467 - val_loss: 0.1865 - learning_rate: 3.5813e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss improved from 0.18646 to 0.18004, saving model to outputs/step3_3_optuna\\trial_36\\models\\fold_7\\best_model.keras\n",
      "109/109 - 5s - 47ms/step - accuracy: 0.8768 - loss: 0.4313 - val_accuracy: 0.9494 - val_loss: 0.1800 - learning_rate: 3.5813e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.18004\n",
      "109/109 - 5s - 46ms/step - accuracy: 0.8845 - loss: 0.3926 - val_accuracy: 0.9391 - val_loss: 0.1967 - learning_rate: 3.5813e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss improved from 0.18004 to 0.17222, saving model to outputs/step3_3_optuna\\trial_36\\models\\fold_7\\best_model.keras\n",
      "109/109 - 3s - 29ms/step - accuracy: 0.8907 - loss: 0.3857 - val_accuracy: 0.9519 - val_loss: 0.1722 - learning_rate: 3.5813e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss improved from 0.17222 to 0.15116, saving model to outputs/step3_3_optuna\\trial_36\\models\\fold_7\\best_model.keras\n",
      "109/109 - 3s - 28ms/step - accuracy: 0.8950 - loss: 0.3730 - val_accuracy: 0.9543 - val_loss: 0.1512 - learning_rate: 3.5813e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.15116\n",
      "109/109 - 3s - 28ms/step - accuracy: 0.8999 - loss: 0.3595 - val_accuracy: 0.9531 - val_loss: 0.1606 - learning_rate: 3.5813e-04\n",
      "Restoring model weights from the end of the best epoch: 29.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 19:54:32,256] Trial 36 finished with value: 0.5185254250125324 and parameters: {'WINDOW_SIZE': 30, 'STRIDE': 5, 'gru1_units': 160, 'gru2_units': 64, 'dense_units': 32, 'dropout_rate': 0.3700113960419861, 'l2': 5.701460131870217e-05, 'cf_filters': 48, 'cf_kernel': 5, 'cf_pool': 3, 'cf_drop': 0.1, 'cf_l2': 5.699223156382538e-05, 'learning_rate': 0.00035812875495028613, 'batch_size': 64}. Best is trial 22 with value: 0.7596586896056158.\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.5185 ---\n",
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n",
      "학습 데이터가 11356개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 1.28466, saving model to outputs/step3_3_optuna\\trial_37\\models\\fold_7\\best_model.keras\n",
      "178/178 - 10s - 56ms/step - accuracy: 0.3440 - loss: 1.9965 - val_accuracy: 0.6125 - val_loss: 1.2847 - learning_rate: 2.7617e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 1.28466 to 0.83093, saving model to outputs/step3_3_optuna\\trial_37\\models\\fold_7\\best_model.keras\n",
      "178/178 - 10s - 54ms/step - accuracy: 0.5797 - loss: 1.2891 - val_accuracy: 0.7358 - val_loss: 0.8309 - learning_rate: 2.7617e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 0.83093 to 0.60787, saving model to outputs/step3_3_optuna\\trial_37\\models\\fold_7\\best_model.keras\n",
      "178/178 - 10s - 57ms/step - accuracy: 0.6896 - loss: 0.9646 - val_accuracy: 0.8163 - val_loss: 0.6079 - learning_rate: 2.7617e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 0.60787 to 0.51002, saving model to outputs/step3_3_optuna\\trial_37\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 30ms/step - accuracy: 0.7553 - loss: 0.7687 - val_accuracy: 0.8470 - val_loss: 0.5100 - learning_rate: 2.7617e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 0.51002 to 0.39811, saving model to outputs/step3_3_optuna\\trial_37\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 30ms/step - accuracy: 0.7945 - loss: 0.6553 - val_accuracy: 0.8731 - val_loss: 0.3981 - learning_rate: 2.7617e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 0.39811 to 0.32403, saving model to outputs/step3_3_optuna\\trial_37\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 28ms/step - accuracy: 0.8209 - loss: 0.5815 - val_accuracy: 0.9018 - val_loss: 0.3240 - learning_rate: 2.7617e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.32403 to 0.30680, saving model to outputs/step3_3_optuna\\trial_37\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 29ms/step - accuracy: 0.8348 - loss: 0.5253 - val_accuracy: 0.8970 - val_loss: 0.3068 - learning_rate: 2.7617e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 0.30680 to 0.27527, saving model to outputs/step3_3_optuna\\trial_37\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 29ms/step - accuracy: 0.8559 - loss: 0.4646 - val_accuracy: 0.9103 - val_loss: 0.2753 - learning_rate: 2.7617e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.27527 to 0.23399, saving model to outputs/step3_3_optuna\\trial_37\\models\\fold_7\\best_model.keras\n",
      "178/178 - 10s - 56ms/step - accuracy: 0.8616 - loss: 0.4411 - val_accuracy: 0.9277 - val_loss: 0.2340 - learning_rate: 2.7617e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 0.23399 to 0.21984, saving model to outputs/step3_3_optuna\\trial_37\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 29ms/step - accuracy: 0.8743 - loss: 0.4033 - val_accuracy: 0.9304 - val_loss: 0.2198 - learning_rate: 2.7617e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 0.21984 to 0.19862, saving model to outputs/step3_3_optuna\\trial_37\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 30ms/step - accuracy: 0.8824 - loss: 0.3732 - val_accuracy: 0.9382 - val_loss: 0.1986 - learning_rate: 2.7617e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 0.19862 to 0.18209, saving model to outputs/step3_3_optuna\\trial_37\\models\\fold_7\\best_model.keras\n",
      "178/178 - 10s - 58ms/step - accuracy: 0.8944 - loss: 0.3386 - val_accuracy: 0.9467 - val_loss: 0.1821 - learning_rate: 2.7617e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.18209\n",
      "178/178 - 10s - 56ms/step - accuracy: 0.8984 - loss: 0.3318 - val_accuracy: 0.9400 - val_loss: 0.1915 - learning_rate: 2.7617e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 0.18209 to 0.16657, saving model to outputs/step3_3_optuna\\trial_37\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 30ms/step - accuracy: 0.9031 - loss: 0.3105 - val_accuracy: 0.9487 - val_loss: 0.1666 - learning_rate: 2.7617e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss improved from 0.16657 to 0.14997, saving model to outputs/step3_3_optuna\\trial_37\\models\\fold_7\\best_model.keras\n",
      "178/178 - 10s - 58ms/step - accuracy: 0.9089 - loss: 0.3025 - val_accuracy: 0.9566 - val_loss: 0.1500 - learning_rate: 2.7617e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss improved from 0.14997 to 0.13553, saving model to outputs/step3_3_optuna\\trial_37\\models\\fold_7\\best_model.keras\n",
      "178/178 - 10s - 58ms/step - accuracy: 0.9148 - loss: 0.2761 - val_accuracy: 0.9608 - val_loss: 0.1355 - learning_rate: 2.7617e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 0.13553 to 0.13263, saving model to outputs/step3_3_optuna\\trial_37\\models\\fold_7\\best_model.keras\n",
      "178/178 - 10s - 57ms/step - accuracy: 0.9170 - loss: 0.2723 - val_accuracy: 0.9600 - val_loss: 0.1326 - learning_rate: 2.7617e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss improved from 0.13263 to 0.11690, saving model to outputs/step3_3_optuna\\trial_37\\models\\fold_7\\best_model.keras\n",
      "178/178 - 10s - 58ms/step - accuracy: 0.9206 - loss: 0.2622 - val_accuracy: 0.9672 - val_loss: 0.1169 - learning_rate: 2.7617e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss improved from 0.11690 to 0.11607, saving model to outputs/step3_3_optuna\\trial_37\\models\\fold_7\\best_model.keras\n",
      "178/178 - 10s - 58ms/step - accuracy: 0.9266 - loss: 0.2521 - val_accuracy: 0.9656 - val_loss: 0.1161 - learning_rate: 2.7617e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss improved from 0.11607 to 0.11166, saving model to outputs/step3_3_optuna\\trial_37\\models\\fold_7\\best_model.keras\n",
      "178/178 - 10s - 56ms/step - accuracy: 0.9290 - loss: 0.2386 - val_accuracy: 0.9689 - val_loss: 0.1117 - learning_rate: 2.7617e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss improved from 0.11166 to 0.10744, saving model to outputs/step3_3_optuna\\trial_37\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 29ms/step - accuracy: 0.9317 - loss: 0.2343 - val_accuracy: 0.9707 - val_loss: 0.1074 - learning_rate: 2.7617e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.10744\n",
      "178/178 - 5s - 29ms/step - accuracy: 0.9367 - loss: 0.2155 - val_accuracy: 0.9629 - val_loss: 0.1269 - learning_rate: 2.7617e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss improved from 0.10744 to 0.08905, saving model to outputs/step3_3_optuna\\trial_37\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 29ms/step - accuracy: 0.9368 - loss: 0.2123 - val_accuracy: 0.9781 - val_loss: 0.0890 - learning_rate: 2.7617e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.08905\n",
      "178/178 - 5s - 30ms/step - accuracy: 0.9395 - loss: 0.2052 - val_accuracy: 0.9735 - val_loss: 0.0963 - learning_rate: 2.7617e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.08905\n",
      "178/178 - 5s - 28ms/step - accuracy: 0.9428 - loss: 0.1956 - val_accuracy: 0.9746 - val_loss: 0.0958 - learning_rate: 2.7617e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss improved from 0.08905 to 0.08516, saving model to outputs/step3_3_optuna\\trial_37\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 30ms/step - accuracy: 0.9439 - loss: 0.1916 - val_accuracy: 0.9778 - val_loss: 0.0852 - learning_rate: 2.7617e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss improved from 0.08516 to 0.08080, saving model to outputs/step3_3_optuna\\trial_37\\models\\fold_7\\best_model.keras\n",
      "178/178 - 10s - 57ms/step - accuracy: 0.9454 - loss: 0.1847 - val_accuracy: 0.9795 - val_loss: 0.0808 - learning_rate: 2.7617e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss improved from 0.08080 to 0.07463, saving model to outputs/step3_3_optuna\\trial_37\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 30ms/step - accuracy: 0.9457 - loss: 0.1844 - val_accuracy: 0.9816 - val_loss: 0.0746 - learning_rate: 2.7617e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.07463\n",
      "178/178 - 5s - 29ms/step - accuracy: 0.9534 - loss: 0.1664 - val_accuracy: 0.9726 - val_loss: 0.0963 - learning_rate: 2.7617e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.07463\n",
      "178/178 - 5s - 29ms/step - accuracy: 0.9519 - loss: 0.1680 - val_accuracy: 0.9722 - val_loss: 0.0948 - learning_rate: 2.7617e-04\n",
      "Restoring model weights from the end of the best epoch: 28.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 19:58:18,105] Trial 37 finished with value: 0.5664484069446366 and parameters: {'WINDOW_SIZE': 30, 'STRIDE': 3, 'gru1_units': 160, 'gru2_units': 0, 'dense_units': 64, 'dropout_rate': 0.32568437463204736, 'l2': 7.047822530417157e-05, 'cf_filters': 48, 'cf_kernel': 3, 'cf_pool': 2, 'cf_drop': 0.1, 'cf_l2': 4.334704764954793e-05, 'learning_rate': 0.0002761711857416344, 'batch_size': 64}. Best is trial 22 with value: 0.7596586896056158.\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.5664 ---\n",
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n",
      "학습 데이터가 6924개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 1.83175, saving model to outputs/step3_3_optuna\\trial_38\\models\\fold_7\\best_model.keras\n",
      "55/55 - 8s - 144ms/step - accuracy: 0.2132 - loss: 2.4182 - val_accuracy: 0.3876 - val_loss: 1.8317 - learning_rate: 2.9814e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 1.83175 to 1.52506, saving model to outputs/step3_3_optuna\\trial_38\\models\\fold_7\\best_model.keras\n",
      "55/55 - 3s - 47ms/step - accuracy: 0.3618 - loss: 1.9425 - val_accuracy: 0.4892 - val_loss: 1.5251 - learning_rate: 2.9814e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 1.52506 to 1.25287, saving model to outputs/step3_3_optuna\\trial_38\\models\\fold_7\\best_model.keras\n",
      "55/55 - 3s - 48ms/step - accuracy: 0.4393 - loss: 1.6760 - val_accuracy: 0.6230 - val_loss: 1.2529 - learning_rate: 2.9814e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 1.25287 to 1.02204, saving model to outputs/step3_3_optuna\\trial_38\\models\\fold_7\\best_model.keras\n",
      "55/55 - 3s - 49ms/step - accuracy: 0.5144 - loss: 1.4497 - val_accuracy: 0.6897 - val_loss: 1.0220 - learning_rate: 2.9814e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 1.02204 to 0.89040, saving model to outputs/step3_3_optuna\\trial_38\\models\\fold_7\\best_model.keras\n",
      "55/55 - 3s - 47ms/step - accuracy: 0.5903 - loss: 1.2498 - val_accuracy: 0.7243 - val_loss: 0.8904 - learning_rate: 2.9814e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 0.89040 to 0.77161, saving model to outputs/step3_3_optuna\\trial_38\\models\\fold_7\\best_model.keras\n",
      "55/55 - 3s - 47ms/step - accuracy: 0.6391 - loss: 1.1238 - val_accuracy: 0.7607 - val_loss: 0.7716 - learning_rate: 2.9814e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.77161 to 0.68120, saving model to outputs/step3_3_optuna\\trial_38\\models\\fold_7\\best_model.keras\n",
      "55/55 - 2s - 43ms/step - accuracy: 0.6811 - loss: 0.9997 - val_accuracy: 0.7923 - val_loss: 0.6812 - learning_rate: 2.9814e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 0.68120 to 0.59822, saving model to outputs/step3_3_optuna\\trial_38\\models\\fold_7\\best_model.keras\n",
      "55/55 - 3s - 46ms/step - accuracy: 0.7159 - loss: 0.9049 - val_accuracy: 0.8186 - val_loss: 0.5982 - learning_rate: 2.9814e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.59822 to 0.52362, saving model to outputs/step3_3_optuna\\trial_38\\models\\fold_7\\best_model.keras\n",
      "55/55 - 3s - 49ms/step - accuracy: 0.7406 - loss: 0.8198 - val_accuracy: 0.8405 - val_loss: 0.5236 - learning_rate: 2.9814e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 0.52362 to 0.46921, saving model to outputs/step3_3_optuna\\trial_38\\models\\fold_7\\best_model.keras\n",
      "55/55 - 5s - 87ms/step - accuracy: 0.7611 - loss: 0.7616 - val_accuracy: 0.8559 - val_loss: 0.4692 - learning_rate: 2.9814e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 0.46921 to 0.42630, saving model to outputs/step3_3_optuna\\trial_38\\models\\fold_7\\best_model.keras\n",
      "55/55 - 3s - 51ms/step - accuracy: 0.7744 - loss: 0.7227 - val_accuracy: 0.8699 - val_loss: 0.4263 - learning_rate: 2.9814e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 0.42630 to 0.40657, saving model to outputs/step3_3_optuna\\trial_38\\models\\fold_7\\best_model.keras\n",
      "55/55 - 3s - 47ms/step - accuracy: 0.7883 - loss: 0.6731 - val_accuracy: 0.8775 - val_loss: 0.4066 - learning_rate: 2.9814e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 0.40657 to 0.38269, saving model to outputs/step3_3_optuna\\trial_38\\models\\fold_7\\best_model.keras\n",
      "55/55 - 2s - 44ms/step - accuracy: 0.8065 - loss: 0.6298 - val_accuracy: 0.8795 - val_loss: 0.3827 - learning_rate: 2.9814e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 0.38269 to 0.35714, saving model to outputs/step3_3_optuna\\trial_38\\models\\fold_7\\best_model.keras\n",
      "55/55 - 3s - 48ms/step - accuracy: 0.8122 - loss: 0.5946 - val_accuracy: 0.8871 - val_loss: 0.3571 - learning_rate: 2.9814e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss improved from 0.35714 to 0.32916, saving model to outputs/step3_3_optuna\\trial_38\\models\\fold_7\\best_model.keras\n",
      "55/55 - 3s - 51ms/step - accuracy: 0.8286 - loss: 0.5605 - val_accuracy: 0.8974 - val_loss: 0.3292 - learning_rate: 2.9814e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss improved from 0.32916 to 0.30551, saving model to outputs/step3_3_optuna\\trial_38\\models\\fold_7\\best_model.keras\n",
      "55/55 - 2s - 44ms/step - accuracy: 0.8283 - loss: 0.5386 - val_accuracy: 0.9040 - val_loss: 0.3055 - learning_rate: 2.9814e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 0.30551 to 0.29500, saving model to outputs/step3_3_optuna\\trial_38\\models\\fold_7\\best_model.keras\n",
      "55/55 - 3s - 46ms/step - accuracy: 0.8394 - loss: 0.5112 - val_accuracy: 0.9062 - val_loss: 0.2950 - learning_rate: 2.9814e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss improved from 0.29500 to 0.27199, saving model to outputs/step3_3_optuna\\trial_38\\models\\fold_7\\best_model.keras\n",
      "55/55 - 3s - 52ms/step - accuracy: 0.8497 - loss: 0.4976 - val_accuracy: 0.9158 - val_loss: 0.2720 - learning_rate: 2.9814e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.27199\n",
      "55/55 - 2s - 42ms/step - accuracy: 0.8523 - loss: 0.4764 - val_accuracy: 0.9097 - val_loss: 0.2750 - learning_rate: 2.9814e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss improved from 0.27199 to 0.23899, saving model to outputs/step3_3_optuna\\trial_38\\models\\fold_7\\best_model.keras\n",
      "55/55 - 3s - 49ms/step - accuracy: 0.8564 - loss: 0.4638 - val_accuracy: 0.9259 - val_loss: 0.2390 - learning_rate: 2.9814e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss improved from 0.23899 to 0.22406, saving model to outputs/step3_3_optuna\\trial_38\\models\\fold_7\\best_model.keras\n",
      "55/55 - 3s - 48ms/step - accuracy: 0.8634 - loss: 0.4365 - val_accuracy: 0.9278 - val_loss: 0.2241 - learning_rate: 2.9814e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss improved from 0.22406 to 0.21874, saving model to outputs/step3_3_optuna\\trial_38\\models\\fold_7\\best_model.keras\n",
      "55/55 - 3s - 47ms/step - accuracy: 0.8707 - loss: 0.4212 - val_accuracy: 0.9313 - val_loss: 0.2187 - learning_rate: 2.9814e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.21874\n",
      "55/55 - 2s - 40ms/step - accuracy: 0.8741 - loss: 0.4118 - val_accuracy: 0.9207 - val_loss: 0.2349 - learning_rate: 2.9814e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss improved from 0.21874 to 0.20385, saving model to outputs/step3_3_optuna\\trial_38\\models\\fold_7\\best_model.keras\n",
      "55/55 - 3s - 49ms/step - accuracy: 0.8762 - loss: 0.4045 - val_accuracy: 0.9335 - val_loss: 0.2039 - learning_rate: 2.9814e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss improved from 0.20385 to 0.19412, saving model to outputs/step3_3_optuna\\trial_38\\models\\fold_7\\best_model.keras\n",
      "55/55 - 3s - 46ms/step - accuracy: 0.8845 - loss: 0.3827 - val_accuracy: 0.9354 - val_loss: 0.1941 - learning_rate: 2.9814e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss improved from 0.19412 to 0.19371, saving model to outputs/step3_3_optuna\\trial_38\\models\\fold_7\\best_model.keras\n",
      "55/55 - 2s - 41ms/step - accuracy: 0.8908 - loss: 0.3625 - val_accuracy: 0.9386 - val_loss: 0.1937 - learning_rate: 2.9814e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss improved from 0.19371 to 0.17905, saving model to outputs/step3_3_optuna\\trial_38\\models\\fold_7\\best_model.keras\n",
      "55/55 - 2s - 45ms/step - accuracy: 0.8830 - loss: 0.3743 - val_accuracy: 0.9413 - val_loss: 0.1791 - learning_rate: 2.9814e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss improved from 0.17905 to 0.16491, saving model to outputs/step3_3_optuna\\trial_38\\models\\fold_7\\best_model.keras\n",
      "55/55 - 2s - 45ms/step - accuracy: 0.8891 - loss: 0.3569 - val_accuracy: 0.9482 - val_loss: 0.1649 - learning_rate: 2.9814e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss improved from 0.16491 to 0.16016, saving model to outputs/step3_3_optuna\\trial_38\\models\\fold_7\\best_model.keras\n",
      "55/55 - 2s - 44ms/step - accuracy: 0.8993 - loss: 0.3287 - val_accuracy: 0.9489 - val_loss: 0.1602 - learning_rate: 2.9814e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss improved from 0.16016 to 0.15405, saving model to outputs/step3_3_optuna\\trial_38\\models\\fold_7\\best_model.keras\n",
      "55/55 - 3s - 50ms/step - accuracy: 0.8954 - loss: 0.3347 - val_accuracy: 0.9502 - val_loss: 0.1540 - learning_rate: 2.9814e-04\n",
      "Restoring model weights from the end of the best epoch: 30.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 19:59:47,990] Trial 38 finished with value: 0.4939170123344071 and parameters: {'WINDOW_SIZE': 30, 'STRIDE': 5, 'gru1_units': 160, 'gru2_units': 64, 'dense_units': 96, 'dropout_rate': 0.3405882878959645, 'l2': 5.2497392463711875e-05, 'cf_filters': 48, 'cf_kernel': 3, 'cf_pool': 3, 'cf_drop': 0.0, 'cf_l2': 4.161709571671153e-05, 'learning_rate': 0.0002981387166615917, 'batch_size': 128}. Best is trial 22 with value: 0.7596586896056158.\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.4939 ---\n",
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "특징 추출(병렬): 100%|██████████| 1111/1111 [00:26<00:00, 42.55it/s] \n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "청크 병합: 100%|██████████| 1111/1111 [00:16<00:00, 69.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터가 12739개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 1.76189, saving model to outputs/step3_3_optuna\\trial_39\\models\\fold_7\\best_model.keras\n",
      "200/200 - 10s - 51ms/step - accuracy: 0.2533 - loss: 2.2977 - val_accuracy: 0.4161 - val_loss: 1.7619 - learning_rate: 2.2957e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 1.76189 to 1.38630, saving model to outputs/step3_3_optuna\\trial_39\\models\\fold_7\\best_model.keras\n",
      "200/200 - 4s - 20ms/step - accuracy: 0.3902 - loss: 1.8664 - val_accuracy: 0.5817 - val_loss: 1.3863 - learning_rate: 2.2957e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 1.38630 to 1.11078, saving model to outputs/step3_3_optuna\\trial_39\\models\\fold_7\\best_model.keras\n",
      "200/200 - 4s - 18ms/step - accuracy: 0.4708 - loss: 1.6198 - val_accuracy: 0.6723 - val_loss: 1.1108 - learning_rate: 2.2957e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 1.11078 to 0.93286, saving model to outputs/step3_3_optuna\\trial_39\\models\\fold_7\\best_model.keras\n",
      "200/200 - 5s - 26ms/step - accuracy: 0.5345 - loss: 1.4194 - val_accuracy: 0.7410 - val_loss: 0.9329 - learning_rate: 2.2957e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 0.93286 to 0.79680, saving model to outputs/step3_3_optuna\\trial_39\\models\\fold_7\\best_model.keras\n",
      "200/200 - 4s - 21ms/step - accuracy: 0.5825 - loss: 1.2886 - val_accuracy: 0.7697 - val_loss: 0.7968 - learning_rate: 2.2957e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 0.79680 to 0.69532, saving model to outputs/step3_3_optuna\\trial_39\\models\\fold_7\\best_model.keras\n",
      "200/200 - 5s - 26ms/step - accuracy: 0.6303 - loss: 1.1647 - val_accuracy: 0.8038 - val_loss: 0.6953 - learning_rate: 2.2957e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.69532 to 0.63840, saving model to outputs/step3_3_optuna\\trial_39\\models\\fold_7\\best_model.keras\n",
      "200/200 - 5s - 23ms/step - accuracy: 0.6559 - loss: 1.0783 - val_accuracy: 0.8216 - val_loss: 0.6384 - learning_rate: 2.2957e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 0.63840 to 0.57171, saving model to outputs/step3_3_optuna\\trial_39\\models\\fold_7\\best_model.keras\n",
      "200/200 - 4s - 18ms/step - accuracy: 0.6804 - loss: 0.9947 - val_accuracy: 0.8412 - val_loss: 0.5717 - learning_rate: 2.2957e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.57171 to 0.54566, saving model to outputs/step3_3_optuna\\trial_39\\models\\fold_7\\best_model.keras\n",
      "200/200 - 5s - 25ms/step - accuracy: 0.6987 - loss: 0.9407 - val_accuracy: 0.8419 - val_loss: 0.5457 - learning_rate: 2.2957e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 0.54566 to 0.51529, saving model to outputs/step3_3_optuna\\trial_39\\models\\fold_7\\best_model.keras\n",
      "200/200 - 4s - 18ms/step - accuracy: 0.7209 - loss: 0.8839 - val_accuracy: 0.8575 - val_loss: 0.5153 - learning_rate: 2.2957e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 0.51529 to 0.46462, saving model to outputs/step3_3_optuna\\trial_39\\models\\fold_7\\best_model.keras\n",
      "200/200 - 4s - 18ms/step - accuracy: 0.7373 - loss: 0.8431 - val_accuracy: 0.8683 - val_loss: 0.4646 - learning_rate: 2.2957e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 0.46462 to 0.45493, saving model to outputs/step3_3_optuna\\trial_39\\models\\fold_7\\best_model.keras\n",
      "200/200 - 5s - 26ms/step - accuracy: 0.7412 - loss: 0.8163 - val_accuracy: 0.8711 - val_loss: 0.4549 - learning_rate: 2.2957e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 0.45493 to 0.44242, saving model to outputs/step3_3_optuna\\trial_39\\models\\fold_7\\best_model.keras\n",
      "200/200 - 4s - 18ms/step - accuracy: 0.7573 - loss: 0.7719 - val_accuracy: 0.8675 - val_loss: 0.4424 - learning_rate: 2.2957e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 0.44242 to 0.41630, saving model to outputs/step3_3_optuna\\trial_39\\models\\fold_7\\best_model.keras\n",
      "200/200 - 4s - 18ms/step - accuracy: 0.7627 - loss: 0.7666 - val_accuracy: 0.8819 - val_loss: 0.4163 - learning_rate: 2.2957e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss improved from 0.41630 to 0.38466, saving model to outputs/step3_3_optuna\\trial_39\\models\\fold_7\\best_model.keras\n",
      "200/200 - 5s - 25ms/step - accuracy: 0.7715 - loss: 0.7308 - val_accuracy: 0.8902 - val_loss: 0.3847 - learning_rate: 2.2957e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.38466\n",
      "200/200 - 5s - 26ms/step - accuracy: 0.7843 - loss: 0.7029 - val_accuracy: 0.8866 - val_loss: 0.3957 - learning_rate: 2.2957e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 0.38466 to 0.35056, saving model to outputs/step3_3_optuna\\trial_39\\models\\fold_7\\best_model.keras\n",
      "200/200 - 3s - 17ms/step - accuracy: 0.7913 - loss: 0.6760 - val_accuracy: 0.8991 - val_loss: 0.3506 - learning_rate: 2.2957e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss improved from 0.35056 to 0.34881, saving model to outputs/step3_3_optuna\\trial_39\\models\\fold_7\\best_model.keras\n",
      "200/200 - 6s - 29ms/step - accuracy: 0.7990 - loss: 0.6639 - val_accuracy: 0.9047 - val_loss: 0.3488 - learning_rate: 2.2957e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss improved from 0.34881 to 0.33752, saving model to outputs/step3_3_optuna\\trial_39\\models\\fold_7\\best_model.keras\n",
      "200/200 - 4s - 22ms/step - accuracy: 0.8006 - loss: 0.6515 - val_accuracy: 0.9021 - val_loss: 0.3375 - learning_rate: 2.2957e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss improved from 0.33752 to 0.32799, saving model to outputs/step3_3_optuna\\trial_39\\models\\fold_7\\best_model.keras\n",
      "200/200 - 3s - 17ms/step - accuracy: 0.8128 - loss: 0.6282 - val_accuracy: 0.9035 - val_loss: 0.3280 - learning_rate: 2.2957e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss improved from 0.32799 to 0.31443, saving model to outputs/step3_3_optuna\\trial_39\\models\\fold_7\\best_model.keras\n",
      "200/200 - 5s - 26ms/step - accuracy: 0.8171 - loss: 0.6027 - val_accuracy: 0.9102 - val_loss: 0.3144 - learning_rate: 2.2957e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss improved from 0.31443 to 0.29350, saving model to outputs/step3_3_optuna\\trial_39\\models\\fold_7\\best_model.keras\n",
      "200/200 - 3s - 17ms/step - accuracy: 0.8184 - loss: 0.6008 - val_accuracy: 0.9162 - val_loss: 0.2935 - learning_rate: 2.2957e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.29350\n",
      "200/200 - 3s - 17ms/step - accuracy: 0.8282 - loss: 0.5800 - val_accuracy: 0.9129 - val_loss: 0.3031 - learning_rate: 2.2957e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss improved from 0.29350 to 0.28278, saving model to outputs/step3_3_optuna\\trial_39\\models\\fold_7\\best_model.keras\n",
      "200/200 - 5s - 26ms/step - accuracy: 0.8316 - loss: 0.5659 - val_accuracy: 0.9202 - val_loss: 0.2828 - learning_rate: 2.2957e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.28278\n",
      "200/200 - 5s - 26ms/step - accuracy: 0.8394 - loss: 0.5521 - val_accuracy: 0.9186 - val_loss: 0.2909 - learning_rate: 2.2957e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss improved from 0.28278 to 0.26592, saving model to outputs/step3_3_optuna\\trial_39\\models\\fold_7\\best_model.keras\n",
      "200/200 - 3s - 17ms/step - accuracy: 0.8403 - loss: 0.5462 - val_accuracy: 0.9298 - val_loss: 0.2659 - learning_rate: 2.2957e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss improved from 0.26592 to 0.25340, saving model to outputs/step3_3_optuna\\trial_39\\models\\fold_7\\best_model.keras\n",
      "200/200 - 5s - 26ms/step - accuracy: 0.8443 - loss: 0.5348 - val_accuracy: 0.9323 - val_loss: 0.2534 - learning_rate: 2.2957e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss improved from 0.25340 to 0.24871, saving model to outputs/step3_3_optuna\\trial_39\\models\\fold_7\\best_model.keras\n",
      "200/200 - 6s - 29ms/step - accuracy: 0.8505 - loss: 0.5214 - val_accuracy: 0.9321 - val_loss: 0.2487 - learning_rate: 2.2957e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss improved from 0.24871 to 0.24696, saving model to outputs/step3_3_optuna\\trial_39\\models\\fold_7\\best_model.keras\n",
      "200/200 - 3s - 17ms/step - accuracy: 0.8491 - loss: 0.5148 - val_accuracy: 0.9339 - val_loss: 0.2470 - learning_rate: 2.2957e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss improved from 0.24696 to 0.23487, saving model to outputs/step3_3_optuna\\trial_39\\models\\fold_7\\best_model.keras\n",
      "200/200 - 5s - 25ms/step - accuracy: 0.8564 - loss: 0.5019 - val_accuracy: 0.9366 - val_loss: 0.2349 - learning_rate: 2.2957e-04\n",
      "Restoring model weights from the end of the best epoch: 30.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 20:03:07,082] Trial 39 finished with value: 0.5272027398302769 and parameters: {'WINDOW_SIZE': 15, 'STRIDE': 3, 'gru1_units': 128, 'gru2_units': 96, 'dense_units': 32, 'dropout_rate': 0.38008014618933206, 'l2': 0.00011399405558270368, 'cf_filters': 48, 'cf_kernel': 7, 'cf_pool': 3, 'cf_drop': 0.2, 'cf_l2': 1.8236880226343663e-05, 'learning_rate': 0.0002295707767739898, 'batch_size': 64}. Best is trial 22 with value: 0.7596586896056158.\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.5272 ---\n",
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n",
      "학습 데이터가 11356개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 1.26760, saving model to outputs/step3_3_optuna\\trial_40\\models\\fold_7\\best_model.keras\n",
      "178/178 - 7s - 38ms/step - accuracy: 0.3590 - loss: 1.9638 - val_accuracy: 0.6111 - val_loss: 1.2676 - learning_rate: 6.2198e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 1.26760 to 0.75760, saving model to outputs/step3_3_optuna\\trial_40\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 27ms/step - accuracy: 0.5857 - loss: 1.2685 - val_accuracy: 0.7634 - val_loss: 0.7576 - learning_rate: 6.2198e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 0.75760 to 0.53592, saving model to outputs/step3_3_optuna\\trial_40\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 29ms/step - accuracy: 0.7077 - loss: 0.9066 - val_accuracy: 0.8265 - val_loss: 0.5359 - learning_rate: 6.2198e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 0.53592 to 0.39104, saving model to outputs/step3_3_optuna\\trial_40\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 29ms/step - accuracy: 0.7771 - loss: 0.7050 - val_accuracy: 0.8737 - val_loss: 0.3910 - learning_rate: 6.2198e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 0.39104 to 0.30567, saving model to outputs/step3_3_optuna\\trial_40\\models\\fold_7\\best_model.keras\n",
      "178/178 - 6s - 32ms/step - accuracy: 0.8195 - loss: 0.5795 - val_accuracy: 0.9038 - val_loss: 0.3057 - learning_rate: 6.2198e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 0.30567 to 0.25474, saving model to outputs/step3_3_optuna\\trial_40\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.8499 - loss: 0.4916 - val_accuracy: 0.9226 - val_loss: 0.2547 - learning_rate: 6.2198e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.25474 to 0.19558, saving model to outputs/step3_3_optuna\\trial_40\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.8708 - loss: 0.4377 - val_accuracy: 0.9389 - val_loss: 0.1956 - learning_rate: 6.2198e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.19558\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.8987 - loss: 0.3690 - val_accuracy: 0.9388 - val_loss: 0.1974 - learning_rate: 6.2198e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.19558 to 0.16139, saving model to outputs/step3_3_optuna\\trial_40\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.9083 - loss: 0.3259 - val_accuracy: 0.9530 - val_loss: 0.1614 - learning_rate: 6.2198e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 0.16139 to 0.14246, saving model to outputs/step3_3_optuna\\trial_40\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 29ms/step - accuracy: 0.9086 - loss: 0.3172 - val_accuracy: 0.9591 - val_loss: 0.1425 - learning_rate: 6.2198e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.14246\n",
      "178/178 - 6s - 32ms/step - accuracy: 0.9221 - loss: 0.2832 - val_accuracy: 0.9555 - val_loss: 0.1597 - learning_rate: 6.2198e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.14246\n",
      "178/178 - 5s - 26ms/step - accuracy: 0.9269 - loss: 0.2675 - val_accuracy: 0.9611 - val_loss: 0.1448 - learning_rate: 6.2198e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 0.14246 to 0.12322, saving model to outputs/step3_3_optuna\\trial_40\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.9339 - loss: 0.2457 - val_accuracy: 0.9674 - val_loss: 0.1232 - learning_rate: 6.2198e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 0.12322 to 0.10693, saving model to outputs/step3_3_optuna\\trial_40\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 29ms/step - accuracy: 0.9374 - loss: 0.2404 - val_accuracy: 0.9768 - val_loss: 0.1069 - learning_rate: 6.2198e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.10693\n",
      "178/178 - 5s - 28ms/step - accuracy: 0.9412 - loss: 0.2261 - val_accuracy: 0.9747 - val_loss: 0.1087 - learning_rate: 6.2198e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss improved from 0.10693 to 0.08902, saving model to outputs/step3_3_optuna\\trial_40\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.9465 - loss: 0.2032 - val_accuracy: 0.9807 - val_loss: 0.0890 - learning_rate: 6.2198e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.08902\n",
      "178/178 - 5s - 29ms/step - accuracy: 0.9493 - loss: 0.2000 - val_accuracy: 0.9796 - val_loss: 0.0959 - learning_rate: 6.2198e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.08902\n",
      "178/178 - 6s - 32ms/step - accuracy: 0.9537 - loss: 0.1892 - val_accuracy: 0.9764 - val_loss: 0.1006 - learning_rate: 6.2198e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss improved from 0.08902 to 0.07917, saving model to outputs/step3_3_optuna\\trial_40\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.9560 - loss: 0.1890 - val_accuracy: 0.9835 - val_loss: 0.0792 - learning_rate: 6.2198e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.07917\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.9586 - loss: 0.1723 - val_accuracy: 0.9813 - val_loss: 0.0908 - learning_rate: 6.2198e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.07917\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.9589 - loss: 0.1685 - val_accuracy: 0.9850 - val_loss: 0.0801 - learning_rate: 6.2198e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.07917\n",
      "178/178 - 4s - 20ms/step - accuracy: 0.9631 - loss: 0.1619 - val_accuracy: 0.9847 - val_loss: 0.0851 - learning_rate: 6.2198e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.07917\n",
      "178/178 - 5s - 29ms/step - accuracy: 0.9615 - loss: 0.1578 - val_accuracy: 0.9832 - val_loss: 0.0875 - learning_rate: 6.2198e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss improved from 0.07917 to 0.07299, saving model to outputs/step3_3_optuna\\trial_40\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 29ms/step - accuracy: 0.9654 - loss: 0.1491 - val_accuracy: 0.9871 - val_loss: 0.0730 - learning_rate: 6.2198e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss improved from 0.07299 to 0.06984, saving model to outputs/step3_3_optuna\\trial_40\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.9651 - loss: 0.1469 - val_accuracy: 0.9876 - val_loss: 0.0698 - learning_rate: 6.2198e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss improved from 0.06984 to 0.06827, saving model to outputs/step3_3_optuna\\trial_40\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.9682 - loss: 0.1411 - val_accuracy: 0.9877 - val_loss: 0.0683 - learning_rate: 6.2198e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.06827\n",
      "178/178 - 4s - 24ms/step - accuracy: 0.9681 - loss: 0.1394 - val_accuracy: 0.9805 - val_loss: 0.0952 - learning_rate: 6.2198e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss improved from 0.06827 to 0.06265, saving model to outputs/step3_3_optuna\\trial_40\\models\\fold_7\\best_model.keras\n",
      "178/178 - 5s - 26ms/step - accuracy: 0.9670 - loss: 0.1430 - val_accuracy: 0.9918 - val_loss: 0.0627 - learning_rate: 6.2198e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss improved from 0.06265 to 0.05571, saving model to outputs/step3_3_optuna\\trial_40\\models\\fold_7\\best_model.keras\n",
      "178/178 - 4s - 21ms/step - accuracy: 0.9733 - loss: 0.1245 - val_accuracy: 0.9934 - val_loss: 0.0557 - learning_rate: 6.2198e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.05571\n",
      "178/178 - 5s - 29ms/step - accuracy: 0.9699 - loss: 0.1349 - val_accuracy: 0.9787 - val_loss: 0.1064 - learning_rate: 6.2198e-04\n",
      "Restoring model weights from the end of the best epoch: 29.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 20:05:29,360] Trial 40 finished with value: 0.598502240618093 and parameters: {'WINDOW_SIZE': 30, 'STRIDE': 3, 'gru1_units': 160, 'gru2_units': 0, 'dense_units': 64, 'dropout_rate': 0.5124968207944202, 'l2': 7.5901090588087e-05, 'cf_filters': 64, 'cf_kernel': 5, 'cf_pool': 3, 'cf_drop': 0.1, 'cf_l2': 9.489405082034799e-05, 'learning_rate': 0.0006219760079491541, 'batch_size': 64}. Best is trial 22 with value: 0.7596586896056158.\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.5985 ---\n",
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n",
      "학습 데이터가 9972개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 1.53134, saving model to outputs/step3_3_optuna\\trial_41\\models\\fold_7\\best_model.keras\n",
      "156/156 - 6s - 39ms/step - accuracy: 0.2945 - loss: 2.1328 - val_accuracy: 0.5497 - val_loss: 1.5313 - learning_rate: 4.1838e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 1.53134 to 0.89086, saving model to outputs/step3_3_optuna\\trial_41\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 31ms/step - accuracy: 0.4857 - loss: 1.5343 - val_accuracy: 0.7409 - val_loss: 0.8909 - learning_rate: 4.1838e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 0.89086 to 0.62099, saving model to outputs/step3_3_optuna\\trial_41\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.6241 - loss: 1.1432 - val_accuracy: 0.8222 - val_loss: 0.6210 - learning_rate: 4.1838e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 0.62099 to 0.47261, saving model to outputs/step3_3_optuna\\trial_41\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.6982 - loss: 0.9324 - val_accuracy: 0.8589 - val_loss: 0.4726 - learning_rate: 4.1838e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 0.47261 to 0.39621, saving model to outputs/step3_3_optuna\\trial_41\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.7424 - loss: 0.8087 - val_accuracy: 0.8817 - val_loss: 0.3962 - learning_rate: 4.1838e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 0.39621 to 0.33249, saving model to outputs/step3_3_optuna\\trial_41\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.7662 - loss: 0.7272 - val_accuracy: 0.9020 - val_loss: 0.3325 - learning_rate: 4.1838e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.33249 to 0.31837, saving model to outputs/step3_3_optuna\\trial_41\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.7867 - loss: 0.6498 - val_accuracy: 0.9020 - val_loss: 0.3184 - learning_rate: 4.1838e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 0.31837 to 0.27260, saving model to outputs/step3_3_optuna\\trial_41\\models\\fold_7\\best_model.keras\n",
      "156/156 - 3s - 20ms/step - accuracy: 0.8045 - loss: 0.6143 - val_accuracy: 0.9209 - val_loss: 0.2726 - learning_rate: 4.1838e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.27260 to 0.24845, saving model to outputs/step3_3_optuna\\trial_41\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.8234 - loss: 0.5537 - val_accuracy: 0.9206 - val_loss: 0.2485 - learning_rate: 4.1838e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 0.24845 to 0.21271, saving model to outputs/step3_3_optuna\\trial_41\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.8385 - loss: 0.5140 - val_accuracy: 0.9397 - val_loss: 0.2127 - learning_rate: 4.1838e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 0.21271 to 0.19683, saving model to outputs/step3_3_optuna\\trial_41\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.8450 - loss: 0.4824 - val_accuracy: 0.9397 - val_loss: 0.1968 - learning_rate: 4.1838e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 0.19683 to 0.17378, saving model to outputs/step3_3_optuna\\trial_41\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.8550 - loss: 0.4506 - val_accuracy: 0.9467 - val_loss: 0.1738 - learning_rate: 4.1838e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 0.17378 to 0.16978, saving model to outputs/step3_3_optuna\\trial_41\\models\\fold_7\\best_model.keras\n",
      "156/156 - 3s - 20ms/step - accuracy: 0.8663 - loss: 0.4341 - val_accuracy: 0.9490 - val_loss: 0.1698 - learning_rate: 4.1838e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 0.16978 to 0.14674, saving model to outputs/step3_3_optuna\\trial_41\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.8722 - loss: 0.3995 - val_accuracy: 0.9576 - val_loss: 0.1467 - learning_rate: 4.1838e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss improved from 0.14674 to 0.13149, saving model to outputs/step3_3_optuna\\trial_41\\models\\fold_7\\best_model.keras\n",
      "156/156 - 3s - 20ms/step - accuracy: 0.8811 - loss: 0.3781 - val_accuracy: 0.9637 - val_loss: 0.1315 - learning_rate: 4.1838e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.13149\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.8866 - loss: 0.3605 - val_accuracy: 0.9569 - val_loss: 0.1399 - learning_rate: 4.1838e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 0.13149 to 0.11653, saving model to outputs/step3_3_optuna\\trial_41\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.8943 - loss: 0.3434 - val_accuracy: 0.9657 - val_loss: 0.1165 - learning_rate: 4.1838e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.11653\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.8955 - loss: 0.3313 - val_accuracy: 0.9630 - val_loss: 0.1170 - learning_rate: 4.1838e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss improved from 0.11653 to 0.10581, saving model to outputs/step3_3_optuna\\trial_41\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.8982 - loss: 0.3171 - val_accuracy: 0.9678 - val_loss: 0.1058 - learning_rate: 4.1838e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss improved from 0.10581 to 0.09532, saving model to outputs/step3_3_optuna\\trial_41\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9067 - loss: 0.2945 - val_accuracy: 0.9700 - val_loss: 0.0953 - learning_rate: 4.1838e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss improved from 0.09532 to 0.09293, saving model to outputs/step3_3_optuna\\trial_41\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9142 - loss: 0.2764 - val_accuracy: 0.9727 - val_loss: 0.0929 - learning_rate: 4.1838e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.09293\n",
      "156/156 - 3s - 20ms/step - accuracy: 0.9157 - loss: 0.2829 - val_accuracy: 0.9664 - val_loss: 0.1155 - learning_rate: 4.1838e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss improved from 0.09293 to 0.08764, saving model to outputs/step3_3_optuna\\trial_41\\models\\fold_7\\best_model.keras\n",
      "156/156 - 3s - 20ms/step - accuracy: 0.9199 - loss: 0.2694 - val_accuracy: 0.9743 - val_loss: 0.0876 - learning_rate: 4.1838e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss improved from 0.08764 to 0.07951, saving model to outputs/step3_3_optuna\\trial_41\\models\\fold_7\\best_model.keras\n",
      "156/156 - 3s - 21ms/step - accuracy: 0.9229 - loss: 0.2505 - val_accuracy: 0.9780 - val_loss: 0.0795 - learning_rate: 4.1838e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.07951\n",
      "156/156 - 3s - 21ms/step - accuracy: 0.9289 - loss: 0.2413 - val_accuracy: 0.9676 - val_loss: 0.0970 - learning_rate: 4.1838e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.07951\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9293 - loss: 0.2374 - val_accuracy: 0.9775 - val_loss: 0.0800 - learning_rate: 4.1838e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.07951\n",
      "156/156 - 3s - 20ms/step - accuracy: 0.9295 - loss: 0.2412 - val_accuracy: 0.9746 - val_loss: 0.0797 - learning_rate: 4.1838e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss improved from 0.07951 to 0.07406, saving model to outputs/step3_3_optuna\\trial_41\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9355 - loss: 0.2241 - val_accuracy: 0.9760 - val_loss: 0.0741 - learning_rate: 4.1838e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss improved from 0.07406 to 0.06543, saving model to outputs/step3_3_optuna\\trial_41\\models\\fold_7\\best_model.keras\n",
      "156/156 - 5s - 33ms/step - accuracy: 0.9387 - loss: 0.2086 - val_accuracy: 0.9807 - val_loss: 0.0654 - learning_rate: 4.1838e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss improved from 0.06543 to 0.06149, saving model to outputs/step3_3_optuna\\trial_41\\models\\fold_7\\best_model.keras\n",
      "156/156 - 3s - 21ms/step - accuracy: 0.9364 - loss: 0.2226 - val_accuracy: 0.9826 - val_loss: 0.0615 - learning_rate: 4.1838e-04\n",
      "Restoring model weights from the end of the best epoch: 30.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 20:07:54,125] Trial 41 finished with value: 0.5805801303163338 and parameters: {'WINDOW_SIZE': 45, 'STRIDE': 3, 'gru1_units': 96, 'gru2_units': 0, 'dense_units': 32, 'dropout_rate': 0.4056950233139288, 'l2': 1.2175458647607521e-05, 'cf_filters': 48, 'cf_kernel': 3, 'cf_pool': 3, 'cf_drop': 0.1, 'cf_l2': 4.195668289601958e-05, 'learning_rate': 0.00041838198837947994, 'batch_size': 64}. Best is trial 22 with value: 0.7596586896056158.\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.5806 ---\n",
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n",
      "학습 데이터가 9972개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 1.80377, saving model to outputs/step3_3_optuna\\trial_42\\models\\fold_7\\best_model.keras\n",
      "78/78 - 6s - 82ms/step - accuracy: 0.2437 - loss: 2.2527 - val_accuracy: 0.3890 - val_loss: 1.8038 - learning_rate: 5.0379e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 1.80377 to 1.26614, saving model to outputs/step3_3_optuna\\trial_42\\models\\fold_7\\best_model.keras\n",
      "78/78 - 5s - 61ms/step - accuracy: 0.3971 - loss: 1.8270 - val_accuracy: 0.6313 - val_loss: 1.2661 - learning_rate: 5.0379e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 1.26614 to 0.88523, saving model to outputs/step3_3_optuna\\trial_42\\models\\fold_7\\best_model.keras\n",
      "78/78 - 5s - 66ms/step - accuracy: 0.5290 - loss: 1.4450 - val_accuracy: 0.7597 - val_loss: 0.8852 - learning_rate: 5.0379e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 0.88523 to 0.66082, saving model to outputs/step3_3_optuna\\trial_42\\models\\fold_7\\best_model.keras\n",
      "78/78 - 5s - 66ms/step - accuracy: 0.6184 - loss: 1.1884 - val_accuracy: 0.8394 - val_loss: 0.6608 - learning_rate: 5.0379e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 0.66082 to 0.51238, saving model to outputs/step3_3_optuna\\trial_42\\models\\fold_7\\best_model.keras\n",
      "78/78 - 5s - 66ms/step - accuracy: 0.6837 - loss: 0.9837 - val_accuracy: 0.8597 - val_loss: 0.5124 - learning_rate: 5.0379e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 0.51238 to 0.43542, saving model to outputs/step3_3_optuna\\trial_42\\models\\fold_7\\best_model.keras\n",
      "78/78 - 5s - 66ms/step - accuracy: 0.7261 - loss: 0.8423 - val_accuracy: 0.8734 - val_loss: 0.4354 - learning_rate: 5.0379e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.43542 to 0.36133, saving model to outputs/step3_3_optuna\\trial_42\\models\\fold_7\\best_model.keras\n",
      "78/78 - 3s - 44ms/step - accuracy: 0.7563 - loss: 0.7643 - val_accuracy: 0.8942 - val_loss: 0.3613 - learning_rate: 5.0379e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 0.36133 to 0.32125, saving model to outputs/step3_3_optuna\\trial_42\\models\\fold_7\\best_model.keras\n",
      "78/78 - 5s - 66ms/step - accuracy: 0.7876 - loss: 0.6795 - val_accuracy: 0.9085 - val_loss: 0.3213 - learning_rate: 5.0379e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.32125 to 0.26733, saving model to outputs/step3_3_optuna\\trial_42\\models\\fold_7\\best_model.keras\n",
      "78/78 - 5s - 66ms/step - accuracy: 0.8185 - loss: 0.5966 - val_accuracy: 0.9282 - val_loss: 0.2673 - learning_rate: 5.0379e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 0.26733 to 0.24641, saving model to outputs/step3_3_optuna\\trial_42\\models\\fold_7\\best_model.keras\n",
      "78/78 - 5s - 66ms/step - accuracy: 0.8223 - loss: 0.5707 - val_accuracy: 0.9388 - val_loss: 0.2464 - learning_rate: 5.0379e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 0.24641 to 0.22686, saving model to outputs/step3_3_optuna\\trial_42\\models\\fold_7\\best_model.keras\n",
      "78/78 - 3s - 43ms/step - accuracy: 0.8432 - loss: 0.5154 - val_accuracy: 0.9383 - val_loss: 0.2269 - learning_rate: 5.0379e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 0.22686 to 0.21001, saving model to outputs/step3_3_optuna\\trial_42\\models\\fold_7\\best_model.keras\n",
      "78/78 - 4s - 51ms/step - accuracy: 0.8542 - loss: 0.4937 - val_accuracy: 0.9427 - val_loss: 0.2100 - learning_rate: 5.0379e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 0.21001 to 0.20594, saving model to outputs/step3_3_optuna\\trial_42\\models\\fold_7\\best_model.keras\n",
      "78/78 - 3s - 43ms/step - accuracy: 0.8624 - loss: 0.4601 - val_accuracy: 0.9419 - val_loss: 0.2059 - learning_rate: 5.0379e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 0.20594 to 0.18521, saving model to outputs/step3_3_optuna\\trial_42\\models\\fold_7\\best_model.keras\n",
      "78/78 - 3s - 43ms/step - accuracy: 0.8701 - loss: 0.4405 - val_accuracy: 0.9526 - val_loss: 0.1852 - learning_rate: 5.0379e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss improved from 0.18521 to 0.16398, saving model to outputs/step3_3_optuna\\trial_42\\models\\fold_7\\best_model.keras\n",
      "78/78 - 6s - 74ms/step - accuracy: 0.8716 - loss: 0.4213 - val_accuracy: 0.9582 - val_loss: 0.1640 - learning_rate: 5.0379e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss improved from 0.16398 to 0.16344, saving model to outputs/step3_3_optuna\\trial_42\\models\\fold_7\\best_model.keras\n",
      "78/78 - 5s - 58ms/step - accuracy: 0.8881 - loss: 0.3966 - val_accuracy: 0.9586 - val_loss: 0.1634 - learning_rate: 5.0379e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 0.16344 to 0.14956, saving model to outputs/step3_3_optuna\\trial_42\\models\\fold_7\\best_model.keras\n",
      "78/78 - 3s - 43ms/step - accuracy: 0.9018 - loss: 0.3617 - val_accuracy: 0.9610 - val_loss: 0.1496 - learning_rate: 5.0379e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss improved from 0.14956 to 0.13834, saving model to outputs/step3_3_optuna\\trial_42\\models\\fold_7\\best_model.keras\n",
      "78/78 - 5s - 66ms/step - accuracy: 0.9067 - loss: 0.3431 - val_accuracy: 0.9669 - val_loss: 0.1383 - learning_rate: 5.0379e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.13834\n",
      "78/78 - 3s - 43ms/step - accuracy: 0.9014 - loss: 0.3557 - val_accuracy: 0.9559 - val_loss: 0.1587 - learning_rate: 5.0379e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss improved from 0.13834 to 0.11455, saving model to outputs/step3_3_optuna\\trial_42\\models\\fold_7\\best_model.keras\n",
      "78/78 - 5s - 67ms/step - accuracy: 0.9183 - loss: 0.3105 - val_accuracy: 0.9739 - val_loss: 0.1145 - learning_rate: 5.0379e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.11455\n",
      "78/78 - 5s - 65ms/step - accuracy: 0.9180 - loss: 0.3053 - val_accuracy: 0.9717 - val_loss: 0.1153 - learning_rate: 5.0379e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss improved from 0.11455 to 0.11418, saving model to outputs/step3_3_optuna\\trial_42\\models\\fold_7\\best_model.keras\n",
      "78/78 - 3s - 43ms/step - accuracy: 0.9243 - loss: 0.2827 - val_accuracy: 0.9732 - val_loss: 0.1142 - learning_rate: 5.0379e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss improved from 0.11418 to 0.11075, saving model to outputs/step3_3_optuna\\trial_42\\models\\fold_7\\best_model.keras\n",
      "78/78 - 6s - 74ms/step - accuracy: 0.9179 - loss: 0.3041 - val_accuracy: 0.9732 - val_loss: 0.1107 - learning_rate: 5.0379e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss improved from 0.11075 to 0.10514, saving model to outputs/step3_3_optuna\\trial_42\\models\\fold_7\\best_model.keras\n",
      "78/78 - 5s - 58ms/step - accuracy: 0.9303 - loss: 0.2793 - val_accuracy: 0.9760 - val_loss: 0.1051 - learning_rate: 5.0379e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.10514\n",
      "78/78 - 4s - 50ms/step - accuracy: 0.9312 - loss: 0.2715 - val_accuracy: 0.9743 - val_loss: 0.1074 - learning_rate: 5.0379e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss improved from 0.10514 to 0.09557, saving model to outputs/step3_3_optuna\\trial_42\\models\\fold_7\\best_model.keras\n",
      "78/78 - 3s - 44ms/step - accuracy: 0.9335 - loss: 0.2513 - val_accuracy: 0.9772 - val_loss: 0.0956 - learning_rate: 5.0379e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss improved from 0.09557 to 0.09243, saving model to outputs/step3_3_optuna\\trial_42\\models\\fold_7\\best_model.keras\n",
      "78/78 - 3s - 44ms/step - accuracy: 0.9387 - loss: 0.2447 - val_accuracy: 0.9775 - val_loss: 0.0924 - learning_rate: 5.0379e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss improved from 0.09243 to 0.08606, saving model to outputs/step3_3_optuna\\trial_42\\models\\fold_7\\best_model.keras\n",
      "78/78 - 3s - 43ms/step - accuracy: 0.9416 - loss: 0.2410 - val_accuracy: 0.9830 - val_loss: 0.0861 - learning_rate: 5.0379e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.08606\n",
      "78/78 - 3s - 42ms/step - accuracy: 0.9439 - loss: 0.2310 - val_accuracy: 0.9816 - val_loss: 0.0900 - learning_rate: 5.0379e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss improved from 0.08606 to 0.08242, saving model to outputs/step3_3_optuna\\trial_42\\models\\fold_7\\best_model.keras\n",
      "78/78 - 3s - 42ms/step - accuracy: 0.9420 - loss: 0.2254 - val_accuracy: 0.9845 - val_loss: 0.0824 - learning_rate: 5.0379e-04\n",
      "Restoring model weights from the end of the best epoch: 30.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 20:10:12,989] Trial 42 finished with value: 0.7232179449654358 and parameters: {'WINDOW_SIZE': 45, 'STRIDE': 3, 'gru1_units': 128, 'gru2_units': 0, 'dense_units': 32, 'dropout_rate': 0.43866237832286215, 'l2': 0.0002045540866865109, 'cf_filters': 64, 'cf_kernel': 3, 'cf_pool': 3, 'cf_drop': 0.1, 'cf_l2': 3.090920394377462e-05, 'learning_rate': 0.0005037912641365128, 'batch_size': 128}. Best is trial 22 with value: 0.7596586896056158.\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.7232 ---\n",
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n",
      "학습 데이터가 9972개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 1.78378, saving model to outputs/step3_3_optuna\\trial_43\\models\\fold_7\\best_model.keras\n",
      "78/78 - 7s - 92ms/step - accuracy: 0.2512 - loss: 2.2475 - val_accuracy: 0.4055 - val_loss: 1.7838 - learning_rate: 5.7459e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 1.78378 to 1.19248, saving model to outputs/step3_3_optuna\\trial_43\\models\\fold_7\\best_model.keras\n",
      "78/78 - 3s - 42ms/step - accuracy: 0.4250 - loss: 1.7768 - val_accuracy: 0.6617 - val_loss: 1.1925 - learning_rate: 5.7459e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 1.19248 to 0.81580, saving model to outputs/step3_3_optuna\\trial_43\\models\\fold_7\\best_model.keras\n",
      "78/78 - 3s - 43ms/step - accuracy: 0.5506 - loss: 1.3831 - val_accuracy: 0.7878 - val_loss: 0.8158 - learning_rate: 5.7459e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 0.81580 to 0.61514, saving model to outputs/step3_3_optuna\\trial_43\\models\\fold_7\\best_model.keras\n",
      "78/78 - 5s - 66ms/step - accuracy: 0.6401 - loss: 1.1366 - val_accuracy: 0.8493 - val_loss: 0.6151 - learning_rate: 5.7459e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 0.61514 to 0.49073, saving model to outputs/step3_3_optuna\\trial_43\\models\\fold_7\\best_model.keras\n",
      "78/78 - 3s - 42ms/step - accuracy: 0.6943 - loss: 0.9476 - val_accuracy: 0.8662 - val_loss: 0.4907 - learning_rate: 5.7459e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 0.49073 to 0.43319, saving model to outputs/step3_3_optuna\\trial_43\\models\\fold_7\\best_model.keras\n",
      "78/78 - 5s - 67ms/step - accuracy: 0.7415 - loss: 0.8150 - val_accuracy: 0.8781 - val_loss: 0.4332 - learning_rate: 5.7459e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.43319 to 0.36820, saving model to outputs/step3_3_optuna\\trial_43\\models\\fold_7\\best_model.keras\n",
      "78/78 - 3s - 43ms/step - accuracy: 0.7648 - loss: 0.7535 - val_accuracy: 0.8981 - val_loss: 0.3682 - learning_rate: 5.7459e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 0.36820 to 0.31866, saving model to outputs/step3_3_optuna\\trial_43\\models\\fold_7\\best_model.keras\n",
      "78/78 - 3s - 43ms/step - accuracy: 0.7960 - loss: 0.6746 - val_accuracy: 0.9100 - val_loss: 0.3187 - learning_rate: 5.7459e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.31866 to 0.27283, saving model to outputs/step3_3_optuna\\trial_43\\models\\fold_7\\best_model.keras\n",
      "78/78 - 6s - 74ms/step - accuracy: 0.8223 - loss: 0.5955 - val_accuracy: 0.9213 - val_loss: 0.2728 - learning_rate: 5.7459e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 0.27283 to 0.24753, saving model to outputs/step3_3_optuna\\trial_43\\models\\fold_7\\best_model.keras\n",
      "78/78 - 5s - 59ms/step - accuracy: 0.8300 - loss: 0.5654 - val_accuracy: 0.9397 - val_loss: 0.2475 - learning_rate: 5.7459e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 0.24753 to 0.21391, saving model to outputs/step3_3_optuna\\trial_43\\models\\fold_7\\best_model.keras\n",
      "78/78 - 5s - 66ms/step - accuracy: 0.8489 - loss: 0.5033 - val_accuracy: 0.9490 - val_loss: 0.2139 - learning_rate: 5.7459e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 0.21391 to 0.21038, saving model to outputs/step3_3_optuna\\trial_43\\models\\fold_7\\best_model.keras\n",
      "78/78 - 3s - 43ms/step - accuracy: 0.8603 - loss: 0.4843 - val_accuracy: 0.9482 - val_loss: 0.2104 - learning_rate: 5.7459e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.21038\n",
      "78/78 - 5s - 66ms/step - accuracy: 0.8711 - loss: 0.4503 - val_accuracy: 0.9359 - val_loss: 0.2244 - learning_rate: 5.7459e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 0.21038 to 0.18182, saving model to outputs/step3_3_optuna\\trial_43\\models\\fold_7\\best_model.keras\n",
      "78/78 - 5s - 67ms/step - accuracy: 0.8763 - loss: 0.4331 - val_accuracy: 0.9559 - val_loss: 0.1818 - learning_rate: 5.7459e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss improved from 0.18182 to 0.16419, saving model to outputs/step3_3_optuna\\trial_43\\models\\fold_7\\best_model.keras\n",
      "78/78 - 5s - 66ms/step - accuracy: 0.8839 - loss: 0.4066 - val_accuracy: 0.9625 - val_loss: 0.1642 - learning_rate: 5.7459e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss improved from 0.16419 to 0.15892, saving model to outputs/step3_3_optuna\\trial_43\\models\\fold_7\\best_model.keras\n",
      "78/78 - 5s - 66ms/step - accuracy: 0.8957 - loss: 0.3862 - val_accuracy: 0.9622 - val_loss: 0.1589 - learning_rate: 5.7459e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 0.15892 to 0.15410, saving model to outputs/step3_3_optuna\\trial_43\\models\\fold_7\\best_model.keras\n",
      "78/78 - 3s - 44ms/step - accuracy: 0.9041 - loss: 0.3542 - val_accuracy: 0.9644 - val_loss: 0.1541 - learning_rate: 5.7459e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss improved from 0.15410 to 0.13769, saving model to outputs/step3_3_optuna\\trial_43\\models\\fold_7\\best_model.keras\n",
      "78/78 - 6s - 74ms/step - accuracy: 0.9109 - loss: 0.3401 - val_accuracy: 0.9693 - val_loss: 0.1377 - learning_rate: 5.7459e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.13769\n",
      "78/78 - 3s - 43ms/step - accuracy: 0.9115 - loss: 0.3296 - val_accuracy: 0.9634 - val_loss: 0.1576 - learning_rate: 5.7459e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.13769\n",
      "78/78 - 3s - 43ms/step - accuracy: 0.9243 - loss: 0.3006 - val_accuracy: 0.9644 - val_loss: 0.1478 - learning_rate: 5.7459e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss improved from 0.13769 to 0.12230, saving model to outputs/step3_3_optuna\\trial_43\\models\\fold_7\\best_model.keras\n",
      "78/78 - 3s - 43ms/step - accuracy: 0.9215 - loss: 0.3094 - val_accuracy: 0.9732 - val_loss: 0.1223 - learning_rate: 5.7459e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss improved from 0.12230 to 0.12078, saving model to outputs/step3_3_optuna\\trial_43\\models\\fold_7\\best_model.keras\n",
      "78/78 - 5s - 66ms/step - accuracy: 0.9267 - loss: 0.2818 - val_accuracy: 0.9756 - val_loss: 0.1208 - learning_rate: 5.7459e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.12078\n",
      "78/78 - 6s - 73ms/step - accuracy: 0.9204 - loss: 0.3052 - val_accuracy: 0.9620 - val_loss: 0.1504 - learning_rate: 5.7459e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss improved from 0.12078 to 0.10453, saving model to outputs/step3_3_optuna\\trial_43\\models\\fold_7\\best_model.keras\n",
      "78/78 - 5s - 59ms/step - accuracy: 0.9300 - loss: 0.2776 - val_accuracy: 0.9778 - val_loss: 0.1045 - learning_rate: 5.7459e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.10453\n",
      "78/78 - 5s - 65ms/step - accuracy: 0.9353 - loss: 0.2682 - val_accuracy: 0.9720 - val_loss: 0.1242 - learning_rate: 5.7459e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.10453\n",
      "78/78 - 3s - 43ms/step - accuracy: 0.9344 - loss: 0.2613 - val_accuracy: 0.9744 - val_loss: 0.1126 - learning_rate: 5.7459e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss improved from 0.10453 to 0.10268, saving model to outputs/step3_3_optuna\\trial_43\\models\\fold_7\\best_model.keras\n",
      "78/78 - 5s - 69ms/step - accuracy: 0.9408 - loss: 0.2447 - val_accuracy: 0.9789 - val_loss: 0.1027 - learning_rate: 5.7459e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss improved from 0.10268 to 0.09000, saving model to outputs/step3_3_optuna\\trial_43\\models\\fold_7\\best_model.keras\n",
      "78/78 - 4s - 46ms/step - accuracy: 0.9476 - loss: 0.2308 - val_accuracy: 0.9850 - val_loss: 0.0900 - learning_rate: 5.7459e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.09000\n",
      "78/78 - 3s - 44ms/step - accuracy: 0.9499 - loss: 0.2213 - val_accuracy: 0.9840 - val_loss: 0.0949 - learning_rate: 5.7459e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.09000\n",
      "78/78 - 3s - 44ms/step - accuracy: 0.9491 - loss: 0.2162 - val_accuracy: 0.9814 - val_loss: 0.0956 - learning_rate: 5.7459e-04\n",
      "Restoring model weights from the end of the best epoch: 28.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 20:12:32,854] Trial 43 finished with value: 0.6305686274310994 and parameters: {'WINDOW_SIZE': 45, 'STRIDE': 3, 'gru1_units': 128, 'gru2_units': 0, 'dense_units': 32, 'dropout_rate': 0.4409926126753842, 'l2': 0.0002787030844481602, 'cf_filters': 64, 'cf_kernel': 3, 'cf_pool': 3, 'cf_drop': 0.1, 'cf_l2': 2.874499634661287e-05, 'learning_rate': 0.0005745910944082187, 'batch_size': 128}. Best is trial 22 with value: 0.7596586896056158.\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.6306 ---\n",
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n",
      "학습 데이터가 12739개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 1.49423, saving model to outputs/step3_3_optuna\\trial_44\\models\\fold_7\\best_model.keras\n",
      "100/100 - 5s - 54ms/step - accuracy: 0.2840 - loss: 2.1892 - val_accuracy: 0.5348 - val_loss: 1.4942 - learning_rate: 3.7740e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 1.49423 to 0.99967, saving model to outputs/step3_3_optuna\\trial_44\\models\\fold_7\\best_model.keras\n",
      "100/100 - 2s - 18ms/step - accuracy: 0.4861 - loss: 1.5481 - val_accuracy: 0.6964 - val_loss: 0.9997 - learning_rate: 3.7740e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 0.99967 to 0.74630, saving model to outputs/step3_3_optuna\\trial_44\\models\\fold_7\\best_model.keras\n",
      "100/100 - 3s - 30ms/step - accuracy: 0.6123 - loss: 1.1968 - val_accuracy: 0.7724 - val_loss: 0.7463 - learning_rate: 3.7740e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 0.74630 to 0.60457, saving model to outputs/step3_3_optuna\\trial_44\\models\\fold_7\\best_model.keras\n",
      "100/100 - 3s - 25ms/step - accuracy: 0.6931 - loss: 0.9678 - val_accuracy: 0.8200 - val_loss: 0.6046 - learning_rate: 3.7740e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 0.60457 to 0.53180, saving model to outputs/step3_3_optuna\\trial_44\\models\\fold_7\\best_model.keras\n",
      "100/100 - 2s - 21ms/step - accuracy: 0.7373 - loss: 0.8364 - val_accuracy: 0.8407 - val_loss: 0.5318 - learning_rate: 3.7740e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 0.53180 to 0.47581, saving model to outputs/step3_3_optuna\\trial_44\\models\\fold_7\\best_model.keras\n",
      "100/100 - 2s - 21ms/step - accuracy: 0.7670 - loss: 0.7535 - val_accuracy: 0.8618 - val_loss: 0.4758 - learning_rate: 3.7740e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.47581 to 0.41533, saving model to outputs/step3_3_optuna\\trial_44\\models\\fold_7\\best_model.keras\n",
      "100/100 - 3s - 26ms/step - accuracy: 0.7853 - loss: 0.6811 - val_accuracy: 0.8820 - val_loss: 0.4153 - learning_rate: 3.7740e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 0.41533 to 0.38824, saving model to outputs/step3_3_optuna\\trial_44\\models\\fold_7\\best_model.keras\n",
      "100/100 - 2s - 23ms/step - accuracy: 0.8036 - loss: 0.6222 - val_accuracy: 0.8871 - val_loss: 0.3882 - learning_rate: 3.7740e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.38824 to 0.34479, saving model to outputs/step3_3_optuna\\trial_44\\models\\fold_7\\best_model.keras\n",
      "100/100 - 2s - 22ms/step - accuracy: 0.8191 - loss: 0.5753 - val_accuracy: 0.9023 - val_loss: 0.3448 - learning_rate: 3.7740e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 0.34479 to 0.30308, saving model to outputs/step3_3_optuna\\trial_44\\models\\fold_7\\best_model.keras\n",
      "100/100 - 2s - 23ms/step - accuracy: 0.8378 - loss: 0.5297 - val_accuracy: 0.9183 - val_loss: 0.3031 - learning_rate: 3.7740e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 0.30308 to 0.29117, saving model to outputs/step3_3_optuna\\trial_44\\models\\fold_7\\best_model.keras\n",
      "100/100 - 3s - 30ms/step - accuracy: 0.8493 - loss: 0.5012 - val_accuracy: 0.9193 - val_loss: 0.2912 - learning_rate: 3.7740e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 0.29117 to 0.26985, saving model to outputs/step3_3_optuna\\trial_44\\models\\fold_7\\best_model.keras\n",
      "100/100 - 2s - 22ms/step - accuracy: 0.8549 - loss: 0.4809 - val_accuracy: 0.9219 - val_loss: 0.2698 - learning_rate: 3.7740e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 0.26985 to 0.26464, saving model to outputs/step3_3_optuna\\trial_44\\models\\fold_7\\best_model.keras\n",
      "100/100 - 3s - 29ms/step - accuracy: 0.8596 - loss: 0.4623 - val_accuracy: 0.9257 - val_loss: 0.2646 - learning_rate: 3.7740e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 0.26464 to 0.24075, saving model to outputs/step3_3_optuna\\trial_44\\models\\fold_7\\best_model.keras\n",
      "100/100 - 2s - 23ms/step - accuracy: 0.8684 - loss: 0.4460 - val_accuracy: 0.9350 - val_loss: 0.2407 - learning_rate: 3.7740e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss improved from 0.24075 to 0.23309, saving model to outputs/step3_3_optuna\\trial_44\\models\\fold_7\\best_model.keras\n",
      "100/100 - 3s - 26ms/step - accuracy: 0.8766 - loss: 0.4180 - val_accuracy: 0.9353 - val_loss: 0.2331 - learning_rate: 3.7740e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss improved from 0.23309 to 0.21455, saving model to outputs/step3_3_optuna\\trial_44\\models\\fold_7\\best_model.keras\n",
      "100/100 - 3s - 26ms/step - accuracy: 0.8792 - loss: 0.4054 - val_accuracy: 0.9434 - val_loss: 0.2146 - learning_rate: 3.7740e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 0.21455 to 0.20047, saving model to outputs/step3_3_optuna\\trial_44\\models\\fold_7\\best_model.keras\n",
      "100/100 - 2s - 18ms/step - accuracy: 0.8906 - loss: 0.3816 - val_accuracy: 0.9450 - val_loss: 0.2005 - learning_rate: 3.7740e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss improved from 0.20047 to 0.19125, saving model to outputs/step3_3_optuna\\trial_44\\models\\fold_7\\best_model.keras\n",
      "100/100 - 3s - 29ms/step - accuracy: 0.8950 - loss: 0.3681 - val_accuracy: 0.9506 - val_loss: 0.1912 - learning_rate: 3.7740e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss improved from 0.19125 to 0.18527, saving model to outputs/step3_3_optuna\\trial_44\\models\\fold_7\\best_model.keras\n",
      "100/100 - 3s - 26ms/step - accuracy: 0.8950 - loss: 0.3539 - val_accuracy: 0.9528 - val_loss: 0.1853 - learning_rate: 3.7740e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.18527\n",
      "100/100 - 3s - 25ms/step - accuracy: 0.8966 - loss: 0.3570 - val_accuracy: 0.9433 - val_loss: 0.1985 - learning_rate: 3.7740e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss improved from 0.18527 to 0.17960, saving model to outputs/step3_3_optuna\\trial_44\\models\\fold_7\\best_model.keras\n",
      "100/100 - 3s - 27ms/step - accuracy: 0.9006 - loss: 0.3329 - val_accuracy: 0.9528 - val_loss: 0.1796 - learning_rate: 3.7740e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss improved from 0.17960 to 0.17437, saving model to outputs/step3_3_optuna\\trial_44\\models\\fold_7\\best_model.keras\n",
      "100/100 - 3s - 26ms/step - accuracy: 0.9103 - loss: 0.3143 - val_accuracy: 0.9558 - val_loss: 0.1744 - learning_rate: 3.7740e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss improved from 0.17437 to 0.15737, saving model to outputs/step3_3_optuna\\trial_44\\models\\fold_7\\best_model.keras\n",
      "100/100 - 3s - 26ms/step - accuracy: 0.9111 - loss: 0.3171 - val_accuracy: 0.9626 - val_loss: 0.1574 - learning_rate: 3.7740e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.15737\n",
      "100/100 - 3s - 26ms/step - accuracy: 0.9119 - loss: 0.3095 - val_accuracy: 0.9602 - val_loss: 0.1601 - learning_rate: 3.7740e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.15737\n",
      "100/100 - 2s - 18ms/step - accuracy: 0.9139 - loss: 0.2994 - val_accuracy: 0.9617 - val_loss: 0.1585 - learning_rate: 3.7740e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss improved from 0.15737 to 0.15262, saving model to outputs/step3_3_optuna\\trial_44\\models\\fold_7\\best_model.keras\n",
      "100/100 - 3s - 30ms/step - accuracy: 0.9204 - loss: 0.2840 - val_accuracy: 0.9630 - val_loss: 0.1526 - learning_rate: 3.7740e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss improved from 0.15262 to 0.14653, saving model to outputs/step3_3_optuna\\trial_44\\models\\fold_7\\best_model.keras\n",
      "100/100 - 3s - 26ms/step - accuracy: 0.9201 - loss: 0.2846 - val_accuracy: 0.9652 - val_loss: 0.1465 - learning_rate: 3.7740e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.14653\n",
      "100/100 - 3s - 25ms/step - accuracy: 0.9231 - loss: 0.2796 - val_accuracy: 0.9645 - val_loss: 0.1475 - learning_rate: 3.7740e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss improved from 0.14653 to 0.13365, saving model to outputs/step3_3_optuna\\trial_44\\models\\fold_7\\best_model.keras\n",
      "100/100 - 2s - 23ms/step - accuracy: 0.9213 - loss: 0.2714 - val_accuracy: 0.9700 - val_loss: 0.1337 - learning_rate: 3.7740e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss improved from 0.13365 to 0.13110, saving model to outputs/step3_3_optuna\\trial_44\\models\\fold_7\\best_model.keras\n",
      "100/100 - 3s - 26ms/step - accuracy: 0.9265 - loss: 0.2717 - val_accuracy: 0.9712 - val_loss: 0.1311 - learning_rate: 3.7740e-04\n",
      "Restoring model weights from the end of the best epoch: 30.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 20:13:55,187] Trial 44 finished with value: 0.57295206864187 and parameters: {'WINDOW_SIZE': 15, 'STRIDE': 3, 'gru1_units': 128, 'gru2_units': 0, 'dense_units': 96, 'dropout_rate': 0.46979506845881347, 'l2': 0.0001807687116492271, 'cf_filters': 64, 'cf_kernel': 3, 'cf_pool': 3, 'cf_drop': 0.1, 'cf_l2': 3.7588642727386714e-05, 'learning_rate': 0.00037739768793710547, 'batch_size': 128}. Best is trial 22 with value: 0.7596586896056158.\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.5730 ---\n",
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n",
      "학습 데이터가 11356개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 1.76008, saving model to outputs/step3_3_optuna\\trial_45\\models\\fold_7\\best_model.keras\n",
      "89/89 - 7s - 77ms/step - accuracy: 0.2570 - loss: 2.2266 - val_accuracy: 0.4619 - val_loss: 1.7601 - learning_rate: 4.4321e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 1.76008 to 1.28482, saving model to outputs/step3_3_optuna\\trial_45\\models\\fold_7\\best_model.keras\n",
      "89/89 - 5s - 52ms/step - accuracy: 0.4205 - loss: 1.7638 - val_accuracy: 0.6563 - val_loss: 1.2848 - learning_rate: 4.4321e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 1.28482 to 0.91905, saving model to outputs/step3_3_optuna\\trial_45\\models\\fold_7\\best_model.keras\n",
      "89/89 - 3s - 32ms/step - accuracy: 0.5501 - loss: 1.4163 - val_accuracy: 0.7411 - val_loss: 0.9190 - learning_rate: 4.4321e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 0.91905 to 0.68796, saving model to outputs/step3_3_optuna\\trial_45\\models\\fold_7\\best_model.keras\n",
      "89/89 - 5s - 56ms/step - accuracy: 0.6337 - loss: 1.1361 - val_accuracy: 0.8111 - val_loss: 0.6880 - learning_rate: 4.4321e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 0.68796 to 0.55991, saving model to outputs/step3_3_optuna\\trial_45\\models\\fold_7\\best_model.keras\n",
      "89/89 - 5s - 59ms/step - accuracy: 0.6957 - loss: 0.9465 - val_accuracy: 0.8386 - val_loss: 0.5599 - learning_rate: 4.4321e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 0.55991 to 0.47698, saving model to outputs/step3_3_optuna\\trial_45\\models\\fold_7\\best_model.keras\n",
      "89/89 - 5s - 58ms/step - accuracy: 0.7360 - loss: 0.8275 - val_accuracy: 0.8574 - val_loss: 0.4770 - learning_rate: 4.4321e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.47698 to 0.39552, saving model to outputs/step3_3_optuna\\trial_45\\models\\fold_7\\best_model.keras\n",
      "89/89 - 5s - 60ms/step - accuracy: 0.7701 - loss: 0.7328 - val_accuracy: 0.8838 - val_loss: 0.3955 - learning_rate: 4.4321e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 0.39552 to 0.33182, saving model to outputs/step3_3_optuna\\trial_45\\models\\fold_7\\best_model.keras\n",
      "89/89 - 5s - 56ms/step - accuracy: 0.7918 - loss: 0.6712 - val_accuracy: 0.9070 - val_loss: 0.3318 - learning_rate: 4.4321e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.33182 to 0.29097, saving model to outputs/step3_3_optuna\\trial_45\\models\\fold_7\\best_model.keras\n",
      "89/89 - 5s - 60ms/step - accuracy: 0.8155 - loss: 0.6050 - val_accuracy: 0.9163 - val_loss: 0.2910 - learning_rate: 4.4321e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 0.29097 to 0.27017, saving model to outputs/step3_3_optuna\\trial_45\\models\\fold_7\\best_model.keras\n",
      "89/89 - 3s - 31ms/step - accuracy: 0.8326 - loss: 0.5637 - val_accuracy: 0.9261 - val_loss: 0.2702 - learning_rate: 4.4321e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 0.27017 to 0.24141, saving model to outputs/step3_3_optuna\\trial_45\\models\\fold_7\\best_model.keras\n",
      "89/89 - 5s - 60ms/step - accuracy: 0.8439 - loss: 0.5159 - val_accuracy: 0.9338 - val_loss: 0.2414 - learning_rate: 4.4321e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 0.24141 to 0.23207, saving model to outputs/step3_3_optuna\\trial_45\\models\\fold_7\\best_model.keras\n",
      "89/89 - 5s - 57ms/step - accuracy: 0.8578 - loss: 0.4864 - val_accuracy: 0.9358 - val_loss: 0.2321 - learning_rate: 4.4321e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.23207\n",
      "89/89 - 3s - 32ms/step - accuracy: 0.8643 - loss: 0.4549 - val_accuracy: 0.9271 - val_loss: 0.2476 - learning_rate: 4.4321e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 0.23207 to 0.19871, saving model to outputs/step3_3_optuna\\trial_45\\models\\fold_7\\best_model.keras\n",
      "89/89 - 5s - 59ms/step - accuracy: 0.8721 - loss: 0.4330 - val_accuracy: 0.9472 - val_loss: 0.1987 - learning_rate: 4.4321e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss improved from 0.19871 to 0.18209, saving model to outputs/step3_3_optuna\\trial_45\\models\\fold_7\\best_model.keras\n",
      "89/89 - 5s - 58ms/step - accuracy: 0.8842 - loss: 0.4106 - val_accuracy: 0.9514 - val_loss: 0.1821 - learning_rate: 4.4321e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss improved from 0.18209 to 0.17681, saving model to outputs/step3_3_optuna\\trial_45\\models\\fold_7\\best_model.keras\n",
      "89/89 - 3s - 31ms/step - accuracy: 0.8870 - loss: 0.3908 - val_accuracy: 0.9514 - val_loss: 0.1768 - learning_rate: 4.4321e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 0.17681 to 0.15567, saving model to outputs/step3_3_optuna\\trial_45\\models\\fold_7\\best_model.keras\n",
      "89/89 - 5s - 60ms/step - accuracy: 0.8967 - loss: 0.3652 - val_accuracy: 0.9605 - val_loss: 0.1557 - learning_rate: 4.4321e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss improved from 0.15567 to 0.14318, saving model to outputs/step3_3_optuna\\trial_45\\models\\fold_7\\best_model.keras\n",
      "89/89 - 5s - 57ms/step - accuracy: 0.9008 - loss: 0.3497 - val_accuracy: 0.9626 - val_loss: 0.1432 - learning_rate: 4.4321e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss improved from 0.14318 to 0.14185, saving model to outputs/step3_3_optuna\\trial_45\\models\\fold_7\\best_model.keras\n",
      "89/89 - 5s - 58ms/step - accuracy: 0.9044 - loss: 0.3423 - val_accuracy: 0.9626 - val_loss: 0.1418 - learning_rate: 4.4321e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss improved from 0.14185 to 0.13014, saving model to outputs/step3_3_optuna\\trial_45\\models\\fold_7\\best_model.keras\n",
      "89/89 - 3s - 32ms/step - accuracy: 0.9117 - loss: 0.3232 - val_accuracy: 0.9671 - val_loss: 0.1301 - learning_rate: 4.4321e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.13014\n",
      "89/89 - 3s - 31ms/step - accuracy: 0.9176 - loss: 0.2996 - val_accuracy: 0.9593 - val_loss: 0.1481 - learning_rate: 4.4321e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss improved from 0.13014 to 0.12495, saving model to outputs/step3_3_optuna\\trial_45\\models\\fold_7\\best_model.keras\n",
      "89/89 - 5s - 57ms/step - accuracy: 0.9171 - loss: 0.3056 - val_accuracy: 0.9699 - val_loss: 0.1250 - learning_rate: 4.4321e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss improved from 0.12495 to 0.11898, saving model to outputs/step3_3_optuna\\trial_45\\models\\fold_7\\best_model.keras\n",
      "89/89 - 5s - 59ms/step - accuracy: 0.9172 - loss: 0.2976 - val_accuracy: 0.9689 - val_loss: 0.1190 - learning_rate: 4.4321e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.11898\n",
      "89/89 - 5s - 56ms/step - accuracy: 0.9230 - loss: 0.2771 - val_accuracy: 0.9596 - val_loss: 0.1447 - learning_rate: 4.4321e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss improved from 0.11898 to 0.10527, saving model to outputs/step3_3_optuna\\trial_45\\models\\fold_7\\best_model.keras\n",
      "89/89 - 3s - 33ms/step - accuracy: 0.9292 - loss: 0.2618 - val_accuracy: 0.9746 - val_loss: 0.1053 - learning_rate: 4.4321e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss improved from 0.10527 to 0.09373, saving model to outputs/step3_3_optuna\\trial_45\\models\\fold_7\\best_model.keras\n",
      "89/89 - 5s - 57ms/step - accuracy: 0.9320 - loss: 0.2577 - val_accuracy: 0.9826 - val_loss: 0.0937 - learning_rate: 4.4321e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.09373\n",
      "89/89 - 5s - 59ms/step - accuracy: 0.9352 - loss: 0.2559 - val_accuracy: 0.9681 - val_loss: 0.1195 - learning_rate: 4.4321e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.09373\n",
      "89/89 - 3s - 32ms/step - accuracy: 0.9353 - loss: 0.2495 - val_accuracy: 0.9749 - val_loss: 0.1063 - learning_rate: 4.4321e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss improved from 0.09373 to 0.09322, saving model to outputs/step3_3_optuna\\trial_45\\models\\fold_7\\best_model.keras\n",
      "89/89 - 5s - 59ms/step - accuracy: 0.9384 - loss: 0.2392 - val_accuracy: 0.9802 - val_loss: 0.0932 - learning_rate: 4.4321e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss improved from 0.09322 to 0.09141, saving model to outputs/step3_3_optuna\\trial_45\\models\\fold_7\\best_model.keras\n",
      "89/89 - 5s - 56ms/step - accuracy: 0.9428 - loss: 0.2298 - val_accuracy: 0.9817 - val_loss: 0.0914 - learning_rate: 4.4321e-04\n",
      "Restoring model weights from the end of the best epoch: 30.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 20:16:18,658] Trial 45 finished with value: 0.5943821884120717 and parameters: {'WINDOW_SIZE': 30, 'STRIDE': 3, 'gru1_units': 128, 'gru2_units': 0, 'dense_units': 32, 'dropout_rate': 0.38029651337428383, 'l2': 0.00022612794647508772, 'cf_filters': 64, 'cf_kernel': 3, 'cf_pool': 3, 'cf_drop': 0.1, 'cf_l2': 2.168065565077696e-05, 'learning_rate': 0.0004432127940311679, 'batch_size': 128}. Best is trial 22 with value: 0.7596586896056158.\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.5944 ---\n",
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n",
      "학습 데이터가 9972개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 1.92970, saving model to outputs/step3_3_optuna\\trial_46\\models\\fold_7\\best_model.keras\n",
      "78/78 - 14s - 185ms/step - accuracy: 0.2282 - loss: 2.3800 - val_accuracy: 0.3864 - val_loss: 1.9297 - learning_rate: 6.3494e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 1.92970 to 1.63272, saving model to outputs/step3_3_optuna\\trial_46\\models\\fold_7\\best_model.keras\n",
      "78/78 - 9s - 120ms/step - accuracy: 0.3687 - loss: 1.9853 - val_accuracy: 0.5026 - val_loss: 1.6327 - learning_rate: 6.3494e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 1.63272 to 1.22802, saving model to outputs/step3_3_optuna\\trial_46\\models\\fold_7\\best_model.keras\n",
      "78/78 - 10s - 133ms/step - accuracy: 0.4561 - loss: 1.7053 - val_accuracy: 0.6651 - val_loss: 1.2280 - learning_rate: 6.3494e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 1.22802 to 0.82063, saving model to outputs/step3_3_optuna\\trial_46\\models\\fold_7\\best_model.keras\n",
      "78/78 - 8s - 105ms/step - accuracy: 0.5618 - loss: 1.3984 - val_accuracy: 0.8033 - val_loss: 0.8206 - learning_rate: 6.3494e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 0.82063 to 0.61940, saving model to outputs/step3_3_optuna\\trial_46\\models\\fold_7\\best_model.keras\n",
      "78/78 - 9s - 116ms/step - accuracy: 0.6481 - loss: 1.1368 - val_accuracy: 0.8573 - val_loss: 0.6194 - learning_rate: 6.3494e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 0.61940 to 0.51225, saving model to outputs/step3_3_optuna\\trial_46\\models\\fold_7\\best_model.keras\n",
      "78/78 - 10s - 133ms/step - accuracy: 0.7050 - loss: 0.9577 - val_accuracy: 0.8705 - val_loss: 0.5122 - learning_rate: 6.3494e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.51225 to 0.45061, saving model to outputs/step3_3_optuna\\trial_46\\models\\fold_7\\best_model.keras\n",
      "78/78 - 12s - 148ms/step - accuracy: 0.7436 - loss: 0.8542 - val_accuracy: 0.8882 - val_loss: 0.4506 - learning_rate: 6.3494e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 0.45061 to 0.38894, saving model to outputs/step3_3_optuna\\trial_46\\models\\fold_7\\best_model.keras\n",
      "78/78 - 10s - 132ms/step - accuracy: 0.7676 - loss: 0.7673 - val_accuracy: 0.9056 - val_loss: 0.3889 - learning_rate: 6.3494e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.38894 to 0.34867, saving model to outputs/step3_3_optuna\\trial_46\\models\\fold_7\\best_model.keras\n",
      "78/78 - 9s - 116ms/step - accuracy: 0.8028 - loss: 0.6858 - val_accuracy: 0.9170 - val_loss: 0.3487 - learning_rate: 6.3494e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 0.34867 to 0.31136, saving model to outputs/step3_3_optuna\\trial_46\\models\\fold_7\\best_model.keras\n",
      "78/78 - 8s - 105ms/step - accuracy: 0.8180 - loss: 0.6277 - val_accuracy: 0.9204 - val_loss: 0.3114 - learning_rate: 6.3494e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 0.31136 to 0.30570, saving model to outputs/step3_3_optuna\\trial_46\\models\\fold_7\\best_model.keras\n",
      "78/78 - 8s - 107ms/step - accuracy: 0.8280 - loss: 0.6045 - val_accuracy: 0.9167 - val_loss: 0.3057 - learning_rate: 6.3494e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 0.30570 to 0.25418, saving model to outputs/step3_3_optuna\\trial_46\\models\\fold_7\\best_model.keras\n",
      "78/78 - 9s - 115ms/step - accuracy: 0.8510 - loss: 0.5375 - val_accuracy: 0.9415 - val_loss: 0.2542 - learning_rate: 6.3494e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 0.25418 to 0.24672, saving model to outputs/step3_3_optuna\\trial_46\\models\\fold_7\\best_model.keras\n",
      "78/78 - 10s - 132ms/step - accuracy: 0.8630 - loss: 0.5042 - val_accuracy: 0.9419 - val_loss: 0.2467 - learning_rate: 6.3494e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 0.24672 to 0.21708, saving model to outputs/step3_3_optuna\\trial_46\\models\\fold_7\\best_model.keras\n",
      "78/78 - 10s - 132ms/step - accuracy: 0.8673 - loss: 0.4738 - val_accuracy: 0.9526 - val_loss: 0.2171 - learning_rate: 6.3494e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.21708\n",
      "78/78 - 11s - 147ms/step - accuracy: 0.8792 - loss: 0.4495 - val_accuracy: 0.9485 - val_loss: 0.2257 - learning_rate: 6.3494e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss improved from 0.21708 to 0.18472, saving model to outputs/step3_3_optuna\\trial_46\\models\\fold_7\\best_model.keras\n",
      "78/78 - 10s - 133ms/step - accuracy: 0.8824 - loss: 0.4318 - val_accuracy: 0.9598 - val_loss: 0.1847 - learning_rate: 6.3494e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.18472\n",
      "78/78 - 8s - 105ms/step - accuracy: 0.8999 - loss: 0.3901 - val_accuracy: 0.9618 - val_loss: 0.1858 - learning_rate: 6.3494e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.18472\n",
      "78/78 - 10s - 132ms/step - accuracy: 0.8985 - loss: 0.3932 - val_accuracy: 0.9536 - val_loss: 0.2016 - learning_rate: 6.3494e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss improved from 0.18472 to 0.18319, saving model to outputs/step3_3_optuna\\trial_46\\models\\fold_7\\best_model.keras\n",
      "78/78 - 8s - 106ms/step - accuracy: 0.9103 - loss: 0.3554 - val_accuracy: 0.9628 - val_loss: 0.1832 - learning_rate: 6.3494e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss improved from 0.18319 to 0.17036, saving model to outputs/step3_3_optuna\\trial_46\\models\\fold_7\\best_model.keras\n",
      "78/78 - 9s - 115ms/step - accuracy: 0.9077 - loss: 0.3657 - val_accuracy: 0.9635 - val_loss: 0.1704 - learning_rate: 6.3494e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss improved from 0.17036 to 0.13581, saving model to outputs/step3_3_optuna\\trial_46\\models\\fold_7\\best_model.keras\n",
      "78/78 - 10s - 133ms/step - accuracy: 0.9195 - loss: 0.3222 - val_accuracy: 0.9755 - val_loss: 0.1358 - learning_rate: 6.3494e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.13581\n",
      "78/78 - 11s - 146ms/step - accuracy: 0.9201 - loss: 0.3206 - val_accuracy: 0.9622 - val_loss: 0.1792 - learning_rate: 6.3494e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss improved from 0.13581 to 0.12917, saving model to outputs/step3_3_optuna\\trial_46\\models\\fold_7\\best_model.keras\n",
      "78/78 - 10s - 133ms/step - accuracy: 0.9267 - loss: 0.3071 - val_accuracy: 0.9761 - val_loss: 0.1292 - learning_rate: 6.3494e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.12917\n",
      "78/78 - 9s - 115ms/step - accuracy: 0.9310 - loss: 0.2884 - val_accuracy: 0.9669 - val_loss: 0.1684 - learning_rate: 6.3494e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.12917\n",
      "78/78 - 7s - 90ms/step - accuracy: 0.9296 - loss: 0.3025 - val_accuracy: 0.9758 - val_loss: 0.1327 - learning_rate: 6.3494e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss improved from 0.12917 to 0.12285, saving model to outputs/step3_3_optuna\\trial_46\\models\\fold_7\\best_model.keras\n",
      "78/78 - 10s - 133ms/step - accuracy: 0.9368 - loss: 0.2784 - val_accuracy: 0.9766 - val_loss: 0.1228 - learning_rate: 6.3494e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.12285\n",
      "78/78 - 11s - 147ms/step - accuracy: 0.9418 - loss: 0.2633 - val_accuracy: 0.9749 - val_loss: 0.1283 - learning_rate: 6.3494e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.12285\n",
      "78/78 - 8s - 105ms/step - accuracy: 0.9367 - loss: 0.2819 - val_accuracy: 0.9751 - val_loss: 0.1307 - learning_rate: 6.3494e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss improved from 0.12285 to 0.11693, saving model to outputs/step3_3_optuna\\trial_46\\models\\fold_7\\best_model.keras\n",
      "78/78 - 10s - 133ms/step - accuracy: 0.9460 - loss: 0.2498 - val_accuracy: 0.9809 - val_loss: 0.1169 - learning_rate: 6.3494e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss improved from 0.11693 to 0.10579, saving model to outputs/step3_3_optuna\\trial_46\\models\\fold_7\\best_model.keras\n",
      "78/78 - 10s - 133ms/step - accuracy: 0.9435 - loss: 0.2534 - val_accuracy: 0.9833 - val_loss: 0.1058 - learning_rate: 6.3494e-04\n",
      "Restoring model weights from the end of the best epoch: 30.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 20:21:20,569] Trial 46 finished with value: 0.6985520256557898 and parameters: {'WINDOW_SIZE': 45, 'STRIDE': 3, 'gru1_units': 128, 'gru2_units': 64, 'dense_units': 32, 'dropout_rate': 0.4089854930458906, 'l2': 0.00022604201431993638, 'cf_filters': 64, 'cf_kernel': 3, 'cf_pool': 2, 'cf_drop': 0.1, 'cf_l2': 3.274480148893964e-05, 'learning_rate': 0.0006349431543019826, 'batch_size': 128}. Best is trial 22 with value: 0.7596586896056158.\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.6986 ---\n",
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n",
      "학습 데이터가 29365개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 1.77294, saving model to outputs/step3_3_optuna\\trial_47\\models\\fold_7\\best_model.keras\n",
      "230/230 - 26s - 113ms/step - accuracy: 0.2875 - loss: 2.1972 - val_accuracy: 0.4373 - val_loss: 1.7729 - learning_rate: 6.3069e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 1.77294 to 1.01284, saving model to outputs/step3_3_optuna\\trial_47\\models\\fold_7\\best_model.keras\n",
      "230/230 - 40s - 175ms/step - accuracy: 0.4731 - loss: 1.6610 - val_accuracy: 0.7300 - val_loss: 1.0128 - learning_rate: 6.3069e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 1.01284 to 0.51535, saving model to outputs/step3_3_optuna\\trial_47\\models\\fold_7\\best_model.keras\n",
      "230/230 - 21s - 89ms/step - accuracy: 0.6505 - loss: 1.0999 - val_accuracy: 0.8708 - val_loss: 0.5154 - learning_rate: 6.3069e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 0.51535 to 0.37343, saving model to outputs/step3_3_optuna\\trial_47\\models\\fold_7\\best_model.keras\n",
      "230/230 - 22s - 94ms/step - accuracy: 0.7357 - loss: 0.8400 - val_accuracy: 0.9083 - val_loss: 0.3734 - learning_rate: 6.3069e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 0.37343 to 0.28148, saving model to outputs/step3_3_optuna\\trial_47\\models\\fold_7\\best_model.keras\n",
      "230/230 - 40s - 174ms/step - accuracy: 0.7901 - loss: 0.7036 - val_accuracy: 0.9316 - val_loss: 0.2815 - learning_rate: 6.3069e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 0.28148 to 0.26006, saving model to outputs/step3_3_optuna\\trial_47\\models\\fold_7\\best_model.keras\n",
      "230/230 - 41s - 178ms/step - accuracy: 0.8235 - loss: 0.6029 - val_accuracy: 0.9320 - val_loss: 0.2601 - learning_rate: 6.3069e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.26006 to 0.17938, saving model to outputs/step3_3_optuna\\trial_47\\models\\fold_7\\best_model.keras\n",
      "230/230 - 41s - 179ms/step - accuracy: 0.8522 - loss: 0.5314 - val_accuracy: 0.9610 - val_loss: 0.1794 - learning_rate: 6.3069e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 0.17938 to 0.17152, saving model to outputs/step3_3_optuna\\trial_47\\models\\fold_7\\best_model.keras\n",
      "230/230 - 42s - 183ms/step - accuracy: 0.8742 - loss: 0.4702 - val_accuracy: 0.9613 - val_loss: 0.1715 - learning_rate: 6.3069e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.17152 to 0.14851, saving model to outputs/step3_3_optuna\\trial_47\\models\\fold_7\\best_model.keras\n",
      "230/230 - 22s - 95ms/step - accuracy: 0.8831 - loss: 0.4335 - val_accuracy: 0.9675 - val_loss: 0.1485 - learning_rate: 6.3069e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 0.14851 to 0.12666, saving model to outputs/step3_3_optuna\\trial_47\\models\\fold_7\\best_model.keras\n",
      "230/230 - 21s - 90ms/step - accuracy: 0.8964 - loss: 0.3909 - val_accuracy: 0.9763 - val_loss: 0.1267 - learning_rate: 6.3069e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 0.12666 to 0.11783, saving model to outputs/step3_3_optuna\\trial_47\\models\\fold_7\\best_model.keras\n",
      "230/230 - 42s - 183ms/step - accuracy: 0.9068 - loss: 0.3598 - val_accuracy: 0.9797 - val_loss: 0.1178 - learning_rate: 6.3069e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 0.11783 to 0.09905, saving model to outputs/step3_3_optuna\\trial_47\\models\\fold_7\\best_model.keras\n",
      "230/230 - 40s - 174ms/step - accuracy: 0.9109 - loss: 0.3467 - val_accuracy: 0.9844 - val_loss: 0.0991 - learning_rate: 6.3069e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.09905\n",
      "230/230 - 41s - 178ms/step - accuracy: 0.9159 - loss: 0.3344 - val_accuracy: 0.9788 - val_loss: 0.1116 - learning_rate: 6.3069e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.09905\n",
      "230/230 - 21s - 90ms/step - accuracy: 0.9231 - loss: 0.3118 - val_accuracy: 0.9830 - val_loss: 0.1053 - learning_rate: 6.3069e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss improved from 0.09905 to 0.08937, saving model to outputs/step3_3_optuna\\trial_47\\models\\fold_7\\best_model.keras\n",
      "230/230 - 42s - 184ms/step - accuracy: 0.9259 - loss: 0.2989 - val_accuracy: 0.9891 - val_loss: 0.0894 - learning_rate: 6.3069e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.08937\n",
      "230/230 - 21s - 90ms/step - accuracy: 0.9269 - loss: 0.2897 - val_accuracy: 0.9859 - val_loss: 0.0965 - learning_rate: 6.3069e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.08937\n",
      "230/230 - 42s - 183ms/step - accuracy: 0.9326 - loss: 0.2750 - val_accuracy: 0.9881 - val_loss: 0.0924 - learning_rate: 6.3069e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss improved from 0.08937 to 0.07718, saving model to outputs/step3_3_optuna\\trial_47\\models\\fold_7\\best_model.keras\n",
      "230/230 - 40s - 174ms/step - accuracy: 0.9348 - loss: 0.2682 - val_accuracy: 0.9926 - val_loss: 0.0772 - learning_rate: 6.3069e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss improved from 0.07718 to 0.07667, saving model to outputs/step3_3_optuna\\trial_47\\models\\fold_7\\best_model.keras\n",
      "230/230 - 42s - 183ms/step - accuracy: 0.9366 - loss: 0.2601 - val_accuracy: 0.9924 - val_loss: 0.0767 - learning_rate: 6.3069e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss improved from 0.07667 to 0.07450, saving model to outputs/step3_3_optuna\\trial_47\\models\\fold_7\\best_model.keras\n",
      "230/230 - 41s - 179ms/step - accuracy: 0.9363 - loss: 0.2639 - val_accuracy: 0.9937 - val_loss: 0.0745 - learning_rate: 6.3069e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.07450\n",
      "230/230 - 20s - 89ms/step - accuracy: 0.9416 - loss: 0.2479 - val_accuracy: 0.9929 - val_loss: 0.0767 - learning_rate: 6.3069e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss improved from 0.07450 to 0.06898, saving model to outputs/step3_3_optuna\\trial_47\\models\\fold_7\\best_model.keras\n",
      "230/230 - 22s - 95ms/step - accuracy: 0.9442 - loss: 0.2377 - val_accuracy: 0.9953 - val_loss: 0.0690 - learning_rate: 6.3069e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.06898\n",
      "230/230 - 40s - 174ms/step - accuracy: 0.9456 - loss: 0.2315 - val_accuracy: 0.9933 - val_loss: 0.0747 - learning_rate: 6.3069e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.06898\n",
      "230/230 - 41s - 178ms/step - accuracy: 0.9461 - loss: 0.2263 - val_accuracy: 0.9932 - val_loss: 0.0744 - learning_rate: 6.3069e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss improved from 0.06898 to 0.06591, saving model to outputs/step3_3_optuna\\trial_47\\models\\fold_7\\best_model.keras\n",
      "230/230 - 22s - 94ms/step - accuracy: 0.9473 - loss: 0.2273 - val_accuracy: 0.9955 - val_loss: 0.0659 - learning_rate: 6.3069e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.06591\n",
      "230/230 - 40s - 174ms/step - accuracy: 0.9514 - loss: 0.2128 - val_accuracy: 0.9936 - val_loss: 0.0706 - learning_rate: 6.3069e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.06591\n",
      "230/230 - 41s - 178ms/step - accuracy: 0.9490 - loss: 0.2195 - val_accuracy: 0.9947 - val_loss: 0.0692 - learning_rate: 6.3069e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss improved from 0.06591 to 0.05965, saving model to outputs/step3_3_optuna\\trial_47\\models\\fold_7\\best_model.keras\n",
      "230/230 - 21s - 89ms/step - accuracy: 0.9515 - loss: 0.2069 - val_accuracy: 0.9969 - val_loss: 0.0596 - learning_rate: 6.3069e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.05965\n",
      "230/230 - 20s - 89ms/step - accuracy: 0.9515 - loss: 0.2153 - val_accuracy: 0.9959 - val_loss: 0.0635 - learning_rate: 6.3069e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss improved from 0.05965 to 0.05791, saving model to outputs/step3_3_optuna\\trial_47\\models\\fold_7\\best_model.keras\n",
      "230/230 - 21s - 90ms/step - accuracy: 0.9542 - loss: 0.1994 - val_accuracy: 0.9978 - val_loss: 0.0579 - learning_rate: 6.3069e-04\n",
      "Restoring model weights from the end of the best epoch: 30.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 20:37:44,931] Trial 47 finished with value: 0.5460158726785411 and parameters: {'WINDOW_SIZE': 45, 'STRIDE': 1, 'gru1_units': 128, 'gru2_units': 64, 'dense_units': 32, 'dropout_rate': 0.5270832282594669, 'l2': 0.00022068349582754366, 'cf_filters': 64, 'cf_kernel': 3, 'cf_pool': 2, 'cf_drop': 0.2, 'cf_l2': 3.2064686208198625e-05, 'learning_rate': 0.0006306937665536319, 'batch_size': 128}. Best is trial 22 with value: 0.7596586896056158.\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.5460 ---\n",
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n",
      "학습 데이터가 9972개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 1.74872, saving model to outputs/step3_3_optuna\\trial_48\\models\\fold_7\\best_model.keras\n",
      "78/78 - 14s - 178ms/step - accuracy: 0.2446 - loss: 2.3667 - val_accuracy: 0.4474 - val_loss: 1.7487 - learning_rate: 5.5432e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 1.74872 to 1.34197, saving model to outputs/step3_3_optuna\\trial_48\\models\\fold_7\\best_model.keras\n",
      "78/78 - 8s - 108ms/step - accuracy: 0.4171 - loss: 1.8113 - val_accuracy: 0.6056 - val_loss: 1.3420 - learning_rate: 5.5432e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 1.34197 to 0.87696, saving model to outputs/step3_3_optuna\\trial_48\\models\\fold_7\\best_model.keras\n",
      "78/78 - 10s - 132ms/step - accuracy: 0.5532 - loss: 1.4337 - val_accuracy: 0.7708 - val_loss: 0.8770 - learning_rate: 5.5432e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 0.87696 to 0.66493, saving model to outputs/step3_3_optuna\\trial_48\\models\\fold_7\\best_model.keras\n",
      "78/78 - 10s - 132ms/step - accuracy: 0.6665 - loss: 1.1125 - val_accuracy: 0.8088 - val_loss: 0.6649 - learning_rate: 5.5432e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 0.66493 to 0.52875, saving model to outputs/step3_3_optuna\\trial_48\\models\\fold_7\\best_model.keras\n",
      "78/78 - 12s - 148ms/step - accuracy: 0.7431 - loss: 0.8917 - val_accuracy: 0.8463 - val_loss: 0.5288 - learning_rate: 5.5432e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 0.52875 to 0.43255, saving model to outputs/step3_3_optuna\\trial_48\\models\\fold_7\\best_model.keras\n",
      "78/78 - 8s - 106ms/step - accuracy: 0.7814 - loss: 0.7587 - val_accuracy: 0.8780 - val_loss: 0.4325 - learning_rate: 5.5432e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.43255 to 0.38563, saving model to outputs/step3_3_optuna\\trial_48\\models\\fold_7\\best_model.keras\n",
      "78/78 - 10s - 132ms/step - accuracy: 0.8144 - loss: 0.6729 - val_accuracy: 0.8950 - val_loss: 0.3856 - learning_rate: 5.5432e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 0.38563 to 0.32617, saving model to outputs/step3_3_optuna\\trial_48\\models\\fold_7\\best_model.keras\n",
      "78/78 - 7s - 89ms/step - accuracy: 0.8387 - loss: 0.5935 - val_accuracy: 0.9178 - val_loss: 0.3262 - learning_rate: 5.5432e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.32617 to 0.30099, saving model to outputs/step3_3_optuna\\trial_48\\models\\fold_7\\best_model.keras\n",
      "78/78 - 10s - 132ms/step - accuracy: 0.8595 - loss: 0.5367 - val_accuracy: 0.9224 - val_loss: 0.3010 - learning_rate: 5.5432e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 0.30099 to 0.26892, saving model to outputs/step3_3_optuna\\trial_48\\models\\fold_7\\best_model.keras\n",
      "78/78 - 10s - 132ms/step - accuracy: 0.8716 - loss: 0.4989 - val_accuracy: 0.9397 - val_loss: 0.2689 - learning_rate: 5.5432e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 0.26892 to 0.24013, saving model to outputs/step3_3_optuna\\trial_48\\models\\fold_7\\best_model.keras\n",
      "78/78 - 10s - 132ms/step - accuracy: 0.8853 - loss: 0.4542 - val_accuracy: 0.9478 - val_loss: 0.2401 - learning_rate: 5.5432e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 0.24013 to 0.22925, saving model to outputs/step3_3_optuna\\trial_48\\models\\fold_7\\best_model.keras\n",
      "78/78 - 10s - 132ms/step - accuracy: 0.8970 - loss: 0.4197 - val_accuracy: 0.9526 - val_loss: 0.2293 - learning_rate: 5.5432e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 0.22925 to 0.22386, saving model to outputs/step3_3_optuna\\trial_48\\models\\fold_7\\best_model.keras\n",
      "78/78 - 8s - 106ms/step - accuracy: 0.9072 - loss: 0.3782 - val_accuracy: 0.9519 - val_loss: 0.2239 - learning_rate: 5.5432e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 0.22386 to 0.20047, saving model to outputs/step3_3_optuna\\trial_48\\models\\fold_7\\best_model.keras\n",
      "78/78 - 10s - 131ms/step - accuracy: 0.9135 - loss: 0.3694 - val_accuracy: 0.9622 - val_loss: 0.2005 - learning_rate: 5.5432e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss improved from 0.20047 to 0.18927, saving model to outputs/step3_3_optuna\\trial_48\\models\\fold_7\\best_model.keras\n",
      "78/78 - 7s - 89ms/step - accuracy: 0.9255 - loss: 0.3300 - val_accuracy: 0.9628 - val_loss: 0.1893 - learning_rate: 5.5432e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss improved from 0.18927 to 0.17649, saving model to outputs/step3_3_optuna\\trial_48\\models\\fold_7\\best_model.keras\n",
      "78/78 - 8s - 105ms/step - accuracy: 0.9197 - loss: 0.3400 - val_accuracy: 0.9671 - val_loss: 0.1765 - learning_rate: 5.5432e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 0.17649 to 0.16184, saving model to outputs/step3_3_optuna\\trial_48\\models\\fold_7\\best_model.keras\n",
      "78/78 - 10s - 132ms/step - accuracy: 0.9250 - loss: 0.3188 - val_accuracy: 0.9712 - val_loss: 0.1618 - learning_rate: 5.5432e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.16184\n",
      "78/78 - 9s - 115ms/step - accuracy: 0.9354 - loss: 0.2896 - val_accuracy: 0.9697 - val_loss: 0.1699 - learning_rate: 5.5432e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss improved from 0.16184 to 0.15056, saving model to outputs/step3_3_optuna\\trial_48\\models\\fold_7\\best_model.keras\n",
      "78/78 - 12s - 149ms/step - accuracy: 0.9437 - loss: 0.2691 - val_accuracy: 0.9738 - val_loss: 0.1506 - learning_rate: 5.5432e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss improved from 0.15056 to 0.14284, saving model to outputs/step3_3_optuna\\trial_48\\models\\fold_7\\best_model.keras\n",
      "78/78 - 9s - 116ms/step - accuracy: 0.9457 - loss: 0.2686 - val_accuracy: 0.9756 - val_loss: 0.1428 - learning_rate: 5.5432e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.14284\n",
      "78/78 - 10s - 131ms/step - accuracy: 0.9447 - loss: 0.2594 - val_accuracy: 0.9741 - val_loss: 0.1465 - learning_rate: 5.5432e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss improved from 0.14284 to 0.13559, saving model to outputs/step3_3_optuna\\trial_48\\models\\fold_7\\best_model.keras\n",
      "78/78 - 10s - 133ms/step - accuracy: 0.9481 - loss: 0.2505 - val_accuracy: 0.9804 - val_loss: 0.1356 - learning_rate: 5.5432e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss improved from 0.13559 to 0.12205, saving model to outputs/step3_3_optuna\\trial_48\\models\\fold_7\\best_model.keras\n",
      "78/78 - 8s - 106ms/step - accuracy: 0.9503 - loss: 0.2365 - val_accuracy: 0.9821 - val_loss: 0.1221 - learning_rate: 5.5432e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.12205\n",
      "78/78 - 9s - 115ms/step - accuracy: 0.9518 - loss: 0.2326 - val_accuracy: 0.9835 - val_loss: 0.1225 - learning_rate: 5.5432e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss improved from 0.12205 to 0.11714, saving model to outputs/step3_3_optuna\\trial_48\\models\\fold_7\\best_model.keras\n",
      "78/78 - 12s - 149ms/step - accuracy: 0.9562 - loss: 0.2174 - val_accuracy: 0.9819 - val_loss: 0.1171 - learning_rate: 5.5432e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.11714\n",
      "78/78 - 9s - 115ms/step - accuracy: 0.9566 - loss: 0.2150 - val_accuracy: 0.9802 - val_loss: 0.1255 - learning_rate: 5.5432e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.11714\n",
      "78/78 - 10s - 132ms/step - accuracy: 0.9572 - loss: 0.2160 - val_accuracy: 0.9811 - val_loss: 0.1216 - learning_rate: 5.5432e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss improved from 0.11714 to 0.11084, saving model to outputs/step3_3_optuna\\trial_48\\models\\fold_7\\best_model.keras\n",
      "78/78 - 7s - 90ms/step - accuracy: 0.9558 - loss: 0.2180 - val_accuracy: 0.9838 - val_loss: 0.1108 - learning_rate: 5.5432e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.11084\n",
      "78/78 - 7s - 89ms/step - accuracy: 0.9604 - loss: 0.2082 - val_accuracy: 0.9831 - val_loss: 0.1144 - learning_rate: 5.5432e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss improved from 0.11084 to 0.10364, saving model to outputs/step3_3_optuna\\trial_48\\models\\fold_7\\best_model.keras\n",
      "78/78 - 10s - 133ms/step - accuracy: 0.9607 - loss: 0.2034 - val_accuracy: 0.9860 - val_loss: 0.1036 - learning_rate: 5.5432e-04\n",
      "Restoring model weights from the end of the best epoch: 30.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 20:42:39,208] Trial 48 finished with value: 0.6573949673674435 and parameters: {'WINDOW_SIZE': 45, 'STRIDE': 3, 'gru1_units': 128, 'gru2_units': 64, 'dense_units': 64, 'dropout_rate': 0.4833518906864834, 'l2': 0.00029583666447782357, 'cf_filters': 64, 'cf_kernel': 3, 'cf_pool': 2, 'cf_drop': 0.1, 'cf_l2': 1.4123989638501852e-05, 'learning_rate': 0.0005543178488606362, 'batch_size': 128}. Best is trial 22 with value: 0.7596586896056158.\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.6574 ---\n",
      "--- 단일 폴드(Fold-7) 테스트 시작 (데이터 샘플링: 30.0%) ---\n",
      "학습 데이터가 29365개로 샘플링 되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from None to 0.80175, saving model to outputs/step3_3_optuna\\trial_49\\models\\fold_7\\best_model.keras\n",
      "230/230 - 30s - 132ms/step - accuracy: 0.4401 - loss: 1.7211 - val_accuracy: 0.7710 - val_loss: 0.8017 - learning_rate: 6.3117e-04\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 0.80175 to 0.40188, saving model to outputs/step3_3_optuna\\trial_49\\models\\fold_7\\best_model.keras\n",
      "230/230 - 40s - 175ms/step - accuracy: 0.7564 - loss: 0.8125 - val_accuracy: 0.8976 - val_loss: 0.4019 - learning_rate: 6.3117e-04\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 0.40188 to 0.28229, saving model to outputs/step3_3_optuna\\trial_49\\models\\fold_7\\best_model.keras\n",
      "230/230 - 41s - 179ms/step - accuracy: 0.8488 - loss: 0.5344 - val_accuracy: 0.9287 - val_loss: 0.2823 - learning_rate: 6.3117e-04\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 0.28229 to 0.22283, saving model to outputs/step3_3_optuna\\trial_49\\models\\fold_7\\best_model.keras\n",
      "230/230 - 41s - 178ms/step - accuracy: 0.8863 - loss: 0.4196 - val_accuracy: 0.9446 - val_loss: 0.2228 - learning_rate: 6.3117e-04\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 0.22283 to 0.17296, saving model to outputs/step3_3_optuna\\trial_49\\models\\fold_7\\best_model.keras\n",
      "230/230 - 41s - 180ms/step - accuracy: 0.9119 - loss: 0.3399 - val_accuracy: 0.9625 - val_loss: 0.1730 - learning_rate: 6.3117e-04\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 0.17296 to 0.14099, saving model to outputs/step3_3_optuna\\trial_49\\models\\fold_7\\best_model.keras\n",
      "230/230 - 41s - 177ms/step - accuracy: 0.9307 - loss: 0.2821 - val_accuracy: 0.9716 - val_loss: 0.1410 - learning_rate: 6.3117e-04\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.14099 to 0.12366, saving model to outputs/step3_3_optuna\\trial_49\\models\\fold_7\\best_model.keras\n",
      "230/230 - 25s - 110ms/step - accuracy: 0.9424 - loss: 0.2427 - val_accuracy: 0.9763 - val_loss: 0.1237 - learning_rate: 6.3117e-04\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 0.12366 to 0.09861, saving model to outputs/step3_3_optuna\\trial_49\\models\\fold_7\\best_model.keras\n",
      "230/230 - 41s - 178ms/step - accuracy: 0.9497 - loss: 0.2176 - val_accuracy: 0.9863 - val_loss: 0.0986 - learning_rate: 6.3117e-04\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.09861\n",
      "230/230 - 41s - 178ms/step - accuracy: 0.9578 - loss: 0.1910 - val_accuracy: 0.9775 - val_loss: 0.1231 - learning_rate: 6.3117e-04\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 0.09861 to 0.09122, saving model to outputs/step3_3_optuna\\trial_49\\models\\fold_7\\best_model.keras\n",
      "230/230 - 41s - 179ms/step - accuracy: 0.9623 - loss: 0.1810 - val_accuracy: 0.9879 - val_loss: 0.0912 - learning_rate: 6.3117e-04\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 0.09122 to 0.07822, saving model to outputs/step3_3_optuna\\trial_49\\models\\fold_7\\best_model.keras\n",
      "230/230 - 41s - 178ms/step - accuracy: 0.9664 - loss: 0.1697 - val_accuracy: 0.9933 - val_loss: 0.0782 - learning_rate: 6.3117e-04\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 0.07822 to 0.07219, saving model to outputs/step3_3_optuna\\trial_49\\models\\fold_7\\best_model.keras\n",
      "230/230 - 41s - 179ms/step - accuracy: 0.9716 - loss: 0.1502 - val_accuracy: 0.9950 - val_loss: 0.0722 - learning_rate: 6.3117e-04\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.07219\n",
      "230/230 - 41s - 178ms/step - accuracy: 0.9752 - loss: 0.1359 - val_accuracy: 0.9925 - val_loss: 0.0773 - learning_rate: 6.3117e-04\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.07219\n",
      "230/230 - 41s - 178ms/step - accuracy: 0.9753 - loss: 0.1332 - val_accuracy: 0.9926 - val_loss: 0.0759 - learning_rate: 6.3117e-04\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss improved from 0.07219 to 0.06640, saving model to outputs/step3_3_optuna\\trial_49\\models\\fold_7\\best_model.keras\n",
      "230/230 - 41s - 179ms/step - accuracy: 0.9794 - loss: 0.1248 - val_accuracy: 0.9959 - val_loss: 0.0664 - learning_rate: 6.3117e-04\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.06640\n",
      "230/230 - 41s - 178ms/step - accuracy: 0.9789 - loss: 0.1218 - val_accuracy: 0.9897 - val_loss: 0.0790 - learning_rate: 6.3117e-04\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 0.06640 to 0.06152, saving model to outputs/step3_3_optuna\\trial_49\\models\\fold_7\\best_model.keras\n",
      "230/230 - 41s - 178ms/step - accuracy: 0.9802 - loss: 0.1161 - val_accuracy: 0.9965 - val_loss: 0.0615 - learning_rate: 6.3117e-04\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.06152\n",
      "230/230 - 25s - 109ms/step - accuracy: 0.9828 - loss: 0.1096 - val_accuracy: 0.9951 - val_loss: 0.0637 - learning_rate: 6.3117e-04\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.06152\n",
      "230/230 - 41s - 179ms/step - accuracy: 0.9835 - loss: 0.1046 - val_accuracy: 0.9963 - val_loss: 0.0616 - learning_rate: 6.3117e-04\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss improved from 0.06152 to 0.06047, saving model to outputs/step3_3_optuna\\trial_49\\models\\fold_7\\best_model.keras\n",
      "230/230 - 41s - 179ms/step - accuracy: 0.9828 - loss: 0.1073 - val_accuracy: 0.9961 - val_loss: 0.0605 - learning_rate: 6.3117e-04\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss improved from 0.06047 to 0.05869, saving model to outputs/step3_3_optuna\\trial_49\\models\\fold_7\\best_model.keras\n",
      "230/230 - 25s - 110ms/step - accuracy: 0.9850 - loss: 0.1012 - val_accuracy: 0.9965 - val_loss: 0.0587 - learning_rate: 6.3117e-04\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss improved from 0.05869 to 0.05362, saving model to outputs/step3_3_optuna\\trial_49\\models\\fold_7\\best_model.keras\n",
      "230/230 - 41s - 178ms/step - accuracy: 0.9859 - loss: 0.0946 - val_accuracy: 0.9984 - val_loss: 0.0536 - learning_rate: 6.3117e-04\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.05362\n",
      "230/230 - 41s - 178ms/step - accuracy: 0.9856 - loss: 0.0962 - val_accuracy: 0.9969 - val_loss: 0.0568 - learning_rate: 6.3117e-04\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.05362\n",
      "230/230 - 25s - 108ms/step - accuracy: 0.9868 - loss: 0.0914 - val_accuracy: 0.9965 - val_loss: 0.0559 - learning_rate: 6.3117e-04\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss improved from 0.05362 to 0.05122, saving model to outputs/step3_3_optuna\\trial_49\\models\\fold_7\\best_model.keras\n",
      "230/230 - 25s - 108ms/step - accuracy: 0.9863 - loss: 0.0900 - val_accuracy: 0.9981 - val_loss: 0.0512 - learning_rate: 6.3117e-04\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.05122\n",
      "230/230 - 25s - 108ms/step - accuracy: 0.9872 - loss: 0.0892 - val_accuracy: 0.9900 - val_loss: 0.0793 - learning_rate: 6.3117e-04\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss improved from 0.05122 to 0.04794, saving model to outputs/step3_3_optuna\\trial_49\\models\\fold_7\\best_model.keras\n",
      "230/230 - 41s - 180ms/step - accuracy: 0.9889 - loss: 0.0847 - val_accuracy: 0.9987 - val_loss: 0.0479 - learning_rate: 6.3117e-04\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.04794\n",
      "230/230 - 41s - 178ms/step - accuracy: 0.9864 - loss: 0.0864 - val_accuracy: 0.9984 - val_loss: 0.0482 - learning_rate: 6.3117e-04\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss improved from 0.04794 to 0.04719, saving model to outputs/step3_3_optuna\\trial_49\\models\\fold_7\\best_model.keras\n",
      "230/230 - 25s - 109ms/step - accuracy: 0.9884 - loss: 0.0820 - val_accuracy: 0.9984 - val_loss: 0.0472 - learning_rate: 6.3117e-04\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.04719\n",
      "230/230 - 25s - 109ms/step - accuracy: 0.9885 - loss: 0.0816 - val_accuracy: 0.9959 - val_loss: 0.0519 - learning_rate: 6.3117e-04\n",
      "Restoring model weights from the end of the best epoch: 29.\n",
      "--- 단일 폴드(Fold-7) 테스트 완료 | Macro F1: 0.6421 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 21:01:21,858] Trial 49 finished with value: 0.6421367149213076 and parameters: {'WINDOW_SIZE': 45, 'STRIDE': 1, 'gru1_units': 128, 'gru2_units': 64, 'dense_units': 96, 'dropout_rate': 0.4632455793434988, 'l2': 0.0001945363957443319, 'cf_filters': 64, 'cf_kernel': 3, 'cf_pool': 2, 'cf_drop': 0.1, 'cf_l2': 6.0801645708310533e-05, 'learning_rate': 0.0006311694939990189, 'batch_size': 128}. Best is trial 22 with value: 0.7596586896056158.\n",
      "\n",
      "\n",
      "Optuna 하이퍼파라미터 탐색: 100%|██████████| 50/50 [2:56:09<00:00, 211.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Optuna 최적화 완료 =====\n",
      "총 50개의 Trial 중 최고 성능 Macro F1: 0.7597\n",
      "최적 하이퍼파라미터:\n",
      "  - WINDOW_SIZE: 45\n",
      "  - STRIDE: 3\n",
      "  - gru1_units: 96\n",
      "  - gru2_units: 0\n",
      "  - dense_units: 32\n",
      "  - dropout_rate: 0.48251104946192314\n",
      "  - l2: 1.56850614158258e-05\n",
      "  - cf_filters: 80\n",
      "  - cf_kernel: 3\n",
      "  - cf_pool: 3\n",
      "  - cf_drop: 0.1\n",
      "  - cf_l2: 0.00019154698595805827\n",
      "  - learning_rate: 0.0005043822289191313\n",
      "  - batch_size: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === Fold 유틸 (앞 셀에서 정의되어 있으면 생략 가능) ===\n",
    "def norm_fold_id(fid):\n",
    "    s = str(fid)\n",
    "    return s if s.startswith(\"fold_\") else f\"fold_{int(s)}\"\n",
    "\n",
    "def fold_as_int(fid):\n",
    "    return int(str(fid).split('_')[-1])\n",
    "\n",
    "\n",
    "# === 1) 이전 단계 최적값으로 베이스 설정 생성 ===\n",
    "best_config_base = copy.deepcopy(config.MANUAL_CONFIG)\n",
    "best_config_base['model_arch']['variant'] = best_architecture\n",
    "best_config_base['feature_extraction']['FEATURE_CONFIG'] = best_feature_config\n",
    "best_config_base['feature_extraction']['TEMPORAL_FEATURES'] = best_temporal_config\n",
    "\n",
    "# Fold ID 안전 처리\n",
    "tuning_fold_id = norm_fold_id(tuning_fold_id)     # ex) 'fold_7'\n",
    "tuning_fold_int = fold_as_int(tuning_fold_id)     # ex) 7\n",
    "\n",
    "# 출력 루트 보장\n",
    "optuna_out_root = 'outputs/step3_3_optuna'\n",
    "os.makedirs(optuna_out_root, exist_ok=True)\n",
    "\n",
    "\n",
    "# === 2) Optuna 목적 함수 (단일 폴드, 30% 서브셋) ===\n",
    "def objective_single_fold(trial: optuna.Trial) -> float:\n",
    "    # 탐색 공간 정의(사용자 구현 함수)\n",
    "    cfg = config.define_search_space(trial)\n",
    "\n",
    "    # 고정 파라미터 적용\n",
    "    cfg['paths']['OUTPUT_DIR'] = os.path.join(optuna_out_root, f\"trial_{trial.number}\")\n",
    "    os.makedirs(cfg['paths']['OUTPUT_DIR'], exist_ok=True)\n",
    "\n",
    "    cfg['model_arch']['variant'] = best_config_base['model_arch']['variant']\n",
    "    cfg['feature_extraction']['FEATURE_CONFIG'] = best_config_base['feature_extraction']['FEATURE_CONFIG']\n",
    "    cfg['feature_extraction']['TEMPORAL_FEATURES'] = best_config_base['feature_extraction']['TEMPORAL_FEATURES']\n",
    "\n",
    "    # 고속 설정\n",
    "    cfg['training']['epochs'] = 30\n",
    "    cfg['training']['early_stopping_patience'] = 5\n",
    "\n",
    "    # 단일 폴드 실행 (정수 fold 권장)\n",
    "    score = run_single_fold(cfg, fold_id=tuning_fold_int, data_subset_fraction=0.3)\n",
    "\n",
    "    # Optuna는 목적값이 float이어야 함\n",
    "    return float(score)\n",
    "\n",
    "\n",
    "# === 3) Study 생성 및 최적화 ===\n",
    "study_db_path = \"sqlite:///outputs/optuna_study.db\"\n",
    "os.makedirs(\"outputs\", exist_ok=True)  # DB 경로 보장\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    study_name='Single_Fold_HPO_v2',\n",
    "    storage=study_db_path,\n",
    "    load_if_exists=True,\n",
    ")\n",
    "\n",
    "n_trials = 50\n",
    "done = len(study.trials)\n",
    "remaining = max(0, n_trials - done)\n",
    "\n",
    "with tqdm(total=remaining, desc=\"Optuna 하이퍼파라미터 탐색\") as pbar:\n",
    "    if remaining > 0:\n",
    "        def _tqdm_cb(study_, trial_):\n",
    "            pbar.update(1)\n",
    "        study.optimize(objective_single_fold, n_trials=remaining, callbacks=[_tqdm_cb])\n",
    "\n",
    "# === 4) 최적 결과 출력 ===\n",
    "print(\"\\n===== Optuna 최적화 완료 =====\")\n",
    "print(f\"총 {len(study.trials)}개의 Trial 중 최고 성능 Macro F1: {study.best_value:.4f}\")\n",
    "print(\"최적 하이퍼파라미터:\")\n",
    "for k, v in study.best_params.items():\n",
    "    print(f\"  - {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2162ecee",
   "metadata": {},
   "source": [
    "📊 Optuna 탐색 결과 정리 및 저장\n",
    "\n",
    "Optuna 스터디({study_name})의 모든 Trial 기록을 DataFrame으로 변환하고,\n",
    "완료된 Trial만 정리하여 성능 순위, 실행 시간(duration) 등을 확인합니다.\n",
    "최종적으로 CSV로 저장합니다.\n",
    "\n",
    "🔧 단계 요약\n",
    "\n",
    "스터디 로드\n",
    "\n",
    "DB: outputs/optuna_study.db\n",
    "\n",
    "이름: Single_Fold_HPO_v2\n",
    "\n",
    "DataFrame 변환\n",
    "\n",
    "주요 속성: trial 번호, 값(value), 상태(state), 파라미터(params), 실행 시간 등\n",
    "\n",
    "정리\n",
    "\n",
    "state == COMPLETE만 필터링\n",
    "\n",
    "실행 시간(datetime_complete - datetime_start) 계산\n",
    "\n",
    "성능 기준(value) 내림차순 정렬 후 rank 부여\n",
    "\n",
    "결과 확인 및 저장\n",
    "\n",
    "Jupyter display()로 전체 trial 상세 확인\n",
    "\n",
    "CSV 저장 경로: outputs/step3_3_optuna/all_trials.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a18ce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>number</th>\n",
       "      <th>value</th>\n",
       "      <th>state</th>\n",
       "      <th>params_STRIDE</th>\n",
       "      <th>params_WINDOW_SIZE</th>\n",
       "      <th>params_batch_size</th>\n",
       "      <th>params_cf_drop</th>\n",
       "      <th>params_cf_filters</th>\n",
       "      <th>params_cf_kernel</th>\n",
       "      <th>params_cf_l2</th>\n",
       "      <th>params_cf_pool</th>\n",
       "      <th>params_dense_units</th>\n",
       "      <th>params_dropout_rate</th>\n",
       "      <th>params_gru1_units</th>\n",
       "      <th>params_gru2_units</th>\n",
       "      <th>params_l2</th>\n",
       "      <th>params_learning_rate</th>\n",
       "      <th>datetime_start</th>\n",
       "      <th>datetime_complete</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>0.759659</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "      <td>64</td>\n",
       "      <td>0.1</td>\n",
       "      <td>80</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>0.482511</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000504</td>\n",
       "      <td>2025-09-08 19:15:38.255708</td>\n",
       "      <td>2025-09-08 19:18:13.073147</td>\n",
       "      <td>0 days 00:02:34.817439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>0.723218</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "      <td>128</td>\n",
       "      <td>0.1</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>0.438662</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.000504</td>\n",
       "      <td>2025-09-08 20:07:54.125792</td>\n",
       "      <td>2025-09-08 20:10:12.865495</td>\n",
       "      <td>0 days 00:02:18.739703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>0.720825</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "      <td>64</td>\n",
       "      <td>0.1</td>\n",
       "      <td>80</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>0.496966</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000483</td>\n",
       "      <td>2025-09-08 19:20:11.569881</td>\n",
       "      <td>2025-09-08 19:22:43.492777</td>\n",
       "      <td>0 days 00:02:31.922896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>0.709706</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "      <td>64</td>\n",
       "      <td>0.1</td>\n",
       "      <td>80</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>0.312072</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.000521</td>\n",
       "      <td>2025-09-08 18:59:56.126156</td>\n",
       "      <td>2025-09-08 19:02:31.038868</td>\n",
       "      <td>0 days 00:02:34.912712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>46</td>\n",
       "      <td>0.698552</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "      <td>128</td>\n",
       "      <td>0.1</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>0.408985</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>0.000226</td>\n",
       "      <td>0.000635</td>\n",
       "      <td>2025-09-08 20:16:18.690515</td>\n",
       "      <td>2025-09-08 20:21:20.429025</td>\n",
       "      <td>0 days 00:05:01.738510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>35</td>\n",
       "      <td>0.698067</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>64</td>\n",
       "      <td>0.1</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>0.377332</td>\n",
       "      <td>160</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>2025-09-08 19:49:25.395184</td>\n",
       "      <td>2025-09-08 19:52:02.975094</td>\n",
       "      <td>0 days 00:02:37.579910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>0.694273</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "      <td>96</td>\n",
       "      <td>0.1</td>\n",
       "      <td>80</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000188</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>0.426359</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000483</td>\n",
       "      <td>2025-09-08 18:56:44.549255</td>\n",
       "      <td>2025-09-08 18:58:19.326397</td>\n",
       "      <td>0 days 00:01:34.777142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>31</td>\n",
       "      <td>0.685642</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "      <td>64</td>\n",
       "      <td>0.1</td>\n",
       "      <td>80</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>0.399077</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000459</td>\n",
       "      <td>2025-09-08 19:39:28.064393</td>\n",
       "      <td>2025-09-08 19:41:52.547400</td>\n",
       "      <td>0 days 00:02:24.483007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "      <td>0.679594</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "      <td>64</td>\n",
       "      <td>0.1</td>\n",
       "      <td>80</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>0.470515</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000503</td>\n",
       "      <td>2025-09-08 19:13:12.462137</td>\n",
       "      <td>2025-09-08 19:15:38.092828</td>\n",
       "      <td>0 days 00:02:25.630691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>33</td>\n",
       "      <td>0.672247</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "      <td>64</td>\n",
       "      <td>0.1</td>\n",
       "      <td>80</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>0.326247</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000623</td>\n",
       "      <td>2025-09-08 19:44:25.352580</td>\n",
       "      <td>2025-09-08 19:46:57.229236</td>\n",
       "      <td>0 days 00:02:31.876656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>48</td>\n",
       "      <td>0.657395</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "      <td>128</td>\n",
       "      <td>0.1</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>0.483352</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>0.000296</td>\n",
       "      <td>0.000554</td>\n",
       "      <td>2025-09-08 20:37:44.957916</td>\n",
       "      <td>2025-09-08 20:42:39.082251</td>\n",
       "      <td>0 days 00:04:54.124335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>0.655832</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>96</td>\n",
       "      <td>0.1</td>\n",
       "      <td>80</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>0.585773</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>2025-09-08 18:35:59.041662</td>\n",
       "      <td>2025-09-08 18:39:06.731574</td>\n",
       "      <td>0 days 00:03:07.689912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>0.644880</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>96</td>\n",
       "      <td>0.1</td>\n",
       "      <td>80</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>0.573466</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000408</td>\n",
       "      <td>2025-09-08 18:39:06.910885</td>\n",
       "      <td>2025-09-08 18:44:06.473775</td>\n",
       "      <td>0 days 00:04:59.562890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>49</td>\n",
       "      <td>0.642137</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>128</td>\n",
       "      <td>0.1</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>2</td>\n",
       "      <td>96</td>\n",
       "      <td>0.463246</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000631</td>\n",
       "      <td>2025-09-08 20:42:39.225414</td>\n",
       "      <td>2025-09-08 21:01:21.657581</td>\n",
       "      <td>0 days 00:18:42.432167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>0.638812</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>96</td>\n",
       "      <td>0.1</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>0.517768</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000614</td>\n",
       "      <td>2025-09-08 18:11:09.895690</td>\n",
       "      <td>2025-09-08 18:14:10.290490</td>\n",
       "      <td>0 days 00:03:00.394800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>34</td>\n",
       "      <td>0.635136</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "      <td>128</td>\n",
       "      <td>0.1</td>\n",
       "      <td>80</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>0.494092</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>2025-09-08 19:46:57.379106</td>\n",
       "      <td>2025-09-08 19:49:25.226635</td>\n",
       "      <td>0 days 00:02:27.847529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>19</td>\n",
       "      <td>0.634287</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "      <td>64</td>\n",
       "      <td>0.2</td>\n",
       "      <td>80</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>0.300340</td>\n",
       "      <td>160</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.000519</td>\n",
       "      <td>2025-09-08 19:07:14.934563</td>\n",
       "      <td>2025-09-08 19:09:54.880851</td>\n",
       "      <td>0 days 00:02:39.946288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>28</td>\n",
       "      <td>0.634144</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>64</td>\n",
       "      <td>0.2</td>\n",
       "      <td>80</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>3</td>\n",
       "      <td>96</td>\n",
       "      <td>0.393511</td>\n",
       "      <td>256</td>\n",
       "      <td>96</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000549</td>\n",
       "      <td>2025-09-08 19:29:08.679616</td>\n",
       "      <td>2025-09-08 19:31:43.327099</td>\n",
       "      <td>0 days 00:02:34.647483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>27</td>\n",
       "      <td>0.632203</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "      <td>64</td>\n",
       "      <td>0.1</td>\n",
       "      <td>80</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>0.476263</td>\n",
       "      <td>160</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.000323</td>\n",
       "      <td>2025-09-08 19:26:35.359468</td>\n",
       "      <td>2025-09-08 19:29:08.546647</td>\n",
       "      <td>0 days 00:02:33.187179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>43</td>\n",
       "      <td>0.630569</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "      <td>128</td>\n",
       "      <td>0.1</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>0.440993</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>0.000575</td>\n",
       "      <td>2025-09-08 20:10:13.006000</td>\n",
       "      <td>2025-09-08 20:12:32.720798</td>\n",
       "      <td>0 days 00:02:19.714798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>0.623059</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>2</td>\n",
       "      <td>96</td>\n",
       "      <td>0.392759</td>\n",
       "      <td>128</td>\n",
       "      <td>96</td>\n",
       "      <td>0.000233</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>2025-09-08 18:14:10.430027</td>\n",
       "      <td>2025-09-08 18:19:37.580591</td>\n",
       "      <td>0 days 00:05:27.150564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>0.618248</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "      <td>64</td>\n",
       "      <td>0.1</td>\n",
       "      <td>80</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>0.429291</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000690</td>\n",
       "      <td>2025-09-08 19:18:13.217018</td>\n",
       "      <td>2025-09-08 19:20:11.368100</td>\n",
       "      <td>0 days 00:01:58.151082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>12</td>\n",
       "      <td>0.617642</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "      <td>96</td>\n",
       "      <td>0.1</td>\n",
       "      <td>80</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000286</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>0.592302</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000382</td>\n",
       "      <td>2025-09-08 18:44:06.656916</td>\n",
       "      <td>2025-09-08 18:46:37.368422</td>\n",
       "      <td>0 days 00:02:30.711506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>25</td>\n",
       "      <td>0.617232</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>64</td>\n",
       "      <td>0.1</td>\n",
       "      <td>80</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>0.498063</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000577</td>\n",
       "      <td>2025-09-08 19:22:43.663764</td>\n",
       "      <td>2025-09-08 19:24:06.977254</td>\n",
       "      <td>0 days 00:01:23.313490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>18</td>\n",
       "      <td>0.613960</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>80</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>0.310234</td>\n",
       "      <td>256</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000504</td>\n",
       "      <td>2025-09-08 19:02:31.222565</td>\n",
       "      <td>2025-09-08 19:07:14.749384</td>\n",
       "      <td>0 days 00:04:43.526819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>13</td>\n",
       "      <td>0.612745</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>96</td>\n",
       "      <td>0.1</td>\n",
       "      <td>80</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>0.554460</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000408</td>\n",
       "      <td>2025-09-08 18:46:37.511994</td>\n",
       "      <td>2025-09-08 18:51:41.814877</td>\n",
       "      <td>0 days 00:05:04.302883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>26</td>\n",
       "      <td>0.609370</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "      <td>64</td>\n",
       "      <td>0.1</td>\n",
       "      <td>80</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>0.513639</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000470</td>\n",
       "      <td>2025-09-08 19:24:07.205330</td>\n",
       "      <td>2025-09-08 19:26:35.138051</td>\n",
       "      <td>0 days 00:02:27.932721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>32</td>\n",
       "      <td>0.607174</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "      <td>64</td>\n",
       "      <td>0.1</td>\n",
       "      <td>80</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>0.456820</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000685</td>\n",
       "      <td>2025-09-08 19:41:52.772584</td>\n",
       "      <td>2025-09-08 19:44:25.154979</td>\n",
       "      <td>0 days 00:02:32.382395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>9</td>\n",
       "      <td>0.602674</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>96</td>\n",
       "      <td>0.2</td>\n",
       "      <td>48</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>0.362598</td>\n",
       "      <td>128</td>\n",
       "      <td>96</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>2025-09-08 18:30:18.425220</td>\n",
       "      <td>2025-09-08 18:35:58.887530</td>\n",
       "      <td>0 days 00:05:40.462310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>40</td>\n",
       "      <td>0.598502</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>64</td>\n",
       "      <td>0.1</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>0.512497</td>\n",
       "      <td>160</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000622</td>\n",
       "      <td>2025-09-08 20:03:07.098553</td>\n",
       "      <td>2025-09-08 20:05:29.244533</td>\n",
       "      <td>0 days 00:02:22.145980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>45</td>\n",
       "      <td>0.594382</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>128</td>\n",
       "      <td>0.1</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>0.380297</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000226</td>\n",
       "      <td>0.000443</td>\n",
       "      <td>2025-09-08 20:13:55.203763</td>\n",
       "      <td>2025-09-08 20:16:18.521137</td>\n",
       "      <td>0 days 00:02:23.317374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>0.582388</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>96</td>\n",
       "      <td>0.1</td>\n",
       "      <td>80</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>0.420133</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>2025-09-08 18:58:19.512363</td>\n",
       "      <td>2025-09-08 18:59:55.943166</td>\n",
       "      <td>0 days 00:01:36.430803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>41</td>\n",
       "      <td>0.580580</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "      <td>64</td>\n",
       "      <td>0.1</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>0.405695</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>2025-09-08 20:05:29.377815</td>\n",
       "      <td>2025-09-08 20:07:53.975223</td>\n",
       "      <td>0 days 00:02:24.597408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34</td>\n",
       "      <td>44</td>\n",
       "      <td>0.572952</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>128</td>\n",
       "      <td>0.1</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>3</td>\n",
       "      <td>96</td>\n",
       "      <td>0.469795</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.000377</td>\n",
       "      <td>2025-09-08 20:12:32.871150</td>\n",
       "      <td>2025-09-08 20:13:55.036249</td>\n",
       "      <td>0 days 00:01:22.165099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35</td>\n",
       "      <td>8</td>\n",
       "      <td>0.572519</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>96</td>\n",
       "      <td>0.1</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>3</td>\n",
       "      <td>96</td>\n",
       "      <td>0.487947</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>0.000254</td>\n",
       "      <td>0.000588</td>\n",
       "      <td>2025-09-08 18:26:00.825936</td>\n",
       "      <td>2025-09-08 18:30:18.272363</td>\n",
       "      <td>0 days 00:04:17.446427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>36</td>\n",
       "      <td>37</td>\n",
       "      <td>0.566448</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>64</td>\n",
       "      <td>0.1</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>0.325684</td>\n",
       "      <td>160</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>2025-09-08 19:54:32.288034</td>\n",
       "      <td>2025-09-08 19:58:17.997805</td>\n",
       "      <td>0 days 00:03:45.709771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>37</td>\n",
       "      <td>5</td>\n",
       "      <td>0.557282</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>5</td>\n",
       "      <td>45</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>2</td>\n",
       "      <td>96</td>\n",
       "      <td>0.352643</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>2025-09-08 18:21:04.904386</td>\n",
       "      <td>2025-09-08 18:23:13.095403</td>\n",
       "      <td>0 days 00:02:08.191017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>38</td>\n",
       "      <td>14</td>\n",
       "      <td>0.552135</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>96</td>\n",
       "      <td>0.1</td>\n",
       "      <td>80</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>0.556700</td>\n",
       "      <td>160</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000318</td>\n",
       "      <td>2025-09-08 18:51:41.998344</td>\n",
       "      <td>2025-09-08 18:56:44.361650</td>\n",
       "      <td>0 days 00:05:02.363306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>39</td>\n",
       "      <td>29</td>\n",
       "      <td>0.547426</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>80</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>0.534692</td>\n",
       "      <td>256</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>2025-09-08 19:31:43.510632</td>\n",
       "      <td>2025-09-08 19:36:42.608472</td>\n",
       "      <td>0 days 00:04:59.097840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>40</td>\n",
       "      <td>47</td>\n",
       "      <td>0.546016</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>0.527083</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>0.000631</td>\n",
       "      <td>2025-09-08 20:21:20.578940</td>\n",
       "      <td>2025-09-08 20:37:44.796660</td>\n",
       "      <td>0 days 00:16:24.217720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>41</td>\n",
       "      <td>20</td>\n",
       "      <td>0.530242</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "      <td>64</td>\n",
       "      <td>0.1</td>\n",
       "      <td>80</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>0.426843</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>2025-09-08 19:09:55.153102</td>\n",
       "      <td>2025-09-08 19:13:12.295480</td>\n",
       "      <td>0 days 00:03:17.142378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>42</td>\n",
       "      <td>39</td>\n",
       "      <td>0.527203</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>64</td>\n",
       "      <td>0.2</td>\n",
       "      <td>48</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>0.380080</td>\n",
       "      <td>128</td>\n",
       "      <td>96</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000230</td>\n",
       "      <td>2025-09-08 19:59:48.005822</td>\n",
       "      <td>2025-09-08 20:03:06.955360</td>\n",
       "      <td>0 days 00:03:18.949538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td>0.526881</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>64</td>\n",
       "      <td>0.1</td>\n",
       "      <td>48</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>0.458458</td>\n",
       "      <td>160</td>\n",
       "      <td>64</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000660</td>\n",
       "      <td>2025-09-08 18:08:53.097493</td>\n",
       "      <td>2025-09-08 18:11:09.753142</td>\n",
       "      <td>0 days 00:02:16.655649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>44</td>\n",
       "      <td>7</td>\n",
       "      <td>0.525319</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>3</td>\n",
       "      <td>96</td>\n",
       "      <td>0.531233</td>\n",
       "      <td>256</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000430</td>\n",
       "      <td>2025-09-08 18:25:16.517596</td>\n",
       "      <td>2025-09-08 18:26:00.690723</td>\n",
       "      <td>0 days 00:00:44.173127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>45</td>\n",
       "      <td>36</td>\n",
       "      <td>0.518525</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>64</td>\n",
       "      <td>0.1</td>\n",
       "      <td>48</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>0.370011</td>\n",
       "      <td>160</td>\n",
       "      <td>64</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000358</td>\n",
       "      <td>2025-09-08 19:52:03.142006</td>\n",
       "      <td>2025-09-08 19:54:32.128801</td>\n",
       "      <td>0 days 00:02:28.986795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>46</td>\n",
       "      <td>30</td>\n",
       "      <td>0.515921</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>0.444440</td>\n",
       "      <td>96</td>\n",
       "      <td>64</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>2025-09-08 19:36:42.809635</td>\n",
       "      <td>2025-09-08 19:39:27.931556</td>\n",
       "      <td>0 days 00:02:45.121921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>0.498559</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>0.348882</td>\n",
       "      <td>256</td>\n",
       "      <td>64</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>2025-09-08 18:05:12.882104</td>\n",
       "      <td>2025-09-08 18:08:52.921922</td>\n",
       "      <td>0 days 00:03:40.039818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>48</td>\n",
       "      <td>38</td>\n",
       "      <td>0.493917</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>3</td>\n",
       "      <td>96</td>\n",
       "      <td>0.340588</td>\n",
       "      <td>160</td>\n",
       "      <td>64</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>2025-09-08 19:58:18.120912</td>\n",
       "      <td>2025-09-08 19:59:47.863910</td>\n",
       "      <td>0 days 00:01:29.742998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>49</td>\n",
       "      <td>6</td>\n",
       "      <td>0.492391</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>0.588660</td>\n",
       "      <td>96</td>\n",
       "      <td>64</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000233</td>\n",
       "      <td>2025-09-08 18:23:13.231413</td>\n",
       "      <td>2025-09-08 18:25:16.378304</td>\n",
       "      <td>0 days 00:02:03.146891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>0.461645</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>128</td>\n",
       "      <td>0.1</td>\n",
       "      <td>48</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>0.344310</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000254</td>\n",
       "      <td>2025-09-08 18:19:37.725669</td>\n",
       "      <td>2025-09-08 18:21:04.764475</td>\n",
       "      <td>0 days 00:01:27.038806</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    rank  number     value     state  params_STRIDE  params_WINDOW_SIZE  \\\n",
       "0      1      22  0.759659  COMPLETE              3                  45   \n",
       "1      2      42  0.723218  COMPLETE              3                  45   \n",
       "2      3      24  0.720825  COMPLETE              3                  45   \n",
       "3      4      17  0.709706  COMPLETE              3                  45   \n",
       "4      5      46  0.698552  COMPLETE              3                  45   \n",
       "5      6      35  0.698067  COMPLETE              3                  30   \n",
       "6      7      15  0.694273  COMPLETE              3                  45   \n",
       "7      8      31  0.685642  COMPLETE              3                  45   \n",
       "8      9      21  0.679594  COMPLETE              3                  45   \n",
       "9     10      33  0.672247  COMPLETE              3                  45   \n",
       "10    11      48  0.657395  COMPLETE              3                  45   \n",
       "11    12      10  0.655832  COMPLETE              1                  45   \n",
       "12    13      11  0.644880  COMPLETE              1                  45   \n",
       "13    14      49  0.642137  COMPLETE              1                  45   \n",
       "14    15       2  0.638812  COMPLETE              1                  45   \n",
       "15    16      34  0.635136  COMPLETE              3                  45   \n",
       "16    17      19  0.634287  COMPLETE              3                  45   \n",
       "17    18      28  0.634144  COMPLETE              5                  30   \n",
       "18    19      27  0.632203  COMPLETE              3                  45   \n",
       "19    20      43  0.630569  COMPLETE              3                  45   \n",
       "20    21       3  0.623059  COMPLETE              1                  30   \n",
       "21    22      23  0.618248  COMPLETE              3                  45   \n",
       "22    23      12  0.617642  COMPLETE              3                  45   \n",
       "23    24      25  0.617232  COMPLETE              5                  30   \n",
       "24    25      18  0.613960  COMPLETE              3                  30   \n",
       "25    26      13  0.612745  COMPLETE              1                  45   \n",
       "26    27      26  0.609370  COMPLETE              3                  45   \n",
       "27    28      32  0.607174  COMPLETE              3                  45   \n",
       "28    29       9  0.602674  COMPLETE              1                  45   \n",
       "29    30      40  0.598502  COMPLETE              3                  30   \n",
       "30    31      45  0.594382  COMPLETE              3                  30   \n",
       "31    32      16  0.582388  COMPLETE              3                  30   \n",
       "32    33      41  0.580580  COMPLETE              3                  45   \n",
       "33    34      44  0.572952  COMPLETE              3                  15   \n",
       "34    35       8  0.572519  COMPLETE              1                  15   \n",
       "35    36      37  0.566448  COMPLETE              3                  30   \n",
       "36    37       5  0.557282  COMPLETE              5                  45   \n",
       "37    38      14  0.552135  COMPLETE              1                  30   \n",
       "38    39      29  0.547426  COMPLETE              3                  45   \n",
       "39    40      47  0.546016  COMPLETE              1                  45   \n",
       "40    41      20  0.530242  COMPLETE              3                  45   \n",
       "41    42      39  0.527203  COMPLETE              3                  15   \n",
       "42    43       1  0.526881  COMPLETE              5                  30   \n",
       "43    44       7  0.525319  COMPLETE              5                  15   \n",
       "44    45      36  0.518525  COMPLETE              5                  30   \n",
       "45    46      30  0.515921  COMPLETE              3                  45   \n",
       "46    47       0  0.498559  COMPLETE              3                  45   \n",
       "47    48      38  0.493917  COMPLETE              5                  30   \n",
       "48    49       6  0.492391  COMPLETE              3                  30   \n",
       "49    50       4  0.461645  COMPLETE              5                  15   \n",
       "\n",
       "    params_batch_size  params_cf_drop  params_cf_filters  params_cf_kernel  \\\n",
       "0                  64             0.1                 80                 3   \n",
       "1                 128             0.1                 64                 3   \n",
       "2                  64             0.1                 80                 3   \n",
       "3                  64             0.1                 80                 3   \n",
       "4                 128             0.1                 64                 3   \n",
       "5                  64             0.1                 48                 3   \n",
       "6                  96             0.1                 80                 3   \n",
       "7                  64             0.1                 80                 3   \n",
       "8                  64             0.1                 80                 3   \n",
       "9                  64             0.1                 80                 3   \n",
       "10                128             0.1                 64                 3   \n",
       "11                 96             0.1                 80                 3   \n",
       "12                 96             0.1                 80                 3   \n",
       "13                128             0.1                 64                 3   \n",
       "14                 96             0.1                 64                 3   \n",
       "15                128             0.1                 80                 3   \n",
       "16                 64             0.2                 80                 5   \n",
       "17                 64             0.2                 80                 5   \n",
       "18                 64             0.1                 80                 3   \n",
       "19                128             0.1                 64                 3   \n",
       "20                128             0.0                 64                 7   \n",
       "21                 64             0.1                 80                 3   \n",
       "22                 96             0.1                 80                 3   \n",
       "23                 64             0.1                 80                 3   \n",
       "24                 64             0.0                 80                 7   \n",
       "25                 96             0.1                 80                 3   \n",
       "26                 64             0.1                 80                 3   \n",
       "27                 64             0.1                 80                 3   \n",
       "28                 96             0.2                 48                 7   \n",
       "29                 64             0.1                 64                 5   \n",
       "30                128             0.1                 64                 3   \n",
       "31                 96             0.1                 80                 3   \n",
       "32                 64             0.1                 48                 3   \n",
       "33                128             0.1                 64                 3   \n",
       "34                 96             0.1                 64                 5   \n",
       "35                 64             0.1                 48                 3   \n",
       "36                128             0.2                 64                 3   \n",
       "37                 96             0.1                 80                 3   \n",
       "38                 64             0.0                 80                 7   \n",
       "39                128             0.2                 64                 3   \n",
       "40                 64             0.1                 80                 3   \n",
       "41                 64             0.2                 48                 7   \n",
       "42                 64             0.1                 48                 5   \n",
       "43                 64             0.0                 48                 3   \n",
       "44                 64             0.1                 48                 5   \n",
       "45                 64             0.0                 64                 7   \n",
       "46                128             0.0                 64                 7   \n",
       "47                128             0.0                 48                 3   \n",
       "48                128             0.2                 48                 3   \n",
       "49                128             0.1                 48                 5   \n",
       "\n",
       "    params_cf_l2  params_cf_pool  params_dense_units  params_dropout_rate  \\\n",
       "0       0.000192               3                  32             0.482511   \n",
       "1       0.000031               3                  32             0.438662   \n",
       "2       0.000053               3                  32             0.496966   \n",
       "3       0.000078               3                  32             0.312072   \n",
       "4       0.000033               2                  32             0.408985   \n",
       "5       0.000040               3                  32             0.377332   \n",
       "6       0.000188               3                  32             0.426359   \n",
       "7       0.000195               3                  32             0.399077   \n",
       "8       0.000202               3                  32             0.470515   \n",
       "9       0.000022               3                  32             0.326247   \n",
       "10      0.000014               2                  64             0.483352   \n",
       "11      0.000263               3                  32             0.585773   \n",
       "12      0.000262               3                  32             0.573466   \n",
       "13      0.000061               2                  96             0.463246   \n",
       "14      0.000036               3                  64             0.517768   \n",
       "15      0.000183               2                  64             0.494092   \n",
       "16      0.000147               3                  32             0.300340   \n",
       "17      0.000066               3                  96             0.393511   \n",
       "18      0.000049               3                  32             0.476263   \n",
       "19      0.000029               3                  32             0.440993   \n",
       "20      0.000055               2                  96             0.392759   \n",
       "21      0.000117               3                  32             0.429291   \n",
       "22      0.000286               3                  32             0.592302   \n",
       "23      0.000050               3                  32             0.498063   \n",
       "24      0.000070               3                  32             0.310234   \n",
       "25      0.000225               3                  32             0.554460   \n",
       "26      0.000026               2                  32             0.513639   \n",
       "27      0.000127               3                  32             0.456820   \n",
       "28      0.000031               2                  64             0.362598   \n",
       "29      0.000095               3                  64             0.512497   \n",
       "30      0.000022               3                  32             0.380297   \n",
       "31      0.000162               3                  32             0.420133   \n",
       "32      0.000042               3                  32             0.405695   \n",
       "33      0.000038               3                  96             0.469795   \n",
       "34      0.000034               3                  96             0.487947   \n",
       "35      0.000043               2                  64             0.325684   \n",
       "36      0.000011               2                  96             0.352643   \n",
       "37      0.000151               3                  32             0.556700   \n",
       "38      0.000101               2                  32             0.534692   \n",
       "39      0.000032               2                  32             0.527083   \n",
       "40      0.000082               3                  32             0.426843   \n",
       "41      0.000018               3                  32             0.380080   \n",
       "42      0.000112               2                  64             0.458458   \n",
       "43      0.000012               3                  96             0.531233   \n",
       "44      0.000057               3                  32             0.370011   \n",
       "45      0.000079               3                  32             0.444440   \n",
       "46      0.000094               2                  32             0.348882   \n",
       "47      0.000042               3                  96             0.340588   \n",
       "48      0.000015               3                  64             0.588660   \n",
       "49      0.000011               3                  64             0.344310   \n",
       "\n",
       "    params_gru1_units  params_gru2_units  params_l2  params_learning_rate  \\\n",
       "0                  96                  0   0.000016              0.000504   \n",
       "1                 128                  0   0.000205              0.000504   \n",
       "2                  96                  0   0.000033              0.000483   \n",
       "3                  96                  0   0.000120              0.000521   \n",
       "4                 128                 64   0.000226              0.000635   \n",
       "5                 160                  0   0.000056              0.000357   \n",
       "6                  96                  0   0.000011              0.000483   \n",
       "7                  96                  0   0.000017              0.000459   \n",
       "8                  96                  0   0.000016              0.000503   \n",
       "9                  96                  0   0.000037              0.000623   \n",
       "10                128                 64   0.000296              0.000554   \n",
       "11                 96                  0   0.000022              0.000372   \n",
       "12                 96                  0   0.000022              0.000408   \n",
       "13                128                 64   0.000195              0.000631   \n",
       "14                 96                  0   0.000019              0.000614   \n",
       "15                 96                  0   0.000018              0.000538   \n",
       "16                160                  0   0.000159              0.000519   \n",
       "17                256                 96   0.000096              0.000549   \n",
       "18                160                  0   0.000142              0.000323   \n",
       "19                128                  0   0.000279              0.000575   \n",
       "20                128                 96   0.000233              0.000193   \n",
       "21                 96                  0   0.000010              0.000690   \n",
       "22                 96                  0   0.000024              0.000382   \n",
       "23                 96                  0   0.000065              0.000577   \n",
       "24                256                  0   0.000125              0.000504   \n",
       "25                 96                  0   0.000024              0.000408   \n",
       "26                 96                  0   0.000031              0.000470   \n",
       "27                 96                  0   0.000014              0.000685   \n",
       "28                128                 96   0.000044              0.000149   \n",
       "29                160                  0   0.000076              0.000622   \n",
       "30                128                  0   0.000226              0.000443   \n",
       "31                 96                  0   0.000010              0.000334   \n",
       "32                 96                  0   0.000012              0.000418   \n",
       "33                128                  0   0.000181              0.000377   \n",
       "34                 96                 96   0.000254              0.000588   \n",
       "35                160                  0   0.000070              0.000276   \n",
       "36                128                 64   0.000045              0.000281   \n",
       "37                160                  0   0.000031              0.000318   \n",
       "38                256                  0   0.000032              0.000192   \n",
       "39                128                 64   0.000221              0.000631   \n",
       "40                 96                 96   0.000092              0.000103   \n",
       "41                128                 96   0.000114              0.000230   \n",
       "42                160                 64   0.000014              0.000660   \n",
       "43                256                  0   0.000012              0.000430   \n",
       "44                160                 64   0.000057              0.000358   \n",
       "45                 96                 64   0.000185              0.000454   \n",
       "46                256                 64   0.000042              0.000138   \n",
       "47                160                 64   0.000052              0.000298   \n",
       "48                 96                 64   0.000088              0.000233   \n",
       "49                128                 64   0.000064              0.000254   \n",
       "\n",
       "               datetime_start          datetime_complete  \\\n",
       "0  2025-09-08 19:15:38.255708 2025-09-08 19:18:13.073147   \n",
       "1  2025-09-08 20:07:54.125792 2025-09-08 20:10:12.865495   \n",
       "2  2025-09-08 19:20:11.569881 2025-09-08 19:22:43.492777   \n",
       "3  2025-09-08 18:59:56.126156 2025-09-08 19:02:31.038868   \n",
       "4  2025-09-08 20:16:18.690515 2025-09-08 20:21:20.429025   \n",
       "5  2025-09-08 19:49:25.395184 2025-09-08 19:52:02.975094   \n",
       "6  2025-09-08 18:56:44.549255 2025-09-08 18:58:19.326397   \n",
       "7  2025-09-08 19:39:28.064393 2025-09-08 19:41:52.547400   \n",
       "8  2025-09-08 19:13:12.462137 2025-09-08 19:15:38.092828   \n",
       "9  2025-09-08 19:44:25.352580 2025-09-08 19:46:57.229236   \n",
       "10 2025-09-08 20:37:44.957916 2025-09-08 20:42:39.082251   \n",
       "11 2025-09-08 18:35:59.041662 2025-09-08 18:39:06.731574   \n",
       "12 2025-09-08 18:39:06.910885 2025-09-08 18:44:06.473775   \n",
       "13 2025-09-08 20:42:39.225414 2025-09-08 21:01:21.657581   \n",
       "14 2025-09-08 18:11:09.895690 2025-09-08 18:14:10.290490   \n",
       "15 2025-09-08 19:46:57.379106 2025-09-08 19:49:25.226635   \n",
       "16 2025-09-08 19:07:14.934563 2025-09-08 19:09:54.880851   \n",
       "17 2025-09-08 19:29:08.679616 2025-09-08 19:31:43.327099   \n",
       "18 2025-09-08 19:26:35.359468 2025-09-08 19:29:08.546647   \n",
       "19 2025-09-08 20:10:13.006000 2025-09-08 20:12:32.720798   \n",
       "20 2025-09-08 18:14:10.430027 2025-09-08 18:19:37.580591   \n",
       "21 2025-09-08 19:18:13.217018 2025-09-08 19:20:11.368100   \n",
       "22 2025-09-08 18:44:06.656916 2025-09-08 18:46:37.368422   \n",
       "23 2025-09-08 19:22:43.663764 2025-09-08 19:24:06.977254   \n",
       "24 2025-09-08 19:02:31.222565 2025-09-08 19:07:14.749384   \n",
       "25 2025-09-08 18:46:37.511994 2025-09-08 18:51:41.814877   \n",
       "26 2025-09-08 19:24:07.205330 2025-09-08 19:26:35.138051   \n",
       "27 2025-09-08 19:41:52.772584 2025-09-08 19:44:25.154979   \n",
       "28 2025-09-08 18:30:18.425220 2025-09-08 18:35:58.887530   \n",
       "29 2025-09-08 20:03:07.098553 2025-09-08 20:05:29.244533   \n",
       "30 2025-09-08 20:13:55.203763 2025-09-08 20:16:18.521137   \n",
       "31 2025-09-08 18:58:19.512363 2025-09-08 18:59:55.943166   \n",
       "32 2025-09-08 20:05:29.377815 2025-09-08 20:07:53.975223   \n",
       "33 2025-09-08 20:12:32.871150 2025-09-08 20:13:55.036249   \n",
       "34 2025-09-08 18:26:00.825936 2025-09-08 18:30:18.272363   \n",
       "35 2025-09-08 19:54:32.288034 2025-09-08 19:58:17.997805   \n",
       "36 2025-09-08 18:21:04.904386 2025-09-08 18:23:13.095403   \n",
       "37 2025-09-08 18:51:41.998344 2025-09-08 18:56:44.361650   \n",
       "38 2025-09-08 19:31:43.510632 2025-09-08 19:36:42.608472   \n",
       "39 2025-09-08 20:21:20.578940 2025-09-08 20:37:44.796660   \n",
       "40 2025-09-08 19:09:55.153102 2025-09-08 19:13:12.295480   \n",
       "41 2025-09-08 19:59:48.005822 2025-09-08 20:03:06.955360   \n",
       "42 2025-09-08 18:08:53.097493 2025-09-08 18:11:09.753142   \n",
       "43 2025-09-08 18:25:16.517596 2025-09-08 18:26:00.690723   \n",
       "44 2025-09-08 19:52:03.142006 2025-09-08 19:54:32.128801   \n",
       "45 2025-09-08 19:36:42.809635 2025-09-08 19:39:27.931556   \n",
       "46 2025-09-08 18:05:12.882104 2025-09-08 18:08:52.921922   \n",
       "47 2025-09-08 19:58:18.120912 2025-09-08 19:59:47.863910   \n",
       "48 2025-09-08 18:23:13.231413 2025-09-08 18:25:16.378304   \n",
       "49 2025-09-08 18:19:37.725669 2025-09-08 18:21:04.764475   \n",
       "\n",
       "                 duration  \n",
       "0  0 days 00:02:34.817439  \n",
       "1  0 days 00:02:18.739703  \n",
       "2  0 days 00:02:31.922896  \n",
       "3  0 days 00:02:34.912712  \n",
       "4  0 days 00:05:01.738510  \n",
       "5  0 days 00:02:37.579910  \n",
       "6  0 days 00:01:34.777142  \n",
       "7  0 days 00:02:24.483007  \n",
       "8  0 days 00:02:25.630691  \n",
       "9  0 days 00:02:31.876656  \n",
       "10 0 days 00:04:54.124335  \n",
       "11 0 days 00:03:07.689912  \n",
       "12 0 days 00:04:59.562890  \n",
       "13 0 days 00:18:42.432167  \n",
       "14 0 days 00:03:00.394800  \n",
       "15 0 days 00:02:27.847529  \n",
       "16 0 days 00:02:39.946288  \n",
       "17 0 days 00:02:34.647483  \n",
       "18 0 days 00:02:33.187179  \n",
       "19 0 days 00:02:19.714798  \n",
       "20 0 days 00:05:27.150564  \n",
       "21 0 days 00:01:58.151082  \n",
       "22 0 days 00:02:30.711506  \n",
       "23 0 days 00:01:23.313490  \n",
       "24 0 days 00:04:43.526819  \n",
       "25 0 days 00:05:04.302883  \n",
       "26 0 days 00:02:27.932721  \n",
       "27 0 days 00:02:32.382395  \n",
       "28 0 days 00:05:40.462310  \n",
       "29 0 days 00:02:22.145980  \n",
       "30 0 days 00:02:23.317374  \n",
       "31 0 days 00:01:36.430803  \n",
       "32 0 days 00:02:24.597408  \n",
       "33 0 days 00:01:22.165099  \n",
       "34 0 days 00:04:17.446427  \n",
       "35 0 days 00:03:45.709771  \n",
       "36 0 days 00:02:08.191017  \n",
       "37 0 days 00:05:02.363306  \n",
       "38 0 days 00:04:59.097840  \n",
       "39 0 days 00:16:24.217720  \n",
       "40 0 days 00:03:17.142378  \n",
       "41 0 days 00:03:18.949538  \n",
       "42 0 days 00:02:16.655649  \n",
       "43 0 days 00:00:44.173127  \n",
       "44 0 days 00:02:28.986795  \n",
       "45 0 days 00:02:45.121921  \n",
       "46 0 days 00:03:40.039818  \n",
       "47 0 days 00:01:29.742998  \n",
       "48 0 days 00:02:03.146891  \n",
       "49 0 days 00:01:27.038806  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[저장 완료] 전체 50개 trial 결과: outputs/step3_3_optuna/all_trials.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import optuna\n",
    "\n",
    "# === 1) 스터디 로드 ===\n",
    "study_name = \"Single_Fold_HPO_v2\"\n",
    "storage = \"sqlite:///outputs/optuna_study.db\"\n",
    "study = optuna.load_study(study_name=study_name, storage=storage)\n",
    "\n",
    "# === 2) DataFrame 변환 ===\n",
    "df = study.trials_dataframe(\n",
    "    attrs=(\n",
    "        \"number\",\n",
    "        \"value\",\n",
    "        \"state\",\n",
    "        \"params\",\n",
    "        \"user_attrs\",\n",
    "        \"system_attrs\",\n",
    "        \"datetime_start\",\n",
    "        \"datetime_complete\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# 완료된 trial만 필터링\n",
    "df = df[df[\"state\"] == \"COMPLETE\"].copy()\n",
    "\n",
    "# 실행 시간(duration) 계산\n",
    "df[\"duration\"] = (\n",
    "    pd.to_datetime(df[\"datetime_complete\"]) - pd.to_datetime(df[\"datetime_start\"])\n",
    ")\n",
    "\n",
    "# === 3) 성능 기준 정렬 + rank 부여 ===\n",
    "df_sorted = df.sort_values(\"value\", ascending=False).reset_index(drop=True)\n",
    "df_sorted.insert(0, \"rank\", range(1, len(df_sorted) + 1))\n",
    "\n",
    "# === 4) 전체 trial 확인 ===\n",
    "pd.set_option(\"display.max_columns\", None)  # 모든 컬럼 보이기\n",
    "display(df_sorted)\n",
    "\n",
    "# === 5) CSV로 저장 ===\n",
    "out_csv = \"outputs/step3_3_optuna/all_trials.csv\"\n",
    "os.makedirs(os.path.dirname(out_csv), exist_ok=True)\n",
    "df_sorted.to_csv(out_csv, index=False)\n",
    "\n",
    "print(f\"\\n[저장 완료] 전체 {len(df_sorted)}개 trial 결과: {out_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "13d5687a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Parameter</th>\n",
       "      <th>Pearson_r</th>\n",
       "      <th>p_value</th>\n",
       "      <th>abs_r</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>params_cf_filters</td>\n",
       "      <td>0.512632</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.512632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>params_WINDOW_SIZE</td>\n",
       "      <td>0.494015</td>\n",
       "      <td>0.000266</td>\n",
       "      <td>0.494015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>params_learning_rate</td>\n",
       "      <td>0.458581</td>\n",
       "      <td>0.000810</td>\n",
       "      <td>0.458581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>params_gru2_units</td>\n",
       "      <td>-0.455226</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>0.455226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>params_cf_kernel</td>\n",
       "      <td>-0.379555</td>\n",
       "      <td>0.006557</td>\n",
       "      <td>0.379555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>params_gru1_units</td>\n",
       "      <td>-0.313976</td>\n",
       "      <td>0.026387</td>\n",
       "      <td>0.313976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>params_cf_l2</td>\n",
       "      <td>0.296045</td>\n",
       "      <td>0.036847</td>\n",
       "      <td>0.296045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>params_dense_units</td>\n",
       "      <td>-0.289057</td>\n",
       "      <td>0.041757</td>\n",
       "      <td>0.289057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>params_STRIDE</td>\n",
       "      <td>-0.266335</td>\n",
       "      <td>0.061545</td>\n",
       "      <td>0.266335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>params_batch_size</td>\n",
       "      <td>-0.146984</td>\n",
       "      <td>0.308396</td>\n",
       "      <td>0.146984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>params_cf_pool</td>\n",
       "      <td>0.117228</td>\n",
       "      <td>0.417503</td>\n",
       "      <td>0.117228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>params_cf_drop</td>\n",
       "      <td>0.097640</td>\n",
       "      <td>0.499950</td>\n",
       "      <td>0.097640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>params_l2</td>\n",
       "      <td>0.042368</td>\n",
       "      <td>0.770177</td>\n",
       "      <td>0.042368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>params_dropout_rate</td>\n",
       "      <td>0.034561</td>\n",
       "      <td>0.811668</td>\n",
       "      <td>0.034561</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Parameter  Pearson_r   p_value     abs_r\n",
       "4      params_cf_filters   0.512632  0.000141  0.512632\n",
       "1     params_WINDOW_SIZE   0.494015  0.000266  0.494015\n",
       "13  params_learning_rate   0.458581  0.000810  0.458581\n",
       "11     params_gru2_units  -0.455226  0.000895  0.455226\n",
       "5       params_cf_kernel  -0.379555  0.006557  0.379555\n",
       "10     params_gru1_units  -0.313976  0.026387  0.313976\n",
       "6           params_cf_l2   0.296045  0.036847  0.296045\n",
       "8     params_dense_units  -0.289057  0.041757  0.289057\n",
       "0          params_STRIDE  -0.266335  0.061545  0.266335\n",
       "2      params_batch_size  -0.146984  0.308396  0.146984\n",
       "7         params_cf_pool   0.117228  0.417503  0.117228\n",
       "3         params_cf_drop   0.097640  0.499950  0.097640\n",
       "12             params_l2   0.042368  0.770177  0.042368\n",
       "9    params_dropout_rate   0.034561  0.811668  0.034561"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "상관계수 결과 저장 완료.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# df_sorted = Optuna 결과 전체 (앞 단계에서 만든 DataFrame)\n",
    "param_cols = [c for c in df_sorted.columns if c.startswith(\"params_\")]\n",
    "\n",
    "corr_results = []\n",
    "for col in param_cols:\n",
    "    x = df_sorted[col].astype(float).values\n",
    "    y = df_sorted[\"value\"].astype(float).values\n",
    "    r, p = pearsonr(x, y)\n",
    "    corr_results.append((col, r, p))\n",
    "\n",
    "# 표로 정리\n",
    "corr_df = pd.DataFrame(corr_results, columns=[\"Parameter\", \"Pearson_r\", \"p_value\"])\n",
    "corr_df[\"abs_r\"] = corr_df[\"Pearson_r\"].abs()\n",
    "corr_df = corr_df.sort_values(\"abs_r\", ascending=False)\n",
    "\n",
    "display(corr_df)\n",
    "\n",
    "# CSV 저장\n",
    "corr_df.to_csv(\"outputs/step3_3_optuna/param_value_correlations.csv\", index=False)\n",
    "print(\"상관계수 결과 저장 완료.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cabe31",
   "metadata": {},
   "source": [
    "\n",
    "| Parameter            | Value    |\n",
    "|-----------------------|---------:|\n",
    "| STRIDE               | 3        |\n",
    "| WINDOW_SIZE          | 30       |\n",
    "| batch_size           | 64       |\n",
    "| cf_drop              | 0.1      |\n",
    "| cf_filters           | 48       |\n",
    "| cf_kernel            | 3        |\n",
    "| cf_l2                | 0.000040 |\n",
    "| cf_pool              | 3        |\n",
    "| dense_units          | 32       |\n",
    "| dropout_rate         | 0.377332 |\n",
    "| gru1_units           | 160      |\n",
    "| gru2_units           | 0        |\n",
    "| l2                   | 0.000056 |\n",
    "| learning_rate        | 0.000357 |\n",
    "\n",
    "🎯 성능\n",
    "\n",
    "Macro F1 = 0.698\n",
    "\n",
    "전체 50개 중 5위 → 상위권 조합\n",
    "\n",
    "⚙️ 모델 구조 및 크기\n",
    "\n",
    "GRU 1층만 사용 (gru2_units = 0) → 모델 가볍고 단순\n",
    "\n",
    "Dense 층 = 32 유닛 → 매우 작은 Fully Connected 구조\n",
    "\n",
    "Conv 블록: 필터 48개, 커널 3 → 계산량 크지 않음\n",
    "\n",
    "🛡️ Regularization\n",
    "\n",
    "Dropout = 0.38\n",
    "→ 다소 높지만, regularization 효과로 성능 안정성 기여 가능\n",
    "\n",
    "⏱️ Latency & 효율성\n",
    "\n",
    "Window Size = 30, Stride = 3\n",
    "\n",
    "윈도우가 짧아 지연(latency) 적음\n",
    "\n",
    "Stride가 짧아 데이터 중복 활용 ↑\n",
    "\n",
    "(실시간 적용 시 Latency vs. Redundancy 트레이드오프 고려 필요)\n",
    "\n",
    "⚡ 리소스 사용\n",
    "\n",
    "추론 속도: GRU 1층 160 유닛 + Conv 48필터 → 상위권 조합 중 성능 대비 가벼움\n",
    "\n",
    "학습 시간: 약 2분 37초 → 다른 Trial 대비 짧은 편\n",
    "\n",
    "👉 가벼운 모델 구조 + 안정적 regularization + 효율적 Conv/GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edccd781",
   "metadata": {},
   "source": [
    "📈 Optuna 탐색 결과 – 하이퍼파라미터와 성능(F1) 간 상관 분석\n",
    "1. 강한 / 유의한 양(+)의 상관\n",
    "\n",
    "params_cf_filters (r = 0.51, p < 0.001)\n",
    "→ Conv 필터 개수가 많을수록 F1 성능이 유의하게 증가하는 경향.\n",
    "\n",
    "params_WINDOW_SIZE (r = 0.49, p < 0.001)\n",
    "→ 입력 윈도우 길이가 길수록 성능이 향상되는 경향.\n",
    "\n",
    "params_learning_rate (r = 0.46, p < 0.001)\n",
    "→ 비교적 큰 학습률이 오히려 좋은 성능을 보였음.\n",
    "(단, 지나치게 크면 학습 불안정 위험 존재)\n",
    "\n",
    "2. 강한 / 유의한 음(–)의 상관\n",
    "\n",
    "params_gru2_units (r = –0.46, p < 0.001)\n",
    "→ 두 번째 GRU 층의 유닛 수가 많을수록 성능이 감소하는 경향.\n",
    "\n",
    "params_cf_kernel (r = –0.38, p = 0.006)\n",
    "→ Conv 커널 크기가 커질수록 성능이 하락.\n",
    "(→ 짧은 kernel이 더 적합)\n",
    "\n",
    "params_gru1_units (r = –0.31, p = 0.026)\n",
    "→ 첫 번째 GRU 층 유닛 수가 많아도 성능이 저하되는 경향.\n",
    "\n",
    "params_dense_units (r = –0.29, p = 0.041)\n",
    "→ Dense 층 유닛 수가 많을수록 오히려 성능이 감소.\n",
    "\n",
    "3. 약하거나 통계적으로 유의하지 않은 상관\n",
    "\n",
    "params_STRIDE (r = –0.26, p ≈ 0.06)\n",
    "→ 약한 음의 관계, 통계적으로는 경계선 수준.\n",
    "\n",
    "그 외(batch_size, dropout_rate, l2, cf_drop, cf_pool)\n",
    "→ p > 0.05 → 통계적으로 유의하지 않음, 뚜렷한 상관관계 없음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e6287ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# corr_df는 이미 만들어진 상태 (Parameter, Pearson_r, p_value, abs_r)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(\n",
    "    corr_df.set_index(\"Parameter\")[[\"Pearson_r\"]],\n",
    "    annot=True, fmt=\".2f\", cmap=\"coolwarm\", center=0, cbar_kws={\"label\": \"Pearson r\"}\n",
    ")\n",
    "plt.title(\"하이퍼파라미터 vs 성능(F1) 상관계수 히트맵\", fontsize=14)\n",
    "plt.ylabel(\"Parameter\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39880adc",
   "metadata": {},
   "source": [
    "# # 최종 검증 및 분석 (학습 곡선 포함)\n",
    "\n",
    "🚀 최종 모델 성능 평가 및 베이스라인 비교\n",
    "1️⃣ 최적화된 설정 반영\n",
    "\n",
    "Optuna 탐색 결과에서 선택된 Trial 파라미터를 반영해 최종 config 구성\n",
    "\n",
    "안전 조치:\n",
    "\n",
    "paths.FEATURES_PATH 자동 보정\n",
    "\n",
    "출력 디렉토리 outputs/step4_final_loso 생성 및 저장\n",
    "\n",
    "pipeline.run_pipeline() 실행으로 전체 Fold LOSO 교차 검증 수행\n",
    "\n",
    "2️⃣ 성능 비교 (Baseline vs Optimized)\n",
    "\n",
    "평가 지표: Macro F1\n",
    "\n",
    "Fold별 점수를 추출 후 페어링하여 통계 검정 수행:\n",
    "\n",
    "대응표본 t-test\n",
    "\n",
    "Wilcoxon signed-rank test\n",
    "\n",
    "효과 크기: Cohen’s dz\n",
    "\n",
    "부트스트랩 95% 신뢰구간 계산 (평균 차이)\n",
    "\n",
    "3️⃣ 주요 출력\n",
    "\n",
    "평균 F1 점수: Baseline vs Optimized\n",
    "\n",
    "통계 검정 결과\n",
    "\n",
    "t, p-value\n",
    "\n",
    "Wilcoxon W, p-value (가능할 경우)\n",
    "\n",
    "효과 크기(Cohen’s dz)\n",
    "\n",
    "평균 차이 및 CI\n",
    "\n",
    "4️⃣ 시각화\n",
    "\n",
    "📦 Boxplot + Stripplot: 두 모델 분포 비교\n",
    "\n",
    "🔗 폴드별 연결 플롯: Baseline → Optimized 변화 추세 확인\n",
    "\n",
    "🔲 혼동 행렬 비교: 전체 Fold raw_predictions 합산 후, Heatmap으로 시각화\n",
    "\n",
    "📈 학습 곡선: aggregated_training_history.png 표시 (모든 Fold 평균)\n",
    "\n",
    "5️⃣ 결론\n",
    "\n",
    "p-value < 0.05라면 → 최적화 모델이 베이스라인 대비 유의미하게 향상됨 ✅\n",
    "\n",
    "p-value ≥ 0.05라면 → 성능 차이는 있으나 통계적으로 유의하지 않음 ➖\n",
    "\n",
    "📌 참고\n",
    "\n",
    "결과 파일 경로\n",
    "\n",
    "Baseline: outputs/step1_loso_test/models/final_loso_cv_report.csv\n",
    "\n",
    "Optimized: outputs/step4_final_loso/models/final_loso_cv_report.csv\n",
    "\n",
    "혼동 행렬: 각 Fold별 raw_predictions.npz를 찾아 합산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "71cff3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 선택된 트라이얼 파라미터로 진행합니다.\n",
      "   SELECT_BY = rank | trial number = 35 | rank = 6\n",
      "   F1(value) = 0.6980669169985\n",
      "최종 최적화된 설정으로 전체 LOSO 교차 검증을 시작합니다. 시간이 많이 소요됩니다...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\n",
      "Epoch 1: val_loss improved from None to 0.89943, saving model to outputs/step4_final_loso\\models\\fold_1\\best_model.keras\n",
      "571/571 - 11s - 20ms/step - accuracy: 0.4709 - loss: 1.6065 - val_accuracy: 0.7577 - val_loss: 0.8994 - learning_rate: 3.5709e-04\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 2: val_loss improved from 0.89943 to 0.58816, saving model to outputs/step4_final_loso\\models\\fold_1\\best_model.keras\n",
      "571/571 - 10s - 17ms/step - accuracy: 0.7366 - loss: 0.8137 - val_accuracy: 0.8436 - val_loss: 0.5882 - learning_rate: 3.5709e-04\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 3: val_loss improved from 0.58816 to 0.46924, saving model to outputs/step4_final_loso\\models\\fold_1\\best_model.keras\n",
      "571/571 - 10s - 17ms/step - accuracy: 0.8193 - loss: 0.5690 - val_accuracy: 0.8934 - val_loss: 0.4692 - learning_rate: 3.5709e-04\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 4: val_loss improved from 0.46924 to 0.41333, saving model to outputs/step4_final_loso\\models\\fold_1\\best_model.keras\n",
      "571/571 - 10s - 17ms/step - accuracy: 0.8606 - loss: 0.4403 - val_accuracy: 0.9111 - val_loss: 0.4133 - learning_rate: 3.5709e-04\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.41333\n",
      "571/571 - 10s - 17ms/step - accuracy: 0.8936 - loss: 0.3537 - val_accuracy: 0.9212 - val_loss: 0.4304 - learning_rate: 3.5709e-04\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.41333\n",
      "571/571 - 10s - 17ms/step - accuracy: 0.9063 - loss: 0.3124 - val_accuracy: 0.9206 - val_loss: 0.4633 - learning_rate: 3.5709e-04\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 7: val_loss improved from 0.41333 to 0.38422, saving model to outputs/step4_final_loso\\models\\fold_1\\best_model.keras\n",
      "571/571 - 10s - 17ms/step - accuracy: 0.9199 - loss: 0.2726 - val_accuracy: 0.9380 - val_loss: 0.3842 - learning_rate: 3.5709e-04\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.38422\n",
      "571/571 - 10s - 17ms/step - accuracy: 0.9313 - loss: 0.2432 - val_accuracy: 0.9214 - val_loss: 0.4442 - learning_rate: 3.5709e-04\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.38422\n",
      "571/571 - 10s - 17ms/step - accuracy: 0.9381 - loss: 0.2194 - val_accuracy: 0.9153 - val_loss: 0.4886 - learning_rate: 3.5709e-04\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.38422\n",
      "571/571 - 10s - 17ms/step - accuracy: 0.9444 - loss: 0.2020 - val_accuracy: 0.9299 - val_loss: 0.4451 - learning_rate: 3.5709e-04\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.38422\n",
      "571/571 - 10s - 17ms/step - accuracy: 0.9499 - loss: 0.1824 - val_accuracy: 0.9416 - val_loss: 0.4258 - learning_rate: 3.5709e-04\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.38422\n",
      "571/571 - 10s - 17ms/step - accuracy: 0.9534 - loss: 0.1727 - val_accuracy: 0.9419 - val_loss: 0.4313 - learning_rate: 3.5709e-04\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.38422\n",
      "571/571 - 10s - 17ms/step - accuracy: 0.9581 - loss: 0.1580 - val_accuracy: 0.9298 - val_loss: 0.5196 - learning_rate: 3.5709e-04\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.38422\n",
      "571/571 - 10s - 17ms/step - accuracy: 0.9591 - loss: 0.1526 - val_accuracy: 0.9444 - val_loss: 0.4521 - learning_rate: 3.5709e-04\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.38422\n",
      "571/571 - 10s - 17ms/step - accuracy: 0.9636 - loss: 0.1382 - val_accuracy: 0.9601 - val_loss: 0.3983 - learning_rate: 3.5709e-04\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.38422\n",
      "571/571 - 10s - 18ms/step - accuracy: 0.9654 - loss: 0.1330 - val_accuracy: 0.9515 - val_loss: 0.4318 - learning_rate: 3.5709e-04\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.00017854664474725723.\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.38422\n",
      "571/571 - 10s - 17ms/step - accuracy: 0.9675 - loss: 0.1260 - val_accuracy: 0.9313 - val_loss: 0.5376 - learning_rate: 3.5709e-04\n",
      "Epoch 17: early stopping\n",
      "Restoring model weights from the end of the best epoch: 7.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\n",
      "Epoch 1: val_loss improved from None to 0.84215, saving model to outputs/step4_final_loso\\models\\fold_2\\best_model.keras\n",
      "571/571 - 11s - 20ms/step - accuracy: 0.4746 - loss: 1.5905 - val_accuracy: 0.7581 - val_loss: 0.8421 - learning_rate: 3.5709e-04\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 2: val_loss improved from 0.84215 to 0.57063, saving model to outputs/step4_final_loso\\models\\fold_2\\best_model.keras\n",
      "571/571 - 10s - 17ms/step - accuracy: 0.7400 - loss: 0.8024 - val_accuracy: 0.8582 - val_loss: 0.5706 - learning_rate: 3.5709e-04\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 3: val_loss improved from 0.57063 to 0.45458, saving model to outputs/step4_final_loso\\models\\fold_2\\best_model.keras\n",
      "571/571 - 10s - 17ms/step - accuracy: 0.8233 - loss: 0.5559 - val_accuracy: 0.8976 - val_loss: 0.4546 - learning_rate: 3.5709e-04\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 4: val_loss improved from 0.45458 to 0.44547, saving model to outputs/step4_final_loso\\models\\fold_2\\best_model.keras\n",
      "571/571 - 10s - 17ms/step - accuracy: 0.8667 - loss: 0.4272 - val_accuracy: 0.9032 - val_loss: 0.4455 - learning_rate: 3.5709e-04\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.44547\n",
      "571/571 - 10s - 17ms/step - accuracy: 0.8946 - loss: 0.3455 - val_accuracy: 0.9102 - val_loss: 0.4786 - learning_rate: 3.5709e-04\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.44547\n",
      "571/571 - 10s - 17ms/step - accuracy: 0.9130 - loss: 0.2946 - val_accuracy: 0.9099 - val_loss: 0.4887 - learning_rate: 3.5709e-04\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 7: val_loss improved from 0.44547 to 0.36811, saving model to outputs/step4_final_loso\\models\\fold_2\\best_model.keras\n",
      "571/571 - 10s - 17ms/step - accuracy: 0.9225 - loss: 0.2585 - val_accuracy: 0.9386 - val_loss: 0.3681 - learning_rate: 3.5709e-04\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.36811\n",
      "571/571 - 10s - 17ms/step - accuracy: 0.9347 - loss: 0.2325 - val_accuracy: 0.9215 - val_loss: 0.4064 - learning_rate: 3.5709e-04\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.36811\n",
      "571/571 - 10s - 17ms/step - accuracy: 0.9385 - loss: 0.2135 - val_accuracy: 0.9330 - val_loss: 0.4323 - learning_rate: 3.5709e-04\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.36811\n",
      "571/571 - 11s - 19ms/step - accuracy: 0.9460 - loss: 0.1926 - val_accuracy: 0.9326 - val_loss: 0.4221 - learning_rate: 3.5709e-04\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.36811\n",
      "571/571 - 11s - 20ms/step - accuracy: 0.9496 - loss: 0.1809 - val_accuracy: 0.9512 - val_loss: 0.3705 - learning_rate: 3.5709e-04\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.36811\n",
      "571/571 - 11s - 20ms/step - accuracy: 0.9541 - loss: 0.1678 - val_accuracy: 0.9439 - val_loss: 0.4272 - learning_rate: 3.5709e-04\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.36811\n",
      "571/571 - 12s - 20ms/step - accuracy: 0.9575 - loss: 0.1614 - val_accuracy: 0.9456 - val_loss: 0.4489 - learning_rate: 3.5709e-04\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.36811\n",
      "571/571 - 10s - 18ms/step - accuracy: 0.9581 - loss: 0.1537 - val_accuracy: 0.9413 - val_loss: 0.4436 - learning_rate: 3.5709e-04\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.36811\n",
      "571/571 - 10s - 18ms/step - accuracy: 0.9628 - loss: 0.1408 - val_accuracy: 0.9228 - val_loss: 0.5403 - learning_rate: 3.5709e-04\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.36811\n",
      "571/571 - 10s - 18ms/step - accuracy: 0.9650 - loss: 0.1309 - val_accuracy: 0.9478 - val_loss: 0.4257 - learning_rate: 3.5709e-04\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.00017854664474725723.\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.36811\n",
      "571/571 - 10s - 18ms/step - accuracy: 0.9654 - loss: 0.1337 - val_accuracy: 0.9424 - val_loss: 0.4258 - learning_rate: 3.5709e-04\n",
      "Epoch 17: early stopping\n",
      "Restoring model weights from the end of the best epoch: 7.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\n",
      "Epoch 1: val_loss improved from None to 0.72026, saving model to outputs/step4_final_loso\\models\\fold_3\\best_model.keras\n",
      "584/584 - 12s - 21ms/step - accuracy: 0.4827 - loss: 1.5916 - val_accuracy: 0.7744 - val_loss: 0.7203 - learning_rate: 3.5709e-04\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 2: val_loss improved from 0.72026 to 0.48188, saving model to outputs/step4_final_loso\\models\\fold_3\\best_model.keras\n",
      "584/584 - 11s - 18ms/step - accuracy: 0.7411 - loss: 0.8168 - val_accuracy: 0.8323 - val_loss: 0.4819 - learning_rate: 3.5709e-04\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 3: val_loss improved from 0.48188 to 0.43518, saving model to outputs/step4_final_loso\\models\\fold_3\\best_model.keras\n",
      "584/584 - 11s - 19ms/step - accuracy: 0.8262 - loss: 0.5524 - val_accuracy: 0.8727 - val_loss: 0.4352 - learning_rate: 3.5709e-04\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 4: val_loss improved from 0.43518 to 0.35972, saving model to outputs/step4_final_loso\\models\\fold_3\\best_model.keras\n",
      "584/584 - 11s - 18ms/step - accuracy: 0.8666 - loss: 0.4348 - val_accuracy: 0.8959 - val_loss: 0.3597 - learning_rate: 3.5709e-04\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.35972\n",
      "584/584 - 11s - 19ms/step - accuracy: 0.8907 - loss: 0.3637 - val_accuracy: 0.9071 - val_loss: 0.3928 - learning_rate: 3.5709e-04\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 6: val_loss improved from 0.35972 to 0.35035, saving model to outputs/step4_final_loso\\models\\fold_3\\best_model.keras\n",
      "584/584 - 11s - 19ms/step - accuracy: 0.9099 - loss: 0.3069 - val_accuracy: 0.9274 - val_loss: 0.3503 - learning_rate: 3.5709e-04\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.35035\n",
      "584/584 - 11s - 19ms/step - accuracy: 0.9209 - loss: 0.2730 - val_accuracy: 0.9215 - val_loss: 0.3634 - learning_rate: 3.5709e-04\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.35035\n",
      "584/584 - 11s - 19ms/step - accuracy: 0.9311 - loss: 0.2469 - val_accuracy: 0.9241 - val_loss: 0.3844 - learning_rate: 3.5709e-04\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.35035\n",
      "584/584 - 11s - 19ms/step - accuracy: 0.9388 - loss: 0.2163 - val_accuracy: 0.9309 - val_loss: 0.3576 - learning_rate: 3.5709e-04\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.35035\n",
      "584/584 - 11s - 19ms/step - accuracy: 0.9421 - loss: 0.2063 - val_accuracy: 0.9246 - val_loss: 0.4435 - learning_rate: 3.5709e-04\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.35035\n",
      "584/584 - 12s - 20ms/step - accuracy: 0.9501 - loss: 0.1860 - val_accuracy: 0.9241 - val_loss: 0.3770 - learning_rate: 3.5709e-04\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.35035\n",
      "584/584 - 12s - 20ms/step - accuracy: 0.9563 - loss: 0.1690 - val_accuracy: 0.9249 - val_loss: 0.4112 - learning_rate: 3.5709e-04\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.35035\n",
      "584/584 - 12s - 20ms/step - accuracy: 0.9567 - loss: 0.1636 - val_accuracy: 0.9240 - val_loss: 0.4525 - learning_rate: 3.5709e-04\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.35035\n",
      "584/584 - 11s - 18ms/step - accuracy: 0.9607 - loss: 0.1532 - val_accuracy: 0.9364 - val_loss: 0.3611 - learning_rate: 3.5709e-04\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.35035\n",
      "584/584 - 11s - 18ms/step - accuracy: 0.9606 - loss: 0.1502 - val_accuracy: 0.9294 - val_loss: 0.4104 - learning_rate: 3.5709e-04\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.00017854664474725723.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.35035\n",
      "584/584 - 11s - 18ms/step - accuracy: 0.9651 - loss: 0.1349 - val_accuracy: 0.9332 - val_loss: 0.4199 - learning_rate: 3.5709e-04\n",
      "Epoch 16: early stopping\n",
      "Restoring model weights from the end of the best epoch: 6.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\n",
      "Epoch 1: val_loss improved from None to 0.79095, saving model to outputs/step4_final_loso\\models\\fold_4\\best_model.keras\n",
      "591/591 - 13s - 21ms/step - accuracy: 0.4947 - loss: 1.5475 - val_accuracy: 0.7809 - val_loss: 0.7910 - learning_rate: 3.5709e-04\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 2: val_loss improved from 0.79095 to 0.51211, saving model to outputs/step4_final_loso\\models\\fold_4\\best_model.keras\n",
      "591/591 - 11s - 19ms/step - accuracy: 0.7654 - loss: 0.7480 - val_accuracy: 0.8302 - val_loss: 0.5121 - learning_rate: 3.5709e-04\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 3: val_loss improved from 0.51211 to 0.48414, saving model to outputs/step4_final_loso\\models\\fold_4\\best_model.keras\n",
      "591/591 - 11s - 18ms/step - accuracy: 0.8457 - loss: 0.5042 - val_accuracy: 0.8515 - val_loss: 0.4841 - learning_rate: 3.5709e-04\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 4: val_loss improved from 0.48414 to 0.36929, saving model to outputs/step4_final_loso\\models\\fold_4\\best_model.keras\n",
      "591/591 - 11s - 19ms/step - accuracy: 0.8832 - loss: 0.3898 - val_accuracy: 0.8856 - val_loss: 0.3693 - learning_rate: 3.5709e-04\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 5: val_loss improved from 0.36929 to 0.35341, saving model to outputs/step4_final_loso\\models\\fold_4\\best_model.keras\n",
      "591/591 - 11s - 19ms/step - accuracy: 0.9030 - loss: 0.3290 - val_accuracy: 0.8932 - val_loss: 0.3534 - learning_rate: 3.5709e-04\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 6: val_loss improved from 0.35341 to 0.34331, saving model to outputs/step4_final_loso\\models\\fold_4\\best_model.keras\n",
      "591/591 - 12s - 20ms/step - accuracy: 0.9213 - loss: 0.2761 - val_accuracy: 0.8945 - val_loss: 0.3433 - learning_rate: 3.5709e-04\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.34331\n",
      "591/591 - 12s - 20ms/step - accuracy: 0.9309 - loss: 0.2486 - val_accuracy: 0.8963 - val_loss: 0.3612 - learning_rate: 3.5709e-04\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 8: val_loss improved from 0.34331 to 0.32932, saving model to outputs/step4_final_loso\\models\\fold_4\\best_model.keras\n",
      "591/591 - 12s - 20ms/step - accuracy: 0.9376 - loss: 0.2265 - val_accuracy: 0.8995 - val_loss: 0.3293 - learning_rate: 3.5709e-04\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 9: val_loss improved from 0.32932 to 0.31804, saving model to outputs/step4_final_loso\\models\\fold_4\\best_model.keras\n",
      "591/591 - 12s - 20ms/step - accuracy: 0.9437 - loss: 0.2064 - val_accuracy: 0.9165 - val_loss: 0.3180 - learning_rate: 3.5709e-04\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 10: val_loss improved from 0.31804 to 0.28248, saving model to outputs/step4_final_loso\\models\\fold_4\\best_model.keras\n",
      "591/591 - 11s - 19ms/step - accuracy: 0.9482 - loss: 0.1895 - val_accuracy: 0.9268 - val_loss: 0.2825 - learning_rate: 3.5709e-04\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.28248\n",
      "591/591 - 11s - 19ms/step - accuracy: 0.9533 - loss: 0.1779 - val_accuracy: 0.9070 - val_loss: 0.3389 - learning_rate: 3.5709e-04\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.28248\n",
      "591/591 - 12s - 20ms/step - accuracy: 0.9560 - loss: 0.1663 - val_accuracy: 0.9260 - val_loss: 0.3146 - learning_rate: 3.5709e-04\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.28248\n",
      "591/591 - 11s - 19ms/step - accuracy: 0.9592 - loss: 0.1557 - val_accuracy: 0.9299 - val_loss: 0.2980 - learning_rate: 3.5709e-04\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 14: val_loss improved from 0.28248 to 0.23340, saving model to outputs/step4_final_loso\\models\\fold_4\\best_model.keras\n",
      "591/591 - 10s - 18ms/step - accuracy: 0.9610 - loss: 0.1487 - val_accuracy: 0.9446 - val_loss: 0.2334 - learning_rate: 3.5709e-04\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.23340\n",
      "591/591 - 11s - 18ms/step - accuracy: 0.9635 - loss: 0.1438 - val_accuracy: 0.9433 - val_loss: 0.2413 - learning_rate: 3.5709e-04\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.23340\n",
      "591/591 - 11s - 18ms/step - accuracy: 0.9665 - loss: 0.1325 - val_accuracy: 0.9286 - val_loss: 0.2951 - learning_rate: 3.5709e-04\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.23340\n",
      "591/591 - 11s - 19ms/step - accuracy: 0.9673 - loss: 0.1320 - val_accuracy: 0.9321 - val_loss: 0.2874 - learning_rate: 3.5709e-04\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.23340\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9694 - loss: 0.1236 - val_accuracy: 0.9231 - val_loss: 0.3635 - learning_rate: 3.5709e-04\n",
      "Epoch 19/100\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.23340\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9686 - loss: 0.1235 - val_accuracy: 0.9369 - val_loss: 0.2664 - learning_rate: 3.5709e-04\n",
      "Epoch 20/100\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.23340\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9732 - loss: 0.1095 - val_accuracy: 0.9410 - val_loss: 0.2892 - learning_rate: 3.5709e-04\n",
      "Epoch 21/100\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.23340\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9729 - loss: 0.1119 - val_accuracy: 0.9437 - val_loss: 0.2756 - learning_rate: 3.5709e-04\n",
      "Epoch 22/100\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.23340\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9760 - loss: 0.1051 - val_accuracy: 0.9434 - val_loss: 0.2732 - learning_rate: 3.5709e-04\n",
      "Epoch 23/100\n",
      "\n",
      "Epoch 23: val_loss improved from 0.23340 to 0.22317, saving model to outputs/step4_final_loso\\models\\fold_4\\best_model.keras\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9743 - loss: 0.1050 - val_accuracy: 0.9550 - val_loss: 0.2232 - learning_rate: 3.5709e-04\n",
      "Epoch 24/100\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.22317\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9756 - loss: 0.1057 - val_accuracy: 0.9456 - val_loss: 0.2755 - learning_rate: 3.5709e-04\n",
      "Epoch 25/100\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.22317\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9779 - loss: 0.0951 - val_accuracy: 0.9352 - val_loss: 0.3216 - learning_rate: 3.5709e-04\n",
      "Epoch 26/100\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.22317\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9778 - loss: 0.0937 - val_accuracy: 0.9498 - val_loss: 0.2432 - learning_rate: 3.5709e-04\n",
      "Epoch 27/100\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.22317\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9791 - loss: 0.0909 - val_accuracy: 0.9608 - val_loss: 0.2233 - learning_rate: 3.5709e-04\n",
      "Epoch 28/100\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.22317\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9790 - loss: 0.0906 - val_accuracy: 0.9474 - val_loss: 0.2654 - learning_rate: 3.5709e-04\n",
      "Epoch 29/100\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.22317\n",
      "591/591 - 12s - 20ms/step - accuracy: 0.9794 - loss: 0.0892 - val_accuracy: 0.9541 - val_loss: 0.2436 - learning_rate: 3.5709e-04\n",
      "Epoch 30/100\n",
      "\n",
      "Epoch 30: val_loss improved from 0.22317 to 0.20308, saving model to outputs/step4_final_loso\\models\\fold_4\\best_model.keras\n",
      "591/591 - 12s - 20ms/step - accuracy: 0.9807 - loss: 0.0860 - val_accuracy: 0.9606 - val_loss: 0.2031 - learning_rate: 3.5709e-04\n",
      "Epoch 31/100\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.20308\n",
      "591/591 - 11s - 19ms/step - accuracy: 0.9809 - loss: 0.0846 - val_accuracy: 0.9470 - val_loss: 0.2663 - learning_rate: 3.5709e-04\n",
      "Epoch 32/100\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.20308\n",
      "591/591 - 11s - 19ms/step - accuracy: 0.9839 - loss: 0.0758 - val_accuracy: 0.9461 - val_loss: 0.2797 - learning_rate: 3.5709e-04\n",
      "Epoch 33/100\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.20308\n",
      "591/591 - 11s - 18ms/step - accuracy: 0.9825 - loss: 0.0778 - val_accuracy: 0.9501 - val_loss: 0.2689 - learning_rate: 3.5709e-04\n",
      "Epoch 34/100\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.20308\n",
      "591/591 - 11s - 18ms/step - accuracy: 0.9818 - loss: 0.0799 - val_accuracy: 0.9584 - val_loss: 0.2273 - learning_rate: 3.5709e-04\n",
      "Epoch 35/100\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.20308\n",
      "591/591 - 11s - 18ms/step - accuracy: 0.9839 - loss: 0.0745 - val_accuracy: 0.9517 - val_loss: 0.2921 - learning_rate: 3.5709e-04\n",
      "Epoch 36/100\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.20308\n",
      "591/591 - 11s - 18ms/step - accuracy: 0.9833 - loss: 0.0774 - val_accuracy: 0.9526 - val_loss: 0.2669 - learning_rate: 3.5709e-04\n",
      "Epoch 37/100\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.20308\n",
      "591/591 - 11s - 19ms/step - accuracy: 0.9843 - loss: 0.0710 - val_accuracy: 0.9488 - val_loss: 0.3152 - learning_rate: 3.5709e-04\n",
      "Epoch 38/100\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.20308\n",
      "591/591 - 12s - 20ms/step - accuracy: 0.9857 - loss: 0.0693 - val_accuracy: 0.9477 - val_loss: 0.2845 - learning_rate: 3.5709e-04\n",
      "Epoch 39/100\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.20308\n",
      "591/591 - 11s - 19ms/step - accuracy: 0.9847 - loss: 0.0726 - val_accuracy: 0.9501 - val_loss: 0.2845 - learning_rate: 3.5709e-04\n",
      "Epoch 40/100\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 0.00017854664474725723.\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.20308\n",
      "591/591 - 12s - 20ms/step - accuracy: 0.9861 - loss: 0.0646 - val_accuracy: 0.9431 - val_loss: 0.3296 - learning_rate: 3.5709e-04\n",
      "Epoch 40: early stopping\n",
      "Restoring model weights from the end of the best epoch: 30.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\n",
      "Epoch 1: val_loss improved from None to 0.83198, saving model to outputs/step4_final_loso\\models\\fold_5\\best_model.keras\n",
      "591/591 - 14s - 23ms/step - accuracy: 0.4945 - loss: 1.5477 - val_accuracy: 0.7401 - val_loss: 0.8320 - learning_rate: 3.5709e-04\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 2: val_loss improved from 0.83198 to 0.57316, saving model to outputs/step4_final_loso\\models\\fold_5\\best_model.keras\n",
      "591/591 - 11s - 18ms/step - accuracy: 0.7612 - loss: 0.7551 - val_accuracy: 0.8109 - val_loss: 0.5732 - learning_rate: 3.5709e-04\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 3: val_loss improved from 0.57316 to 0.54627, saving model to outputs/step4_final_loso\\models\\fold_5\\best_model.keras\n",
      "591/591 - 11s - 18ms/step - accuracy: 0.8459 - loss: 0.5029 - val_accuracy: 0.8338 - val_loss: 0.5463 - learning_rate: 3.5709e-04\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 4: val_loss improved from 0.54627 to 0.41240, saving model to outputs/step4_final_loso\\models\\fold_5\\best_model.keras\n",
      "591/591 - 12s - 20ms/step - accuracy: 0.8839 - loss: 0.3906 - val_accuracy: 0.8739 - val_loss: 0.4124 - learning_rate: 3.5709e-04\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 5: val_loss improved from 0.41240 to 0.38490, saving model to outputs/step4_final_loso\\models\\fold_5\\best_model.keras\n",
      "591/591 - 12s - 21ms/step - accuracy: 0.9021 - loss: 0.3275 - val_accuracy: 0.8816 - val_loss: 0.3849 - learning_rate: 3.5709e-04\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 6: val_loss improved from 0.38490 to 0.37129, saving model to outputs/step4_final_loso\\models\\fold_5\\best_model.keras\n",
      "591/591 - 12s - 20ms/step - accuracy: 0.9203 - loss: 0.2756 - val_accuracy: 0.8920 - val_loss: 0.3713 - learning_rate: 3.5709e-04\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.37129\n",
      "591/591 - 12s - 20ms/step - accuracy: 0.9293 - loss: 0.2543 - val_accuracy: 0.8746 - val_loss: 0.4301 - learning_rate: 3.5709e-04\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 8: val_loss improved from 0.37129 to 0.35839, saving model to outputs/step4_final_loso\\models\\fold_5\\best_model.keras\n",
      "591/591 - 12s - 20ms/step - accuracy: 0.9366 - loss: 0.2265 - val_accuracy: 0.9015 - val_loss: 0.3584 - learning_rate: 3.5709e-04\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.35839\n",
      "591/591 - 11s - 18ms/step - accuracy: 0.9408 - loss: 0.2156 - val_accuracy: 0.9038 - val_loss: 0.3625 - learning_rate: 3.5709e-04\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 10: val_loss improved from 0.35839 to 0.34983, saving model to outputs/step4_final_loso\\models\\fold_5\\best_model.keras\n",
      "591/591 - 11s - 18ms/step - accuracy: 0.9484 - loss: 0.1919 - val_accuracy: 0.9107 - val_loss: 0.3498 - learning_rate: 3.5709e-04\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.34983\n",
      "591/591 - 11s - 19ms/step - accuracy: 0.9527 - loss: 0.1814 - val_accuracy: 0.9023 - val_loss: 0.3690 - learning_rate: 3.5709e-04\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 12: val_loss improved from 0.34983 to 0.33715, saving model to outputs/step4_final_loso\\models\\fold_5\\best_model.keras\n",
      "591/591 - 11s - 18ms/step - accuracy: 0.9539 - loss: 0.1719 - val_accuracy: 0.9130 - val_loss: 0.3371 - learning_rate: 3.5709e-04\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 13: val_loss improved from 0.33715 to 0.31080, saving model to outputs/step4_final_loso\\models\\fold_5\\best_model.keras\n",
      "591/591 - 11s - 18ms/step - accuracy: 0.9590 - loss: 0.1563 - val_accuracy: 0.9284 - val_loss: 0.3108 - learning_rate: 3.5709e-04\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.31080\n",
      "591/591 - 11s - 19ms/step - accuracy: 0.9626 - loss: 0.1450 - val_accuracy: 0.9150 - val_loss: 0.3665 - learning_rate: 3.5709e-04\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 15: val_loss improved from 0.31080 to 0.29711, saving model to outputs/step4_final_loso\\models\\fold_5\\best_model.keras\n",
      "591/591 - 11s - 19ms/step - accuracy: 0.9643 - loss: 0.1416 - val_accuracy: 0.9306 - val_loss: 0.2971 - learning_rate: 3.5709e-04\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.29711\n",
      "591/591 - 12s - 20ms/step - accuracy: 0.9647 - loss: 0.1394 - val_accuracy: 0.9208 - val_loss: 0.3358 - learning_rate: 3.5709e-04\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.29711\n",
      "591/591 - 11s - 18ms/step - accuracy: 0.9663 - loss: 0.1309 - val_accuracy: 0.9250 - val_loss: 0.3159 - learning_rate: 3.5709e-04\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.29711\n",
      "591/591 - 11s - 18ms/step - accuracy: 0.9679 - loss: 0.1263 - val_accuracy: 0.9094 - val_loss: 0.3885 - learning_rate: 3.5709e-04\n",
      "Epoch 19/100\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.29711\n",
      "591/591 - 11s - 18ms/step - accuracy: 0.9708 - loss: 0.1168 - val_accuracy: 0.9263 - val_loss: 0.3324 - learning_rate: 3.5709e-04\n",
      "Epoch 20/100\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.29711\n",
      "591/591 - 10s - 18ms/step - accuracy: 0.9706 - loss: 0.1166 - val_accuracy: 0.9256 - val_loss: 0.3469 - learning_rate: 3.5709e-04\n",
      "Epoch 21/100\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.29711\n",
      "591/591 - 10s - 18ms/step - accuracy: 0.9724 - loss: 0.1112 - val_accuracy: 0.9254 - val_loss: 0.3037 - learning_rate: 3.5709e-04\n",
      "Epoch 22/100\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.29711\n",
      "591/591 - 10s - 18ms/step - accuracy: 0.9739 - loss: 0.1088 - val_accuracy: 0.9361 - val_loss: 0.3537 - learning_rate: 3.5709e-04\n",
      "Epoch 23/100\n",
      "\n",
      "Epoch 23: val_loss improved from 0.29711 to 0.26114, saving model to outputs/step4_final_loso\\models\\fold_5\\best_model.keras\n",
      "591/591 - 11s - 18ms/step - accuracy: 0.9746 - loss: 0.1044 - val_accuracy: 0.9357 - val_loss: 0.2611 - learning_rate: 3.5709e-04\n",
      "Epoch 24/100\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.26114\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9763 - loss: 0.1022 - val_accuracy: 0.9259 - val_loss: 0.3415 - learning_rate: 3.5709e-04\n",
      "Epoch 25/100\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.26114\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9757 - loss: 0.1010 - val_accuracy: 0.9196 - val_loss: 0.3606 - learning_rate: 3.5709e-04\n",
      "Epoch 26/100\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.26114\n",
      "591/591 - 11s - 18ms/step - accuracy: 0.9780 - loss: 0.0937 - val_accuracy: 0.9381 - val_loss: 0.3149 - learning_rate: 3.5709e-04\n",
      "Epoch 27/100\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.26114\n",
      "591/591 - 11s - 19ms/step - accuracy: 0.9782 - loss: 0.0964 - val_accuracy: 0.9406 - val_loss: 0.2787 - learning_rate: 3.5709e-04\n",
      "Epoch 28/100\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.26114\n",
      "591/591 - 11s - 18ms/step - accuracy: 0.9785 - loss: 0.0905 - val_accuracy: 0.9367 - val_loss: 0.3394 - learning_rate: 3.5709e-04\n",
      "Epoch 29/100\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.26114\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9796 - loss: 0.0886 - val_accuracy: 0.9477 - val_loss: 0.2864 - learning_rate: 3.5709e-04\n",
      "Epoch 30/100\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.26114\n",
      "591/591 - 15s - 26ms/step - accuracy: 0.9805 - loss: 0.0866 - val_accuracy: 0.9367 - val_loss: 0.3309 - learning_rate: 3.5709e-04\n",
      "Epoch 31/100\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.26114\n",
      "591/591 - 16s - 27ms/step - accuracy: 0.9815 - loss: 0.0804 - val_accuracy: 0.9275 - val_loss: 0.4306 - learning_rate: 3.5709e-04\n",
      "Epoch 32/100\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.26114\n",
      "591/591 - 20s - 35ms/step - accuracy: 0.9830 - loss: 0.0798 - val_accuracy: 0.9473 - val_loss: 0.2801 - learning_rate: 3.5709e-04\n",
      "Epoch 33/100\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 0.00017854664474725723.\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.26114\n",
      "591/591 - 20s - 35ms/step - accuracy: 0.9802 - loss: 0.0856 - val_accuracy: 0.9407 - val_loss: 0.3316 - learning_rate: 3.5709e-04\n",
      "Epoch 33: early stopping\n",
      "Restoring model weights from the end of the best epoch: 23.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\n",
      "Epoch 1: val_loss improved from None to 0.76701, saving model to outputs/step4_final_loso\\models\\fold_6\\best_model.keras\n",
      "591/591 - 11s - 19ms/step - accuracy: 0.4932 - loss: 1.5519 - val_accuracy: 0.7567 - val_loss: 0.7670 - learning_rate: 3.5709e-04\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 2: val_loss improved from 0.76701 to 0.53760, saving model to outputs/step4_final_loso\\models\\fold_6\\best_model.keras\n",
      "591/591 - 11s - 18ms/step - accuracy: 0.7650 - loss: 0.7478 - val_accuracy: 0.8169 - val_loss: 0.5376 - learning_rate: 3.5709e-04\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 3: val_loss improved from 0.53760 to 0.49523, saving model to outputs/step4_final_loso\\models\\fold_6\\best_model.keras\n",
      "591/591 - 11s - 19ms/step - accuracy: 0.8457 - loss: 0.5069 - val_accuracy: 0.8390 - val_loss: 0.4952 - learning_rate: 3.5709e-04\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 4: val_loss improved from 0.49523 to 0.35385, saving model to outputs/step4_final_loso\\models\\fold_6\\best_model.keras\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.8798 - loss: 0.3979 - val_accuracy: 0.8899 - val_loss: 0.3539 - learning_rate: 3.5709e-04\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.35385\n",
      "591/591 - 10s - 18ms/step - accuracy: 0.9018 - loss: 0.3351 - val_accuracy: 0.8852 - val_loss: 0.3937 - learning_rate: 3.5709e-04\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.35385\n",
      "591/591 - 11s - 19ms/step - accuracy: 0.9187 - loss: 0.2829 - val_accuracy: 0.8923 - val_loss: 0.3785 - learning_rate: 3.5709e-04\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.35385\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9274 - loss: 0.2574 - val_accuracy: 0.8993 - val_loss: 0.3556 - learning_rate: 3.5709e-04\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.35385\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9346 - loss: 0.2347 - val_accuracy: 0.9042 - val_loss: 0.3577 - learning_rate: 3.5709e-04\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 9: val_loss improved from 0.35385 to 0.31731, saving model to outputs/step4_final_loso\\models\\fold_6\\best_model.keras\n",
      "591/591 - 11s - 19ms/step - accuracy: 0.9407 - loss: 0.2147 - val_accuracy: 0.9196 - val_loss: 0.3173 - learning_rate: 3.5709e-04\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.31731\n",
      "591/591 - 11s - 19ms/step - accuracy: 0.9470 - loss: 0.1988 - val_accuracy: 0.9139 - val_loss: 0.3271 - learning_rate: 3.5709e-04\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.31731\n",
      "591/591 - 11s - 19ms/step - accuracy: 0.9510 - loss: 0.1857 - val_accuracy: 0.9137 - val_loss: 0.3198 - learning_rate: 3.5709e-04\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.31731\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9539 - loss: 0.1718 - val_accuracy: 0.9091 - val_loss: 0.3726 - learning_rate: 3.5709e-04\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 13: val_loss improved from 0.31731 to 0.26220, saving model to outputs/step4_final_loso\\models\\fold_6\\best_model.keras\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9573 - loss: 0.1586 - val_accuracy: 0.9372 - val_loss: 0.2622 - learning_rate: 3.5709e-04\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.26220\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9600 - loss: 0.1511 - val_accuracy: 0.9329 - val_loss: 0.3069 - learning_rate: 3.5709e-04\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 15: val_loss improved from 0.26220 to 0.25663, saving model to outputs/step4_final_loso\\models\\fold_6\\best_model.keras\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9639 - loss: 0.1402 - val_accuracy: 0.9364 - val_loss: 0.2566 - learning_rate: 3.5709e-04\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.25663\n",
      "591/591 - 11s - 19ms/step - accuracy: 0.9664 - loss: 0.1358 - val_accuracy: 0.9415 - val_loss: 0.2619 - learning_rate: 3.5709e-04\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.25663\n",
      "591/591 - 11s - 19ms/step - accuracy: 0.9658 - loss: 0.1312 - val_accuracy: 0.9297 - val_loss: 0.3027 - learning_rate: 3.5709e-04\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.25663\n",
      "591/591 - 11s - 18ms/step - accuracy: 0.9685 - loss: 0.1268 - val_accuracy: 0.9280 - val_loss: 0.3296 - learning_rate: 3.5709e-04\n",
      "Epoch 19/100\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.25663\n",
      "591/591 - 11s - 19ms/step - accuracy: 0.9695 - loss: 0.1223 - val_accuracy: 0.9366 - val_loss: 0.3350 - learning_rate: 3.5709e-04\n",
      "Epoch 20/100\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.25663\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9712 - loss: 0.1158 - val_accuracy: 0.9418 - val_loss: 0.2614 - learning_rate: 3.5709e-04\n",
      "Epoch 21/100\n",
      "\n",
      "Epoch 21: val_loss improved from 0.25663 to 0.24752, saving model to outputs/step4_final_loso\\models\\fold_6\\best_model.keras\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9735 - loss: 0.1094 - val_accuracy: 0.9510 - val_loss: 0.2475 - learning_rate: 3.5709e-04\n",
      "Epoch 22/100\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.24752\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9738 - loss: 0.1071 - val_accuracy: 0.9413 - val_loss: 0.2793 - learning_rate: 3.5709e-04\n",
      "Epoch 23/100\n",
      "\n",
      "Epoch 23: val_loss improved from 0.24752 to 0.22858, saving model to outputs/step4_final_loso\\models\\fold_6\\best_model.keras\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9755 - loss: 0.1027 - val_accuracy: 0.9550 - val_loss: 0.2286 - learning_rate: 3.5709e-04\n",
      "Epoch 24/100\n",
      "\n",
      "Epoch 24: val_loss improved from 0.22858 to 0.22392, saving model to outputs/step4_final_loso\\models\\fold_6\\best_model.keras\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9755 - loss: 0.1027 - val_accuracy: 0.9520 - val_loss: 0.2239 - learning_rate: 3.5709e-04\n",
      "Epoch 25/100\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.22392\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9773 - loss: 0.0973 - val_accuracy: 0.9447 - val_loss: 0.2563 - learning_rate: 3.5709e-04\n",
      "Epoch 26/100\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.22392\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9793 - loss: 0.0916 - val_accuracy: 0.9513 - val_loss: 0.2716 - learning_rate: 3.5709e-04\n",
      "Epoch 27/100\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.22392\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9787 - loss: 0.0923 - val_accuracy: 0.9575 - val_loss: 0.2398 - learning_rate: 3.5709e-04\n",
      "Epoch 28/100\n",
      "\n",
      "Epoch 28: val_loss improved from 0.22392 to 0.22168, saving model to outputs/step4_final_loso\\models\\fold_6\\best_model.keras\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9781 - loss: 0.0918 - val_accuracy: 0.9534 - val_loss: 0.2217 - learning_rate: 3.5709e-04\n",
      "Epoch 29/100\n",
      "\n",
      "Epoch 29: val_loss improved from 0.22168 to 0.21271, saving model to outputs/step4_final_loso\\models\\fold_6\\best_model.keras\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9803 - loss: 0.0875 - val_accuracy: 0.9546 - val_loss: 0.2127 - learning_rate: 3.5709e-04\n",
      "Epoch 30/100\n",
      "\n",
      "Epoch 30: val_loss improved from 0.21271 to 0.20513, saving model to outputs/step4_final_loso\\models\\fold_6\\best_model.keras\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9808 - loss: 0.0862 - val_accuracy: 0.9626 - val_loss: 0.2051 - learning_rate: 3.5709e-04\n",
      "Epoch 31/100\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.20513\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9814 - loss: 0.0839 - val_accuracy: 0.9569 - val_loss: 0.2196 - learning_rate: 3.5709e-04\n",
      "Epoch 32/100\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.20513\n",
      "591/591 - 10s - 16ms/step - accuracy: 0.9814 - loss: 0.0824 - val_accuracy: 0.9516 - val_loss: 0.2450 - learning_rate: 3.5709e-04\n",
      "Epoch 33/100\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.20513\n",
      "591/591 - 10s - 16ms/step - accuracy: 0.9821 - loss: 0.0804 - val_accuracy: 0.9586 - val_loss: 0.2113 - learning_rate: 3.5709e-04\n",
      "Epoch 34/100\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.20513\n",
      "591/591 - 10s - 16ms/step - accuracy: 0.9829 - loss: 0.0771 - val_accuracy: 0.9587 - val_loss: 0.2072 - learning_rate: 3.5709e-04\n",
      "Epoch 35/100\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.20513\n",
      "591/591 - 10s - 16ms/step - accuracy: 0.9825 - loss: 0.0787 - val_accuracy: 0.9599 - val_loss: 0.2161 - learning_rate: 3.5709e-04\n",
      "Epoch 36/100\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.20513\n",
      "591/591 - 10s - 16ms/step - accuracy: 0.9837 - loss: 0.0746 - val_accuracy: 0.9453 - val_loss: 0.3184 - learning_rate: 3.5709e-04\n",
      "Epoch 37/100\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.20513\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9849 - loss: 0.0714 - val_accuracy: 0.9406 - val_loss: 0.3135 - learning_rate: 3.5709e-04\n",
      "Epoch 38/100\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.20513\n",
      "591/591 - 10s - 16ms/step - accuracy: 0.9849 - loss: 0.0711 - val_accuracy: 0.9453 - val_loss: 0.2768 - learning_rate: 3.5709e-04\n",
      "Epoch 39/100\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.20513\n",
      "591/591 - 10s - 16ms/step - accuracy: 0.9838 - loss: 0.0737 - val_accuracy: 0.9598 - val_loss: 0.2504 - learning_rate: 3.5709e-04\n",
      "Epoch 40/100\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 0.00017854664474725723.\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.20513\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9863 - loss: 0.0651 - val_accuracy: 0.9584 - val_loss: 0.2346 - learning_rate: 3.5709e-04\n",
      "Epoch 40: early stopping\n",
      "Restoring model weights from the end of the best epoch: 30.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\n",
      "Epoch 1: val_loss improved from None to 0.79609, saving model to outputs/step4_final_loso\\models\\fold_7\\best_model.keras\n",
      "591/591 - 11s - 19ms/step - accuracy: 0.5009 - loss: 1.5319 - val_accuracy: 0.7539 - val_loss: 0.7961 - learning_rate: 3.5709e-04\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 2: val_loss improved from 0.79609 to 0.47971, saving model to outputs/step4_final_loso\\models\\fold_7\\best_model.keras\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.7707 - loss: 0.7280 - val_accuracy: 0.8470 - val_loss: 0.4797 - learning_rate: 3.5709e-04\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 3: val_loss improved from 0.47971 to 0.46751, saving model to outputs/step4_final_loso\\models\\fold_7\\best_model.keras\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.8496 - loss: 0.4909 - val_accuracy: 0.8547 - val_loss: 0.4675 - learning_rate: 3.5709e-04\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 4: val_loss improved from 0.46751 to 0.33638, saving model to outputs/step4_final_loso\\models\\fold_7\\best_model.keras\n",
      "591/591 - 10s - 16ms/step - accuracy: 0.8846 - loss: 0.3850 - val_accuracy: 0.8987 - val_loss: 0.3364 - learning_rate: 3.5709e-04\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.33638\n",
      "591/591 - 10s - 16ms/step - accuracy: 0.9067 - loss: 0.3233 - val_accuracy: 0.8947 - val_loss: 0.3411 - learning_rate: 3.5709e-04\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 6: val_loss improved from 0.33638 to 0.30791, saving model to outputs/step4_final_loso\\models\\fold_7\\best_model.keras\n",
      "591/591 - 10s - 16ms/step - accuracy: 0.9220 - loss: 0.2704 - val_accuracy: 0.9149 - val_loss: 0.3079 - learning_rate: 3.5709e-04\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.30791\n",
      "591/591 - 10s - 16ms/step - accuracy: 0.9306 - loss: 0.2501 - val_accuracy: 0.9020 - val_loss: 0.3393 - learning_rate: 3.5709e-04\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 8: val_loss improved from 0.30791 to 0.30640, saving model to outputs/step4_final_loso\\models\\fold_7\\best_model.keras\n",
      "591/591 - 10s - 16ms/step - accuracy: 0.9385 - loss: 0.2227 - val_accuracy: 0.9116 - val_loss: 0.3064 - learning_rate: 3.5709e-04\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 9: val_loss improved from 0.30640 to 0.29507, saving model to outputs/step4_final_loso\\models\\fold_7\\best_model.keras\n",
      "591/591 - 10s - 16ms/step - accuracy: 0.9463 - loss: 0.2016 - val_accuracy: 0.9257 - val_loss: 0.2951 - learning_rate: 3.5709e-04\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.29507\n",
      "591/591 - 10s - 16ms/step - accuracy: 0.9496 - loss: 0.1875 - val_accuracy: 0.9188 - val_loss: 0.3021 - learning_rate: 3.5709e-04\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 11: val_loss improved from 0.29507 to 0.28989, saving model to outputs/step4_final_loso\\models\\fold_7\\best_model.keras\n",
      "591/591 - 10s - 16ms/step - accuracy: 0.9550 - loss: 0.1733 - val_accuracy: 0.9295 - val_loss: 0.2899 - learning_rate: 3.5709e-04\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 12: val_loss improved from 0.28989 to 0.28759, saving model to outputs/step4_final_loso\\models\\fold_7\\best_model.keras\n",
      "591/591 - 10s - 16ms/step - accuracy: 0.9559 - loss: 0.1688 - val_accuracy: 0.9246 - val_loss: 0.2876 - learning_rate: 3.5709e-04\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 13: val_loss improved from 0.28759 to 0.25363, saving model to outputs/step4_final_loso\\models\\fold_7\\best_model.keras\n",
      "591/591 - 10s - 16ms/step - accuracy: 0.9613 - loss: 0.1505 - val_accuracy: 0.9394 - val_loss: 0.2536 - learning_rate: 3.5709e-04\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.25363\n",
      "591/591 - 10s - 16ms/step - accuracy: 0.9636 - loss: 0.1394 - val_accuracy: 0.9352 - val_loss: 0.2699 - learning_rate: 3.5709e-04\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 15: val_loss improved from 0.25363 to 0.19772, saving model to outputs/step4_final_loso\\models\\fold_7\\best_model.keras\n",
      "591/591 - 11s - 18ms/step - accuracy: 0.9646 - loss: 0.1405 - val_accuracy: 0.9520 - val_loss: 0.1977 - learning_rate: 3.5709e-04\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.19772\n",
      "591/591 - 12s - 20ms/step - accuracy: 0.9697 - loss: 0.1279 - val_accuracy: 0.9364 - val_loss: 0.3015 - learning_rate: 3.5709e-04\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.19772\n",
      "591/591 - 11s - 19ms/step - accuracy: 0.9679 - loss: 0.1267 - val_accuracy: 0.9345 - val_loss: 0.2779 - learning_rate: 3.5709e-04\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.19772\n",
      "591/591 - 12s - 20ms/step - accuracy: 0.9693 - loss: 0.1242 - val_accuracy: 0.9456 - val_loss: 0.2411 - learning_rate: 3.5709e-04\n",
      "Epoch 19/100\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.19772\n",
      "591/591 - 12s - 20ms/step - accuracy: 0.9721 - loss: 0.1133 - val_accuracy: 0.9342 - val_loss: 0.3032 - learning_rate: 3.5709e-04\n",
      "Epoch 20/100\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.19772\n",
      "591/591 - 11s - 18ms/step - accuracy: 0.9739 - loss: 0.1064 - val_accuracy: 0.9467 - val_loss: 0.2620 - learning_rate: 3.5709e-04\n",
      "Epoch 21/100\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.19772\n",
      "591/591 - 10s - 18ms/step - accuracy: 0.9746 - loss: 0.1053 - val_accuracy: 0.9540 - val_loss: 0.2226 - learning_rate: 3.5709e-04\n",
      "Epoch 22/100\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.19772\n",
      "591/591 - 10s - 18ms/step - accuracy: 0.9744 - loss: 0.1074 - val_accuracy: 0.9327 - val_loss: 0.3488 - learning_rate: 3.5709e-04\n",
      "Epoch 23/100\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.19772\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9765 - loss: 0.0992 - val_accuracy: 0.9580 - val_loss: 0.2117 - learning_rate: 3.5709e-04\n",
      "Epoch 24/100\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.19772\n",
      "591/591 - 10s - 18ms/step - accuracy: 0.9787 - loss: 0.0964 - val_accuracy: 0.9510 - val_loss: 0.2473 - learning_rate: 3.5709e-04\n",
      "Epoch 25/100\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.00017854664474725723.\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.19772\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9784 - loss: 0.0932 - val_accuracy: 0.9534 - val_loss: 0.2365 - learning_rate: 3.5709e-04\n",
      "Epoch 25: early stopping\n",
      "Restoring model weights from the end of the best epoch: 15.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\n",
      "Epoch 1: val_loss improved from None to 0.79651, saving model to outputs/step4_final_loso\\models\\fold_8\\best_model.keras\n",
      "591/591 - 13s - 22ms/step - accuracy: 0.4959 - loss: 1.5449 - val_accuracy: 0.7468 - val_loss: 0.7965 - learning_rate: 3.5709e-04\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 2: val_loss improved from 0.79651 to 0.49467, saving model to outputs/step4_final_loso\\models\\fold_8\\best_model.keras\n",
      "591/591 - 11s - 18ms/step - accuracy: 0.7691 - loss: 0.7370 - val_accuracy: 0.8298 - val_loss: 0.4947 - learning_rate: 3.5709e-04\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 3: val_loss improved from 0.49467 to 0.39945, saving model to outputs/step4_final_loso\\models\\fold_8\\best_model.keras\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.8491 - loss: 0.4915 - val_accuracy: 0.8672 - val_loss: 0.3994 - learning_rate: 3.5709e-04\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 4: val_loss improved from 0.39945 to 0.33727, saving model to outputs/step4_final_loso\\models\\fold_8\\best_model.keras\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.8837 - loss: 0.3893 - val_accuracy: 0.8831 - val_loss: 0.3373 - learning_rate: 3.5709e-04\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.33727\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9038 - loss: 0.3282 - val_accuracy: 0.8853 - val_loss: 0.3787 - learning_rate: 3.5709e-04\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 6: val_loss improved from 0.33727 to 0.30984, saving model to outputs/step4_final_loso\\models\\fold_8\\best_model.keras\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9214 - loss: 0.2773 - val_accuracy: 0.9075 - val_loss: 0.3098 - learning_rate: 3.5709e-04\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.30984\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9280 - loss: 0.2530 - val_accuracy: 0.8974 - val_loss: 0.3280 - learning_rate: 3.5709e-04\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 8: val_loss improved from 0.30984 to 0.30452, saving model to outputs/step4_final_loso\\models\\fold_8\\best_model.keras\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9370 - loss: 0.2273 - val_accuracy: 0.9125 - val_loss: 0.3045 - learning_rate: 3.5709e-04\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 9: val_loss improved from 0.30452 to 0.30175, saving model to outputs/step4_final_loso\\models\\fold_8\\best_model.keras\n",
      "591/591 - 11s - 18ms/step - accuracy: 0.9439 - loss: 0.2035 - val_accuracy: 0.9161 - val_loss: 0.3017 - learning_rate: 3.5709e-04\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.30175\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9495 - loss: 0.1874 - val_accuracy: 0.9127 - val_loss: 0.3384 - learning_rate: 3.5709e-04\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 11: val_loss improved from 0.30175 to 0.29796, saving model to outputs/step4_final_loso\\models\\fold_8\\best_model.keras\n",
      "591/591 - 10s - 18ms/step - accuracy: 0.9528 - loss: 0.1783 - val_accuracy: 0.9185 - val_loss: 0.2980 - learning_rate: 3.5709e-04\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 12: val_loss improved from 0.29796 to 0.28242, saving model to outputs/step4_final_loso\\models\\fold_8\\best_model.keras\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9567 - loss: 0.1662 - val_accuracy: 0.9195 - val_loss: 0.2824 - learning_rate: 3.5709e-04\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 13: val_loss improved from 0.28242 to 0.27418, saving model to outputs/step4_final_loso\\models\\fold_8\\best_model.keras\n",
      "591/591 - 10s - 18ms/step - accuracy: 0.9590 - loss: 0.1537 - val_accuracy: 0.9314 - val_loss: 0.2742 - learning_rate: 3.5709e-04\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 14: val_loss improved from 0.27418 to 0.27037, saving model to outputs/step4_final_loso\\models\\fold_8\\best_model.keras\n",
      "591/591 - 11s - 18ms/step - accuracy: 0.9610 - loss: 0.1461 - val_accuracy: 0.9372 - val_loss: 0.2704 - learning_rate: 3.5709e-04\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.27037\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9652 - loss: 0.1361 - val_accuracy: 0.9326 - val_loss: 0.3137 - learning_rate: 3.5709e-04\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.27037\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9675 - loss: 0.1297 - val_accuracy: 0.9109 - val_loss: 0.3726 - learning_rate: 3.5709e-04\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.27037\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9675 - loss: 0.1273 - val_accuracy: 0.9345 - val_loss: 0.2740 - learning_rate: 3.5709e-04\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.27037\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9699 - loss: 0.1186 - val_accuracy: 0.9332 - val_loss: 0.3342 - learning_rate: 3.5709e-04\n",
      "Epoch 19/100\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.27037\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9709 - loss: 0.1168 - val_accuracy: 0.9354 - val_loss: 0.2906 - learning_rate: 3.5709e-04\n",
      "Epoch 20/100\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.27037\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9747 - loss: 0.1051 - val_accuracy: 0.9404 - val_loss: 0.2814 - learning_rate: 3.5709e-04\n",
      "Epoch 21/100\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.27037\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9757 - loss: 0.1041 - val_accuracy: 0.9366 - val_loss: 0.2995 - learning_rate: 3.5709e-04\n",
      "Epoch 22/100\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.27037\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9757 - loss: 0.0976 - val_accuracy: 0.9446 - val_loss: 0.2993 - learning_rate: 3.5709e-04\n",
      "Epoch 23/100\n",
      "\n",
      "Epoch 23: val_loss improved from 0.27037 to 0.23342, saving model to outputs/step4_final_loso\\models\\fold_8\\best_model.keras\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9769 - loss: 0.0990 - val_accuracy: 0.9477 - val_loss: 0.2334 - learning_rate: 3.5709e-04\n",
      "Epoch 24/100\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.23342\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9791 - loss: 0.0931 - val_accuracy: 0.9520 - val_loss: 0.2702 - learning_rate: 3.5709e-04\n",
      "Epoch 25/100\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.23342\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9776 - loss: 0.0928 - val_accuracy: 0.9246 - val_loss: 0.3433 - learning_rate: 3.5709e-04\n",
      "Epoch 26/100\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.23342\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9792 - loss: 0.0879 - val_accuracy: 0.9439 - val_loss: 0.2862 - learning_rate: 3.5709e-04\n",
      "Epoch 27/100\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.23342\n",
      "591/591 - 11s - 18ms/step - accuracy: 0.9812 - loss: 0.0833 - val_accuracy: 0.9376 - val_loss: 0.3115 - learning_rate: 3.5709e-04\n",
      "Epoch 28/100\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.23342\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9812 - loss: 0.0866 - val_accuracy: 0.9508 - val_loss: 0.2694 - learning_rate: 3.5709e-04\n",
      "Epoch 29/100\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.23342\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9818 - loss: 0.0811 - val_accuracy: 0.9467 - val_loss: 0.2896 - learning_rate: 3.5709e-04\n",
      "Epoch 30/100\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.23342\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9813 - loss: 0.0834 - val_accuracy: 0.9538 - val_loss: 0.2430 - learning_rate: 3.5709e-04\n",
      "Epoch 31/100\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.23342\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9830 - loss: 0.0763 - val_accuracy: 0.9474 - val_loss: 0.2756 - learning_rate: 3.5709e-04\n",
      "Epoch 32/100\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.23342\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9852 - loss: 0.0725 - val_accuracy: 0.9409 - val_loss: 0.3153 - learning_rate: 3.5709e-04\n",
      "Epoch 33/100\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 0.00017854664474725723.\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.23342\n",
      "591/591 - 10s - 18ms/step - accuracy: 0.9829 - loss: 0.0776 - val_accuracy: 0.9488 - val_loss: 0.3062 - learning_rate: 3.5709e-04\n",
      "Epoch 33: early stopping\n",
      "Restoring model weights from the end of the best epoch: 23.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\n",
      "Epoch 1: val_loss improved from None to 0.77863, saving model to outputs/step4_final_loso\\models\\fold_9\\best_model.keras\n",
      "591/591 - 12s - 20ms/step - accuracy: 0.4944 - loss: 1.5483 - val_accuracy: 0.7582 - val_loss: 0.7786 - learning_rate: 3.5709e-04\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 2: val_loss improved from 0.77863 to 0.49674, saving model to outputs/step4_final_loso\\models\\fold_9\\best_model.keras\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.7650 - loss: 0.7395 - val_accuracy: 0.8340 - val_loss: 0.4967 - learning_rate: 3.5709e-04\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 3: val_loss improved from 0.49674 to 0.45682, saving model to outputs/step4_final_loso\\models\\fold_9\\best_model.keras\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.8435 - loss: 0.5042 - val_accuracy: 0.8533 - val_loss: 0.4568 - learning_rate: 3.5709e-04\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 4: val_loss improved from 0.45682 to 0.35304, saving model to outputs/step4_final_loso\\models\\fold_9\\best_model.keras\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.8834 - loss: 0.3883 - val_accuracy: 0.8864 - val_loss: 0.3530 - learning_rate: 3.5709e-04\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 5: val_loss improved from 0.35304 to 0.35096, saving model to outputs/step4_final_loso\\models\\fold_9\\best_model.keras\n",
      "591/591 - 10s - 18ms/step - accuracy: 0.9041 - loss: 0.3253 - val_accuracy: 0.8850 - val_loss: 0.3510 - learning_rate: 3.5709e-04\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.35096\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9215 - loss: 0.2721 - val_accuracy: 0.9049 - val_loss: 0.3514 - learning_rate: 3.5709e-04\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 7: val_loss improved from 0.35096 to 0.34242, saving model to outputs/step4_final_loso\\models\\fold_9\\best_model.keras\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9284 - loss: 0.2529 - val_accuracy: 0.8981 - val_loss: 0.3424 - learning_rate: 3.5709e-04\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.34242\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9377 - loss: 0.2217 - val_accuracy: 0.9029 - val_loss: 0.3676 - learning_rate: 3.5709e-04\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.34242\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9441 - loss: 0.2050 - val_accuracy: 0.9067 - val_loss: 0.3707 - learning_rate: 3.5709e-04\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.34242\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9497 - loss: 0.1860 - val_accuracy: 0.9023 - val_loss: 0.4043 - learning_rate: 3.5709e-04\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 11: val_loss improved from 0.34242 to 0.33193, saving model to outputs/step4_final_loso\\models\\fold_9\\best_model.keras\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9544 - loss: 0.1740 - val_accuracy: 0.9116 - val_loss: 0.3319 - learning_rate: 3.5709e-04\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 12: val_loss improved from 0.33193 to 0.32963, saving model to outputs/step4_final_loso\\models\\fold_9\\best_model.keras\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9583 - loss: 0.1637 - val_accuracy: 0.9167 - val_loss: 0.3296 - learning_rate: 3.5709e-04\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 13: val_loss improved from 0.32963 to 0.31469, saving model to outputs/step4_final_loso\\models\\fold_9\\best_model.keras\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9605 - loss: 0.1534 - val_accuracy: 0.9186 - val_loss: 0.3147 - learning_rate: 3.5709e-04\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.31469\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9635 - loss: 0.1403 - val_accuracy: 0.9289 - val_loss: 0.3245 - learning_rate: 3.5709e-04\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.31469\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9650 - loss: 0.1386 - val_accuracy: 0.9260 - val_loss: 0.3216 - learning_rate: 3.5709e-04\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.31469\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9672 - loss: 0.1347 - val_accuracy: 0.9223 - val_loss: 0.3737 - learning_rate: 3.5709e-04\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.31469\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9685 - loss: 0.1266 - val_accuracy: 0.9251 - val_loss: 0.3592 - learning_rate: 3.5709e-04\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.31469\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9719 - loss: 0.1165 - val_accuracy: 0.9306 - val_loss: 0.3279 - learning_rate: 3.5709e-04\n",
      "Epoch 19/100\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.31469\n",
      "591/591 - 12s - 20ms/step - accuracy: 0.9714 - loss: 0.1156 - val_accuracy: 0.9338 - val_loss: 0.3369 - learning_rate: 3.5709e-04\n",
      "Epoch 20/100\n",
      "\n",
      "Epoch 20: val_loss improved from 0.31469 to 0.25243, saving model to outputs/step4_final_loso\\models\\fold_9\\best_model.keras\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9734 - loss: 0.1116 - val_accuracy: 0.9465 - val_loss: 0.2524 - learning_rate: 3.5709e-04\n",
      "Epoch 21/100\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.25243\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9748 - loss: 0.1060 - val_accuracy: 0.9345 - val_loss: 0.3154 - learning_rate: 3.5709e-04\n",
      "Epoch 22/100\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.25243\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9756 - loss: 0.1040 - val_accuracy: 0.9358 - val_loss: 0.3270 - learning_rate: 3.5709e-04\n",
      "Epoch 23/100\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.25243\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9763 - loss: 0.1009 - val_accuracy: 0.9450 - val_loss: 0.2707 - learning_rate: 3.5709e-04\n",
      "Epoch 24/100\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.25243\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9778 - loss: 0.0979 - val_accuracy: 0.9396 - val_loss: 0.2972 - learning_rate: 3.5709e-04\n",
      "Epoch 25/100\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.25243\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9780 - loss: 0.0910 - val_accuracy: 0.9332 - val_loss: 0.3282 - learning_rate: 3.5709e-04\n",
      "Epoch 26/100\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.25243\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9787 - loss: 0.0936 - val_accuracy: 0.9418 - val_loss: 0.3185 - learning_rate: 3.5709e-04\n",
      "Epoch 27/100\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.25243\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9801 - loss: 0.0893 - val_accuracy: 0.9387 - val_loss: 0.3428 - learning_rate: 3.5709e-04\n",
      "Epoch 28/100\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.25243\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9803 - loss: 0.0898 - val_accuracy: 0.9404 - val_loss: 0.2841 - learning_rate: 3.5709e-04\n",
      "Epoch 29/100\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.25243\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9810 - loss: 0.0836 - val_accuracy: 0.9363 - val_loss: 0.3784 - learning_rate: 3.5709e-04\n",
      "Epoch 30/100\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 0.00017854664474725723.\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.25243\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9820 - loss: 0.0845 - val_accuracy: 0.9516 - val_loss: 0.2819 - learning_rate: 3.5709e-04\n",
      "Epoch 30: early stopping\n",
      "Restoring model weights from the end of the best epoch: 20.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\n",
      "Epoch 1: val_loss improved from None to 0.78632, saving model to outputs/step4_final_loso\\models\\fold_10\\best_model.keras\n",
      "591/591 - 12s - 20ms/step - accuracy: 0.4979 - loss: 1.5446 - val_accuracy: 0.7478 - val_loss: 0.7863 - learning_rate: 3.5709e-04\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 2: val_loss improved from 0.78632 to 0.49410, saving model to outputs/step4_final_loso\\models\\fold_10\\best_model.keras\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.7654 - loss: 0.7461 - val_accuracy: 0.8291 - val_loss: 0.4941 - learning_rate: 3.5709e-04\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 3: val_loss improved from 0.49410 to 0.44648, saving model to outputs/step4_final_loso\\models\\fold_10\\best_model.keras\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.8413 - loss: 0.5153 - val_accuracy: 0.8589 - val_loss: 0.4465 - learning_rate: 3.5709e-04\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 4: val_loss improved from 0.44648 to 0.39038, saving model to outputs/step4_final_loso\\models\\fold_10\\best_model.keras\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.8788 - loss: 0.4040 - val_accuracy: 0.8690 - val_loss: 0.3904 - learning_rate: 3.5709e-04\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 5: val_loss improved from 0.39038 to 0.34590, saving model to outputs/step4_final_loso\\models\\fold_10\\best_model.keras\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.8989 - loss: 0.3442 - val_accuracy: 0.8950 - val_loss: 0.3459 - learning_rate: 3.5709e-04\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 6: val_loss improved from 0.34590 to 0.33699, saving model to outputs/step4_final_loso\\models\\fold_10\\best_model.keras\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9161 - loss: 0.2911 - val_accuracy: 0.9024 - val_loss: 0.3370 - learning_rate: 3.5709e-04\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 7: val_loss improved from 0.33699 to 0.28902, saving model to outputs/step4_final_loso\\models\\fold_10\\best_model.keras\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9250 - loss: 0.2653 - val_accuracy: 0.9094 - val_loss: 0.2890 - learning_rate: 3.5709e-04\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.28902\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9339 - loss: 0.2374 - val_accuracy: 0.9085 - val_loss: 0.3194 - learning_rate: 3.5709e-04\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 9: val_loss improved from 0.28902 to 0.28196, saving model to outputs/step4_final_loso\\models\\fold_10\\best_model.keras\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9406 - loss: 0.2191 - val_accuracy: 0.9243 - val_loss: 0.2820 - learning_rate: 3.5709e-04\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.28196\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9459 - loss: 0.1986 - val_accuracy: 0.8975 - val_loss: 0.4394 - learning_rate: 3.5709e-04\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.28196\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9488 - loss: 0.1900 - val_accuracy: 0.9104 - val_loss: 0.3007 - learning_rate: 3.5709e-04\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 12: val_loss improved from 0.28196 to 0.27889, saving model to outputs/step4_final_loso\\models\\fold_10\\best_model.keras\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9532 - loss: 0.1774 - val_accuracy: 0.9278 - val_loss: 0.2789 - learning_rate: 3.5709e-04\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 13: val_loss improved from 0.27889 to 0.24934, saving model to outputs/step4_final_loso\\models\\fold_10\\best_model.keras\n",
      "591/591 - 10s - 18ms/step - accuracy: 0.9571 - loss: 0.1608 - val_accuracy: 0.9300 - val_loss: 0.2493 - learning_rate: 3.5709e-04\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 14: val_loss improved from 0.24934 to 0.24012, saving model to outputs/step4_final_loso\\models\\fold_10\\best_model.keras\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9611 - loss: 0.1485 - val_accuracy: 0.9378 - val_loss: 0.2401 - learning_rate: 3.5709e-04\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.24012\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9626 - loss: 0.1455 - val_accuracy: 0.9216 - val_loss: 0.3195 - learning_rate: 3.5709e-04\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.24012\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9664 - loss: 0.1341 - val_accuracy: 0.9360 - val_loss: 0.2836 - learning_rate: 3.5709e-04\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.24012\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9667 - loss: 0.1325 - val_accuracy: 0.9312 - val_loss: 0.2491 - learning_rate: 3.5709e-04\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.24012\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9683 - loss: 0.1259 - val_accuracy: 0.9284 - val_loss: 0.2818 - learning_rate: 3.5709e-04\n",
      "Epoch 19/100\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.24012\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9705 - loss: 0.1188 - val_accuracy: 0.9378 - val_loss: 0.2761 - learning_rate: 3.5709e-04\n",
      "Epoch 20/100\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.24012\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9708 - loss: 0.1161 - val_accuracy: 0.9421 - val_loss: 0.2579 - learning_rate: 3.5709e-04\n",
      "Epoch 21/100\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.24012\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9729 - loss: 0.1105 - val_accuracy: 0.9483 - val_loss: 0.2665 - learning_rate: 3.5709e-04\n",
      "Epoch 22/100\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.24012\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9732 - loss: 0.1097 - val_accuracy: 0.9385 - val_loss: 0.3070 - learning_rate: 3.5709e-04\n",
      "Epoch 23/100\n",
      "\n",
      "Epoch 23: val_loss improved from 0.24012 to 0.22955, saving model to outputs/step4_final_loso\\models\\fold_10\\best_model.keras\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9751 - loss: 0.1045 - val_accuracy: 0.9407 - val_loss: 0.2295 - learning_rate: 3.5709e-04\n",
      "Epoch 24/100\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.22955\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9771 - loss: 0.0987 - val_accuracy: 0.9462 - val_loss: 0.2551 - learning_rate: 3.5709e-04\n",
      "Epoch 25/100\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.22955\n",
      "591/591 - 11s - 18ms/step - accuracy: 0.9767 - loss: 0.0972 - val_accuracy: 0.9494 - val_loss: 0.2881 - learning_rate: 3.5709e-04\n",
      "Epoch 26/100\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.22955\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9786 - loss: 0.0935 - val_accuracy: 0.9446 - val_loss: 0.2856 - learning_rate: 3.5709e-04\n",
      "Epoch 27/100\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.22955\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9796 - loss: 0.0880 - val_accuracy: 0.9428 - val_loss: 0.2753 - learning_rate: 3.5709e-04\n",
      "Epoch 28/100\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.22955\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9785 - loss: 0.0941 - val_accuracy: 0.9470 - val_loss: 0.2441 - learning_rate: 3.5709e-04\n",
      "Epoch 29/100\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.22955\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9801 - loss: 0.0866 - val_accuracy: 0.9366 - val_loss: 0.3388 - learning_rate: 3.5709e-04\n",
      "Epoch 30/100\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.22955\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9797 - loss: 0.0895 - val_accuracy: 0.9513 - val_loss: 0.2491 - learning_rate: 3.5709e-04\n",
      "Epoch 31/100\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.22955\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9835 - loss: 0.0794 - val_accuracy: 0.9459 - val_loss: 0.3338 - learning_rate: 3.5709e-04\n",
      "Epoch 32/100\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.22955\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9817 - loss: 0.0824 - val_accuracy: 0.9458 - val_loss: 0.3472 - learning_rate: 3.5709e-04\n",
      "Epoch 33/100\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 0.00017854664474725723.\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.22955\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9826 - loss: 0.0801 - val_accuracy: 0.9519 - val_loss: 0.2678 - learning_rate: 3.5709e-04\n",
      "Epoch 33: early stopping\n",
      "Restoring model weights from the end of the best epoch: 23.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\n",
      "Epoch 1: val_loss improved from None to 0.78140, saving model to outputs/step4_final_loso\\models\\fold_11\\best_model.keras\n",
      "591/591 - 12s - 20ms/step - accuracy: 0.4960 - loss: 1.5371 - val_accuracy: 0.7555 - val_loss: 0.7814 - learning_rate: 3.5709e-04\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 2: val_loss improved from 0.78140 to 0.51502, saving model to outputs/step4_final_loso\\models\\fold_11\\best_model.keras\n",
      "591/591 - 11s - 18ms/step - accuracy: 0.7649 - loss: 0.7389 - val_accuracy: 0.8234 - val_loss: 0.5150 - learning_rate: 3.5709e-04\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 3: val_loss improved from 0.51502 to 0.45166, saving model to outputs/step4_final_loso\\models\\fold_11\\best_model.keras\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.8446 - loss: 0.4994 - val_accuracy: 0.8570 - val_loss: 0.4517 - learning_rate: 3.5709e-04\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 4: val_loss improved from 0.45166 to 0.38110, saving model to outputs/step4_final_loso\\models\\fold_11\\best_model.keras\n",
      "591/591 - 11s - 18ms/step - accuracy: 0.8816 - loss: 0.3921 - val_accuracy: 0.8727 - val_loss: 0.3811 - learning_rate: 3.5709e-04\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 5: val_loss improved from 0.38110 to 0.38014, saving model to outputs/step4_final_loso\\models\\fold_11\\best_model.keras\n",
      "591/591 - 11s - 18ms/step - accuracy: 0.9018 - loss: 0.3350 - val_accuracy: 0.8843 - val_loss: 0.3801 - learning_rate: 3.5709e-04\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 6: val_loss improved from 0.38014 to 0.35027, saving model to outputs/step4_final_loso\\models\\fold_11\\best_model.keras\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9194 - loss: 0.2806 - val_accuracy: 0.8968 - val_loss: 0.3503 - learning_rate: 3.5709e-04\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 7: val_loss improved from 0.35027 to 0.34489, saving model to outputs/step4_final_loso\\models\\fold_11\\best_model.keras\n",
      "591/591 - 9s - 16ms/step - accuracy: 0.9282 - loss: 0.2571 - val_accuracy: 0.8951 - val_loss: 0.3449 - learning_rate: 3.5709e-04\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 8: val_loss improved from 0.34489 to 0.32183, saving model to outputs/step4_final_loso\\models\\fold_11\\best_model.keras\n",
      "591/591 - 9s - 15ms/step - accuracy: 0.9389 - loss: 0.2245 - val_accuracy: 0.9055 - val_loss: 0.3218 - learning_rate: 3.5709e-04\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.32183\n",
      "591/591 - 9s - 16ms/step - accuracy: 0.9426 - loss: 0.2093 - val_accuracy: 0.9110 - val_loss: 0.3273 - learning_rate: 3.5709e-04\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.32183\n",
      "591/591 - 9s - 15ms/step - accuracy: 0.9494 - loss: 0.1877 - val_accuracy: 0.9170 - val_loss: 0.3248 - learning_rate: 3.5709e-04\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 11: val_loss improved from 0.32183 to 0.32112, saving model to outputs/step4_final_loso\\models\\fold_11\\best_model.keras\n",
      "591/591 - 9s - 15ms/step - accuracy: 0.9534 - loss: 0.1767 - val_accuracy: 0.9057 - val_loss: 0.3211 - learning_rate: 3.5709e-04\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 12: val_loss improved from 0.32112 to 0.29091, saving model to outputs/step4_final_loso\\models\\fold_11\\best_model.keras\n",
      "591/591 - 9s - 15ms/step - accuracy: 0.9562 - loss: 0.1649 - val_accuracy: 0.9244 - val_loss: 0.2909 - learning_rate: 3.5709e-04\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 13: val_loss improved from 0.29091 to 0.24522, saving model to outputs/step4_final_loso\\models\\fold_11\\best_model.keras\n",
      "591/591 - 9s - 16ms/step - accuracy: 0.9598 - loss: 0.1551 - val_accuracy: 0.9345 - val_loss: 0.2452 - learning_rate: 3.5709e-04\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.24522\n",
      "591/591 - 9s - 15ms/step - accuracy: 0.9649 - loss: 0.1385 - val_accuracy: 0.9396 - val_loss: 0.2550 - learning_rate: 3.5709e-04\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.24522\n",
      "591/591 - 9s - 16ms/step - accuracy: 0.9639 - loss: 0.1417 - val_accuracy: 0.9315 - val_loss: 0.2892 - learning_rate: 3.5709e-04\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.24522\n",
      "591/591 - 9s - 16ms/step - accuracy: 0.9683 - loss: 0.1308 - val_accuracy: 0.9335 - val_loss: 0.2696 - learning_rate: 3.5709e-04\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.24522\n",
      "591/591 - 9s - 16ms/step - accuracy: 0.9683 - loss: 0.1256 - val_accuracy: 0.9398 - val_loss: 0.3019 - learning_rate: 3.5709e-04\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.24522\n",
      "591/591 - 10s - 16ms/step - accuracy: 0.9719 - loss: 0.1152 - val_accuracy: 0.9275 - val_loss: 0.3317 - learning_rate: 3.5709e-04\n",
      "Epoch 19/100\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.24522\n",
      "591/591 - 10s - 16ms/step - accuracy: 0.9727 - loss: 0.1153 - val_accuracy: 0.9161 - val_loss: 0.4230 - learning_rate: 3.5709e-04\n",
      "Epoch 20/100\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.24522\n",
      "591/591 - 9s - 16ms/step - accuracy: 0.9718 - loss: 0.1151 - val_accuracy: 0.9358 - val_loss: 0.2967 - learning_rate: 3.5709e-04\n",
      "Epoch 21/100\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.24522\n",
      "591/591 - 9s - 16ms/step - accuracy: 0.9749 - loss: 0.1040 - val_accuracy: 0.9416 - val_loss: 0.2629 - learning_rate: 3.5709e-04\n",
      "Epoch 22/100\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.24522\n",
      "591/591 - 9s - 16ms/step - accuracy: 0.9753 - loss: 0.1062 - val_accuracy: 0.9277 - val_loss: 0.3225 - learning_rate: 3.5709e-04\n",
      "Epoch 23/100\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.00017854664474725723.\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.24522\n",
      "591/591 - 10s - 16ms/step - accuracy: 0.9773 - loss: 0.0980 - val_accuracy: 0.9424 - val_loss: 0.2550 - learning_rate: 3.5709e-04\n",
      "Epoch 23: early stopping\n",
      "Restoring model weights from the end of the best epoch: 13.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\n",
      "Epoch 1: val_loss improved from None to 0.86580, saving model to outputs/step4_final_loso\\models\\fold_12\\best_model.keras\n",
      "592/592 - 11s - 19ms/step - accuracy: 0.4820 - loss: 1.5827 - val_accuracy: 0.7275 - val_loss: 0.8658 - learning_rate: 3.5709e-04\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 2: val_loss improved from 0.86580 to 0.57010, saving model to outputs/step4_final_loso\\models\\fold_12\\best_model.keras\n",
      "592/592 - 10s - 17ms/step - accuracy: 0.7534 - loss: 0.7734 - val_accuracy: 0.8342 - val_loss: 0.5701 - learning_rate: 3.5709e-04\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 3: val_loss improved from 0.57010 to 0.51580, saving model to outputs/step4_final_loso\\models\\fold_12\\best_model.keras\n",
      "592/592 - 10s - 17ms/step - accuracy: 0.8443 - loss: 0.5118 - val_accuracy: 0.8500 - val_loss: 0.5158 - learning_rate: 3.5709e-04\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 4: val_loss improved from 0.51580 to 0.39778, saving model to outputs/step4_final_loso\\models\\fold_12\\best_model.keras\n",
      "592/592 - 10s - 16ms/step - accuracy: 0.8833 - loss: 0.4009 - val_accuracy: 0.8810 - val_loss: 0.3978 - learning_rate: 3.5709e-04\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 5: val_loss improved from 0.39778 to 0.38013, saving model to outputs/step4_final_loso\\models\\fold_12\\best_model.keras\n",
      "592/592 - 10s - 17ms/step - accuracy: 0.9041 - loss: 0.3325 - val_accuracy: 0.8867 - val_loss: 0.3801 - learning_rate: 3.5709e-04\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 6: val_loss improved from 0.38013 to 0.35455, saving model to outputs/step4_final_loso\\models\\fold_12\\best_model.keras\n",
      "592/592 - 11s - 18ms/step - accuracy: 0.9206 - loss: 0.2832 - val_accuracy: 0.9014 - val_loss: 0.3545 - learning_rate: 3.5709e-04\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.35455\n",
      "592/592 - 12s - 20ms/step - accuracy: 0.9293 - loss: 0.2519 - val_accuracy: 0.9098 - val_loss: 0.3591 - learning_rate: 3.5709e-04\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 8: val_loss improved from 0.35455 to 0.33784, saving model to outputs/step4_final_loso\\models\\fold_12\\best_model.keras\n",
      "592/592 - 12s - 20ms/step - accuracy: 0.9395 - loss: 0.2222 - val_accuracy: 0.9130 - val_loss: 0.3378 - learning_rate: 3.5709e-04\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.33784\n",
      "592/592 - 11s - 18ms/step - accuracy: 0.9423 - loss: 0.2095 - val_accuracy: 0.8937 - val_loss: 0.3953 - learning_rate: 3.5709e-04\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 10: val_loss improved from 0.33784 to 0.32520, saving model to outputs/step4_final_loso\\models\\fold_12\\best_model.keras\n",
      "592/592 - 11s - 18ms/step - accuracy: 0.9518 - loss: 0.1880 - val_accuracy: 0.9179 - val_loss: 0.3252 - learning_rate: 3.5709e-04\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 11: val_loss improved from 0.32520 to 0.30145, saving model to outputs/step4_final_loso\\models\\fold_12\\best_model.keras\n",
      "592/592 - 10s - 17ms/step - accuracy: 0.9545 - loss: 0.1769 - val_accuracy: 0.9240 - val_loss: 0.3015 - learning_rate: 3.5709e-04\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.30145\n",
      "592/592 - 10s - 17ms/step - accuracy: 0.9563 - loss: 0.1665 - val_accuracy: 0.9210 - val_loss: 0.3809 - learning_rate: 3.5709e-04\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 13: val_loss improved from 0.30145 to 0.24949, saving model to outputs/step4_final_loso\\models\\fold_12\\best_model.keras\n",
      "592/592 - 11s - 19ms/step - accuracy: 0.9584 - loss: 0.1602 - val_accuracy: 0.9342 - val_loss: 0.2495 - learning_rate: 3.5709e-04\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.24949\n",
      "592/592 - 11s - 18ms/step - accuracy: 0.9610 - loss: 0.1484 - val_accuracy: 0.9321 - val_loss: 0.2703 - learning_rate: 3.5709e-04\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.24949\n",
      "592/592 - 10s - 17ms/step - accuracy: 0.9649 - loss: 0.1394 - val_accuracy: 0.9372 - val_loss: 0.2740 - learning_rate: 3.5709e-04\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.24949\n",
      "592/592 - 11s - 18ms/step - accuracy: 0.9668 - loss: 0.1326 - val_accuracy: 0.9296 - val_loss: 0.3302 - learning_rate: 3.5709e-04\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.24949\n",
      "592/592 - 10s - 17ms/step - accuracy: 0.9690 - loss: 0.1249 - val_accuracy: 0.9473 - val_loss: 0.2550 - learning_rate: 3.5709e-04\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.24949\n",
      "592/592 - 9s - 16ms/step - accuracy: 0.9705 - loss: 0.1229 - val_accuracy: 0.9421 - val_loss: 0.2957 - learning_rate: 3.5709e-04\n",
      "Epoch 19/100\n",
      "\n",
      "Epoch 19: val_loss improved from 0.24949 to 0.22408, saving model to outputs/step4_final_loso\\models\\fold_12\\best_model.keras\n",
      "592/592 - 9s - 16ms/step - accuracy: 0.9709 - loss: 0.1164 - val_accuracy: 0.9507 - val_loss: 0.2241 - learning_rate: 3.5709e-04\n",
      "Epoch 20/100\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.22408\n",
      "592/592 - 10s - 16ms/step - accuracy: 0.9729 - loss: 0.1133 - val_accuracy: 0.9398 - val_loss: 0.3071 - learning_rate: 3.5709e-04\n",
      "Epoch 21/100\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.22408\n",
      "592/592 - 9s - 16ms/step - accuracy: 0.9755 - loss: 0.1062 - val_accuracy: 0.9467 - val_loss: 0.2776 - learning_rate: 3.5709e-04\n",
      "Epoch 22/100\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.22408\n",
      "592/592 - 11s - 18ms/step - accuracy: 0.9745 - loss: 0.1064 - val_accuracy: 0.9473 - val_loss: 0.2835 - learning_rate: 3.5709e-04\n",
      "Epoch 23/100\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.22408\n",
      "592/592 - 11s - 18ms/step - accuracy: 0.9763 - loss: 0.1028 - val_accuracy: 0.9409 - val_loss: 0.2980 - learning_rate: 3.5709e-04\n",
      "Epoch 24/100\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.22408\n",
      "592/592 - 10s - 18ms/step - accuracy: 0.9776 - loss: 0.0958 - val_accuracy: 0.9382 - val_loss: 0.3169 - learning_rate: 3.5709e-04\n",
      "Epoch 25/100\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.22408\n",
      "592/592 - 10s - 17ms/step - accuracy: 0.9771 - loss: 0.0989 - val_accuracy: 0.9470 - val_loss: 0.2612 - learning_rate: 3.5709e-04\n",
      "Epoch 26/100\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.22408\n",
      "592/592 - 10s - 17ms/step - accuracy: 0.9781 - loss: 0.0930 - val_accuracy: 0.9491 - val_loss: 0.2760 - learning_rate: 3.5709e-04\n",
      "Epoch 27/100\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.22408\n",
      "592/592 - 11s - 18ms/step - accuracy: 0.9780 - loss: 0.0939 - val_accuracy: 0.9409 - val_loss: 0.2642 - learning_rate: 3.5709e-04\n",
      "Epoch 28/100\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.22408\n",
      "592/592 - 10s - 18ms/step - accuracy: 0.9794 - loss: 0.0879 - val_accuracy: 0.9486 - val_loss: 0.3198 - learning_rate: 3.5709e-04\n",
      "Epoch 29/100\n",
      "\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 0.00017854664474725723.\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.22408\n",
      "592/592 - 11s - 18ms/step - accuracy: 0.9821 - loss: 0.0818 - val_accuracy: 0.9575 - val_loss: 0.2630 - learning_rate: 3.5709e-04\n",
      "Epoch 29: early stopping\n",
      "Restoring model weights from the end of the best epoch: 19.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\n",
      "Epoch 1: val_loss improved from None to 0.75775, saving model to outputs/step4_final_loso\\models\\fold_13\\best_model.keras\n",
      "593/593 - 13s - 21ms/step - accuracy: 0.4957 - loss: 1.5369 - val_accuracy: 0.7678 - val_loss: 0.7578 - learning_rate: 3.5709e-04\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 2: val_loss improved from 0.75775 to 0.51131, saving model to outputs/step4_final_loso\\models\\fold_13\\best_model.keras\n",
      "593/593 - 11s - 19ms/step - accuracy: 0.7607 - loss: 0.7535 - val_accuracy: 0.8285 - val_loss: 0.5113 - learning_rate: 3.5709e-04\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 3: val_loss improved from 0.51131 to 0.41515, saving model to outputs/step4_final_loso\\models\\fold_13\\best_model.keras\n",
      "593/593 - 11s - 18ms/step - accuracy: 0.8436 - loss: 0.5120 - val_accuracy: 0.8791 - val_loss: 0.4152 - learning_rate: 3.5709e-04\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 4: val_loss improved from 0.41515 to 0.39606, saving model to outputs/step4_final_loso\\models\\fold_13\\best_model.keras\n",
      "593/593 - 11s - 18ms/step - accuracy: 0.8793 - loss: 0.4046 - val_accuracy: 0.8878 - val_loss: 0.3961 - learning_rate: 3.5709e-04\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 5: val_loss improved from 0.39606 to 0.30997, saving model to outputs/step4_final_loso\\models\\fold_13\\best_model.keras\n",
      "593/593 - 11s - 18ms/step - accuracy: 0.9011 - loss: 0.3355 - val_accuracy: 0.9049 - val_loss: 0.3100 - learning_rate: 3.5709e-04\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 6: val_loss improved from 0.30997 to 0.28117, saving model to outputs/step4_final_loso\\models\\fold_13\\best_model.keras\n",
      "593/593 - 10s - 17ms/step - accuracy: 0.9191 - loss: 0.2817 - val_accuracy: 0.9198 - val_loss: 0.2812 - learning_rate: 3.5709e-04\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 7: val_loss improved from 0.28117 to 0.24499, saving model to outputs/step4_final_loso\\models\\fold_13\\best_model.keras\n",
      "593/593 - 10s - 17ms/step - accuracy: 0.9257 - loss: 0.2591 - val_accuracy: 0.9358 - val_loss: 0.2450 - learning_rate: 3.5709e-04\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.24499\n",
      "593/593 - 10s - 17ms/step - accuracy: 0.9372 - loss: 0.2275 - val_accuracy: 0.9287 - val_loss: 0.2646 - learning_rate: 3.5709e-04\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.24499\n",
      "593/593 - 10s - 17ms/step - accuracy: 0.9432 - loss: 0.2036 - val_accuracy: 0.9325 - val_loss: 0.2701 - learning_rate: 3.5709e-04\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.24499\n",
      "593/593 - 13s - 22ms/step - accuracy: 0.9494 - loss: 0.1891 - val_accuracy: 0.9255 - val_loss: 0.2686 - learning_rate: 3.5709e-04\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 11: val_loss improved from 0.24499 to 0.24056, saving model to outputs/step4_final_loso\\models\\fold_13\\best_model.keras\n",
      "593/593 - 16s - 27ms/step - accuracy: 0.9527 - loss: 0.1746 - val_accuracy: 0.9398 - val_loss: 0.2406 - learning_rate: 3.5709e-04\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 12: val_loss improved from 0.24056 to 0.23534, saving model to outputs/step4_final_loso\\models\\fold_13\\best_model.keras\n",
      "593/593 - 20s - 33ms/step - accuracy: 0.9575 - loss: 0.1609 - val_accuracy: 0.9472 - val_loss: 0.2353 - learning_rate: 3.5709e-04\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 13: val_loss improved from 0.23534 to 0.22528, saving model to outputs/step4_final_loso\\models\\fold_13\\best_model.keras\n",
      "593/593 - 15s - 25ms/step - accuracy: 0.9580 - loss: 0.1567 - val_accuracy: 0.9469 - val_loss: 0.2253 - learning_rate: 3.5709e-04\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.22528\n",
      "593/593 - 19s - 32ms/step - accuracy: 0.9615 - loss: 0.1481 - val_accuracy: 0.9485 - val_loss: 0.2363 - learning_rate: 3.5709e-04\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.22528\n",
      "593/593 - 10s - 16ms/step - accuracy: 0.9661 - loss: 0.1313 - val_accuracy: 0.9457 - val_loss: 0.2489 - learning_rate: 3.5709e-04\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.22528\n",
      "593/593 - 10s - 16ms/step - accuracy: 0.9655 - loss: 0.1329 - val_accuracy: 0.9531 - val_loss: 0.2390 - learning_rate: 3.5709e-04\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.22528\n",
      "593/593 - 9s - 16ms/step - accuracy: 0.9696 - loss: 0.1209 - val_accuracy: 0.9495 - val_loss: 0.2643 - learning_rate: 3.5709e-04\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.22528\n",
      "593/593 - 11s - 18ms/step - accuracy: 0.9710 - loss: 0.1180 - val_accuracy: 0.9542 - val_loss: 0.2408 - learning_rate: 3.5709e-04\n",
      "Epoch 19/100\n",
      "\n",
      "Epoch 19: val_loss improved from 0.22528 to 0.22297, saving model to outputs/step4_final_loso\\models\\fold_13\\best_model.keras\n",
      "593/593 - 10s - 16ms/step - accuracy: 0.9707 - loss: 0.1153 - val_accuracy: 0.9551 - val_loss: 0.2230 - learning_rate: 3.5709e-04\n",
      "Epoch 20/100\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.22297\n",
      "593/593 - 12s - 20ms/step - accuracy: 0.9722 - loss: 0.1135 - val_accuracy: 0.9545 - val_loss: 0.2544 - learning_rate: 3.5709e-04\n",
      "Epoch 21/100\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.22297\n",
      "593/593 - 13s - 22ms/step - accuracy: 0.9764 - loss: 0.1016 - val_accuracy: 0.9536 - val_loss: 0.2621 - learning_rate: 3.5709e-04\n",
      "Epoch 22/100\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.22297\n",
      "593/593 - 12s - 20ms/step - accuracy: 0.9750 - loss: 0.1034 - val_accuracy: 0.9584 - val_loss: 0.2636 - learning_rate: 3.5709e-04\n",
      "Epoch 23/100\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.22297\n",
      "593/593 - 10s - 17ms/step - accuracy: 0.9758 - loss: 0.0999 - val_accuracy: 0.9553 - val_loss: 0.2492 - learning_rate: 3.5709e-04\n",
      "Epoch 24/100\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.22297\n",
      "593/593 - 12s - 20ms/step - accuracy: 0.9779 - loss: 0.0932 - val_accuracy: 0.9588 - val_loss: 0.2501 - learning_rate: 3.5709e-04\n",
      "Epoch 25/100\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.22297\n",
      "593/593 - 12s - 20ms/step - accuracy: 0.9779 - loss: 0.0940 - val_accuracy: 0.9597 - val_loss: 0.2281 - learning_rate: 3.5709e-04\n",
      "Epoch 26/100\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.22297\n",
      "593/593 - 11s - 18ms/step - accuracy: 0.9791 - loss: 0.0905 - val_accuracy: 0.9570 - val_loss: 0.2503 - learning_rate: 3.5709e-04\n",
      "Epoch 27/100\n",
      "\n",
      "Epoch 27: val_loss improved from 0.22297 to 0.21510, saving model to outputs/step4_final_loso\\models\\fold_13\\best_model.keras\n",
      "593/593 - 12s - 20ms/step - accuracy: 0.9805 - loss: 0.0880 - val_accuracy: 0.9618 - val_loss: 0.2151 - learning_rate: 3.5709e-04\n",
      "Epoch 28/100\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.21510\n",
      "593/593 - 13s - 21ms/step - accuracy: 0.9809 - loss: 0.0843 - val_accuracy: 0.9523 - val_loss: 0.2790 - learning_rate: 3.5709e-04\n",
      "Epoch 29/100\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.21510\n",
      "593/593 - 11s - 19ms/step - accuracy: 0.9807 - loss: 0.0815 - val_accuracy: 0.9529 - val_loss: 0.3064 - learning_rate: 3.5709e-04\n",
      "Epoch 30/100\n",
      "\n",
      "Epoch 30: val_loss improved from 0.21510 to 0.20483, saving model to outputs/step4_final_loso\\models\\fold_13\\best_model.keras\n",
      "593/593 - 10s - 16ms/step - accuracy: 0.9821 - loss: 0.0807 - val_accuracy: 0.9634 - val_loss: 0.2048 - learning_rate: 3.5709e-04\n",
      "Epoch 31/100\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.20483\n",
      "593/593 - 10s - 16ms/step - accuracy: 0.9825 - loss: 0.0790 - val_accuracy: 0.9570 - val_loss: 0.2507 - learning_rate: 3.5709e-04\n",
      "Epoch 32/100\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.20483\n",
      "593/593 - 10s - 17ms/step - accuracy: 0.9835 - loss: 0.0747 - val_accuracy: 0.9640 - val_loss: 0.2257 - learning_rate: 3.5709e-04\n",
      "Epoch 33/100\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.20483\n",
      "593/593 - 11s - 19ms/step - accuracy: 0.9832 - loss: 0.0760 - val_accuracy: 0.9575 - val_loss: 0.2784 - learning_rate: 3.5709e-04\n",
      "Epoch 34/100\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.20483\n",
      "593/593 - 10s - 17ms/step - accuracy: 0.9833 - loss: 0.0771 - val_accuracy: 0.9600 - val_loss: 0.2340 - learning_rate: 3.5709e-04\n",
      "Epoch 35/100\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.20483\n",
      "593/593 - 11s - 18ms/step - accuracy: 0.9847 - loss: 0.0697 - val_accuracy: 0.9582 - val_loss: 0.2644 - learning_rate: 3.5709e-04\n",
      "Epoch 36/100\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.20483\n",
      "593/593 - 11s - 18ms/step - accuracy: 0.9846 - loss: 0.0710 - val_accuracy: 0.9678 - val_loss: 0.2220 - learning_rate: 3.5709e-04\n",
      "Epoch 37/100\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.20483\n",
      "593/593 - 11s - 19ms/step - accuracy: 0.9852 - loss: 0.0683 - val_accuracy: 0.9635 - val_loss: 0.2692 - learning_rate: 3.5709e-04\n",
      "Epoch 38/100\n",
      "\n",
      "Epoch 38: val_loss improved from 0.20483 to 0.20093, saving model to outputs/step4_final_loso\\models\\fold_13\\best_model.keras\n",
      "593/593 - 11s - 18ms/step - accuracy: 0.9858 - loss: 0.0680 - val_accuracy: 0.9659 - val_loss: 0.2009 - learning_rate: 3.5709e-04\n",
      "Epoch 39/100\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.20093\n",
      "593/593 - 11s - 18ms/step - accuracy: 0.9854 - loss: 0.0691 - val_accuracy: 0.9662 - val_loss: 0.2504 - learning_rate: 3.5709e-04\n",
      "Epoch 40/100\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.20093\n",
      "593/593 - 10s - 16ms/step - accuracy: 0.9862 - loss: 0.0644 - val_accuracy: 0.9625 - val_loss: 0.2523 - learning_rate: 3.5709e-04\n",
      "Epoch 41/100\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.20093\n",
      "593/593 - 10s - 16ms/step - accuracy: 0.9847 - loss: 0.0726 - val_accuracy: 0.9553 - val_loss: 0.3150 - learning_rate: 3.5709e-04\n",
      "Epoch 42/100\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.20093\n",
      "593/593 - 10s - 16ms/step - accuracy: 0.9880 - loss: 0.0585 - val_accuracy: 0.9669 - val_loss: 0.2504 - learning_rate: 3.5709e-04\n",
      "Epoch 43/100\n",
      "\n",
      "Epoch 43: val_loss improved from 0.20093 to 0.19505, saving model to outputs/step4_final_loso\\models\\fold_13\\best_model.keras\n",
      "593/593 - 10s - 16ms/step - accuracy: 0.9874 - loss: 0.0643 - val_accuracy: 0.9720 - val_loss: 0.1950 - learning_rate: 3.5709e-04\n",
      "Epoch 44/100\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.19505\n",
      "593/593 - 10s - 16ms/step - accuracy: 0.9871 - loss: 0.0608 - val_accuracy: 0.9627 - val_loss: 0.2414 - learning_rate: 3.5709e-04\n",
      "Epoch 45/100\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.19505\n",
      "593/593 - 10s - 17ms/step - accuracy: 0.9866 - loss: 0.0655 - val_accuracy: 0.9535 - val_loss: 0.2673 - learning_rate: 3.5709e-04\n",
      "Epoch 46/100\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.19505\n",
      "593/593 - 10s - 16ms/step - accuracy: 0.9882 - loss: 0.0605 - val_accuracy: 0.9600 - val_loss: 0.2592 - learning_rate: 3.5709e-04\n",
      "Epoch 47/100\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.19505\n",
      "593/593 - 9s - 15ms/step - accuracy: 0.9875 - loss: 0.0618 - val_accuracy: 0.9593 - val_loss: 0.2781 - learning_rate: 3.5709e-04\n",
      "Epoch 48/100\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.19505\n",
      "593/593 - 10s - 16ms/step - accuracy: 0.9885 - loss: 0.0599 - val_accuracy: 0.9662 - val_loss: 0.2284 - learning_rate: 3.5709e-04\n",
      "Epoch 49/100\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.19505\n",
      "593/593 - 10s - 16ms/step - accuracy: 0.9885 - loss: 0.0585 - val_accuracy: 0.9650 - val_loss: 0.2716 - learning_rate: 3.5709e-04\n",
      "Epoch 50/100\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.19505\n",
      "593/593 - 9s - 16ms/step - accuracy: 0.9885 - loss: 0.0577 - val_accuracy: 0.9663 - val_loss: 0.2458 - learning_rate: 3.5709e-04\n",
      "Epoch 51/100\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.19505\n",
      "593/593 - 9s - 16ms/step - accuracy: 0.9882 - loss: 0.0601 - val_accuracy: 0.9634 - val_loss: 0.2076 - learning_rate: 3.5709e-04\n",
      "Epoch 52/100\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.19505\n",
      "593/593 - 9s - 16ms/step - accuracy: 0.9882 - loss: 0.0581 - val_accuracy: 0.9669 - val_loss: 0.2359 - learning_rate: 3.5709e-04\n",
      "Epoch 53/100\n",
      "\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 0.00017854664474725723.\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.19505\n",
      "593/593 - 9s - 15ms/step - accuracy: 0.9892 - loss: 0.0541 - val_accuracy: 0.9649 - val_loss: 0.2644 - learning_rate: 3.5709e-04\n",
      "Epoch 53: early stopping\n",
      "Restoring model weights from the end of the best epoch: 43.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\n",
      "Epoch 1: val_loss improved from None to 0.80456, saving model to outputs/step4_final_loso\\models\\fold_14\\best_model.keras\n",
      "592/592 - 11s - 18ms/step - accuracy: 0.4946 - loss: 1.5535 - val_accuracy: 0.7588 - val_loss: 0.8046 - learning_rate: 3.5709e-04\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 2: val_loss improved from 0.80456 to 0.50421, saving model to outputs/step4_final_loso\\models\\fold_14\\best_model.keras\n",
      "592/592 - 9s - 16ms/step - accuracy: 0.7540 - loss: 0.7604 - val_accuracy: 0.8329 - val_loss: 0.5042 - learning_rate: 3.5709e-04\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 3: val_loss improved from 0.50421 to 0.41332, saving model to outputs/step4_final_loso\\models\\fold_14\\best_model.keras\n",
      "592/592 - 9s - 16ms/step - accuracy: 0.8370 - loss: 0.5229 - val_accuracy: 0.8652 - val_loss: 0.4133 - learning_rate: 3.5709e-04\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 4: val_loss improved from 0.41332 to 0.38027, saving model to outputs/step4_final_loso\\models\\fold_14\\best_model.keras\n",
      "592/592 - 9s - 16ms/step - accuracy: 0.8732 - loss: 0.4195 - val_accuracy: 0.8800 - val_loss: 0.3803 - learning_rate: 3.5709e-04\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 5: val_loss improved from 0.38027 to 0.35984, saving model to outputs/step4_final_loso\\models\\fold_14\\best_model.keras\n",
      "592/592 - 9s - 16ms/step - accuracy: 0.8993 - loss: 0.3428 - val_accuracy: 0.8897 - val_loss: 0.3598 - learning_rate: 3.5709e-04\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.35984\n",
      "592/592 - 9s - 16ms/step - accuracy: 0.9141 - loss: 0.2961 - val_accuracy: 0.8916 - val_loss: 0.3850 - learning_rate: 3.5709e-04\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.35984\n",
      "592/592 - 10s - 16ms/step - accuracy: 0.9259 - loss: 0.2596 - val_accuracy: 0.9005 - val_loss: 0.3790 - learning_rate: 3.5709e-04\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 8: val_loss improved from 0.35984 to 0.31901, saving model to outputs/step4_final_loso\\models\\fold_14\\best_model.keras\n",
      "592/592 - 9s - 16ms/step - accuracy: 0.9328 - loss: 0.2348 - val_accuracy: 0.9092 - val_loss: 0.3190 - learning_rate: 3.5709e-04\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 9: val_loss improved from 0.31901 to 0.29128, saving model to outputs/step4_final_loso\\models\\fold_14\\best_model.keras\n",
      "592/592 - 9s - 16ms/step - accuracy: 0.9390 - loss: 0.2171 - val_accuracy: 0.9309 - val_loss: 0.2913 - learning_rate: 3.5709e-04\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.29128\n",
      "592/592 - 9s - 16ms/step - accuracy: 0.9449 - loss: 0.1991 - val_accuracy: 0.9302 - val_loss: 0.3003 - learning_rate: 3.5709e-04\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.29128\n",
      "592/592 - 9s - 16ms/step - accuracy: 0.9500 - loss: 0.1876 - val_accuracy: 0.9296 - val_loss: 0.2962 - learning_rate: 3.5709e-04\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.29128\n",
      "592/592 - 9s - 16ms/step - accuracy: 0.9530 - loss: 0.1736 - val_accuracy: 0.9322 - val_loss: 0.2921 - learning_rate: 3.5709e-04\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.29128\n",
      "592/592 - 9s - 16ms/step - accuracy: 0.9568 - loss: 0.1601 - val_accuracy: 0.9297 - val_loss: 0.2992 - learning_rate: 3.5709e-04\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 14: val_loss improved from 0.29128 to 0.27358, saving model to outputs/step4_final_loso\\models\\fold_14\\best_model.keras\n",
      "592/592 - 9s - 16ms/step - accuracy: 0.9594 - loss: 0.1499 - val_accuracy: 0.9430 - val_loss: 0.2736 - learning_rate: 3.5709e-04\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.27358\n",
      "592/592 - 10s - 18ms/step - accuracy: 0.9623 - loss: 0.1419 - val_accuracy: 0.9324 - val_loss: 0.3182 - learning_rate: 3.5709e-04\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.27358\n",
      "592/592 - 11s - 18ms/step - accuracy: 0.9640 - loss: 0.1360 - val_accuracy: 0.9319 - val_loss: 0.2905 - learning_rate: 3.5709e-04\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 17: val_loss improved from 0.27358 to 0.25521, saving model to outputs/step4_final_loso\\models\\fold_14\\best_model.keras\n",
      "592/592 - 11s - 19ms/step - accuracy: 0.9642 - loss: 0.1353 - val_accuracy: 0.9411 - val_loss: 0.2552 - learning_rate: 3.5709e-04\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.25521\n",
      "592/592 - 12s - 21ms/step - accuracy: 0.9670 - loss: 0.1264 - val_accuracy: 0.9353 - val_loss: 0.2713 - learning_rate: 3.5709e-04\n",
      "Epoch 19/100\n",
      "\n",
      "Epoch 19: val_loss improved from 0.25521 to 0.23399, saving model to outputs/step4_final_loso\\models\\fold_14\\best_model.keras\n",
      "592/592 - 10s - 16ms/step - accuracy: 0.9677 - loss: 0.1245 - val_accuracy: 0.9423 - val_loss: 0.2340 - learning_rate: 3.5709e-04\n",
      "Epoch 20/100\n",
      "\n",
      "Epoch 20: val_loss improved from 0.23399 to 0.22260, saving model to outputs/step4_final_loso\\models\\fold_14\\best_model.keras\n",
      "592/592 - 10s - 17ms/step - accuracy: 0.9700 - loss: 0.1151 - val_accuracy: 0.9500 - val_loss: 0.2226 - learning_rate: 3.5709e-04\n",
      "Epoch 21/100\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.22260\n",
      "592/592 - 9s - 16ms/step - accuracy: 0.9738 - loss: 0.1109 - val_accuracy: 0.9439 - val_loss: 0.2667 - learning_rate: 3.5709e-04\n",
      "Epoch 22/100\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.22260\n",
      "592/592 - 9s - 16ms/step - accuracy: 0.9730 - loss: 0.1093 - val_accuracy: 0.9488 - val_loss: 0.2652 - learning_rate: 3.5709e-04\n",
      "Epoch 23/100\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.22260\n",
      "592/592 - 9s - 16ms/step - accuracy: 0.9754 - loss: 0.0997 - val_accuracy: 0.9421 - val_loss: 0.2595 - learning_rate: 3.5709e-04\n",
      "Epoch 24/100\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.22260\n",
      "592/592 - 9s - 16ms/step - accuracy: 0.9766 - loss: 0.0974 - val_accuracy: 0.9460 - val_loss: 0.2859 - learning_rate: 3.5709e-04\n",
      "Epoch 25/100\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.22260\n",
      "592/592 - 9s - 16ms/step - accuracy: 0.9789 - loss: 0.0907 - val_accuracy: 0.9438 - val_loss: 0.3112 - learning_rate: 3.5709e-04\n",
      "Epoch 26/100\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.22260\n",
      "592/592 - 9s - 16ms/step - accuracy: 0.9783 - loss: 0.0923 - val_accuracy: 0.9454 - val_loss: 0.2934 - learning_rate: 3.5709e-04\n",
      "Epoch 27/100\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.22260\n",
      "592/592 - 9s - 16ms/step - accuracy: 0.9799 - loss: 0.0897 - val_accuracy: 0.9547 - val_loss: 0.2467 - learning_rate: 3.5709e-04\n",
      "Epoch 28/100\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.22260\n",
      "592/592 - 9s - 16ms/step - accuracy: 0.9797 - loss: 0.0893 - val_accuracy: 0.9486 - val_loss: 0.2928 - learning_rate: 3.5709e-04\n",
      "Epoch 29/100\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.22260\n",
      "592/592 - 9s - 16ms/step - accuracy: 0.9793 - loss: 0.0913 - val_accuracy: 0.9559 - val_loss: 0.2395 - learning_rate: 3.5709e-04\n",
      "Epoch 30/100\n",
      "\n",
      "Epoch 30: val_loss improved from 0.22260 to 0.17893, saving model to outputs/step4_final_loso\\models\\fold_14\\best_model.keras\n",
      "592/592 - 9s - 16ms/step - accuracy: 0.9804 - loss: 0.0841 - val_accuracy: 0.9581 - val_loss: 0.1789 - learning_rate: 3.5709e-04\n",
      "Epoch 31/100\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.17893\n",
      "592/592 - 10s - 16ms/step - accuracy: 0.9812 - loss: 0.0820 - val_accuracy: 0.9544 - val_loss: 0.2157 - learning_rate: 3.5709e-04\n",
      "Epoch 32/100\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.17893\n",
      "592/592 - 10s - 16ms/step - accuracy: 0.9802 - loss: 0.0867 - val_accuracy: 0.9482 - val_loss: 0.2876 - learning_rate: 3.5709e-04\n",
      "Epoch 33/100\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.17893\n",
      "592/592 - 9s - 16ms/step - accuracy: 0.9818 - loss: 0.0835 - val_accuracy: 0.9573 - val_loss: 0.2434 - learning_rate: 3.5709e-04\n",
      "Epoch 34/100\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.17893\n",
      "592/592 - 10s - 17ms/step - accuracy: 0.9835 - loss: 0.0770 - val_accuracy: 0.9454 - val_loss: 0.3307 - learning_rate: 3.5709e-04\n",
      "Epoch 35/100\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.17893\n",
      "592/592 - 9s - 16ms/step - accuracy: 0.9840 - loss: 0.0756 - val_accuracy: 0.9495 - val_loss: 0.2998 - learning_rate: 3.5709e-04\n",
      "Epoch 36/100\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.17893\n",
      "592/592 - 9s - 16ms/step - accuracy: 0.9817 - loss: 0.0796 - val_accuracy: 0.9507 - val_loss: 0.2755 - learning_rate: 3.5709e-04\n",
      "Epoch 37/100\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.17893\n",
      "592/592 - 10s - 16ms/step - accuracy: 0.9845 - loss: 0.0711 - val_accuracy: 0.9562 - val_loss: 0.2425 - learning_rate: 3.5709e-04\n",
      "Epoch 38/100\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.17893\n",
      "592/592 - 10s - 17ms/step - accuracy: 0.9851 - loss: 0.0697 - val_accuracy: 0.9455 - val_loss: 0.2993 - learning_rate: 3.5709e-04\n",
      "Epoch 39/100\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.17893\n",
      "592/592 - 10s - 17ms/step - accuracy: 0.9832 - loss: 0.0757 - val_accuracy: 0.9569 - val_loss: 0.2377 - learning_rate: 3.5709e-04\n",
      "Epoch 40/100\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 0.00017854664474725723.\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.17893\n",
      "592/592 - 10s - 17ms/step - accuracy: 0.9844 - loss: 0.0721 - val_accuracy: 0.9491 - val_loss: 0.3098 - learning_rate: 3.5709e-04\n",
      "Epoch 40: early stopping\n",
      "Restoring model weights from the end of the best epoch: 30.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\n",
      "Epoch 1: val_loss improved from None to 0.77946, saving model to outputs/step4_final_loso\\models\\fold_15\\best_model.keras\n",
      "591/591 - 12s - 20ms/step - accuracy: 0.5012 - loss: 1.5251 - val_accuracy: 0.7656 - val_loss: 0.7795 - learning_rate: 3.5709e-04\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 2: val_loss improved from 0.77946 to 0.52315, saving model to outputs/step4_final_loso\\models\\fold_15\\best_model.keras\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.7688 - loss: 0.7364 - val_accuracy: 0.8378 - val_loss: 0.5231 - learning_rate: 3.5709e-04\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 3: val_loss improved from 0.52315 to 0.43064, saving model to outputs/step4_final_loso\\models\\fold_15\\best_model.keras\n",
      "591/591 - 11s - 18ms/step - accuracy: 0.8452 - loss: 0.5029 - val_accuracy: 0.8660 - val_loss: 0.4306 - learning_rate: 3.5709e-04\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 4: val_loss improved from 0.43064 to 0.35062, saving model to outputs/step4_final_loso\\models\\fold_15\\best_model.keras\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.8836 - loss: 0.3865 - val_accuracy: 0.8914 - val_loss: 0.3506 - learning_rate: 3.5709e-04\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.35062\n",
      "591/591 - 11s - 18ms/step - accuracy: 0.9043 - loss: 0.3232 - val_accuracy: 0.8793 - val_loss: 0.4043 - learning_rate: 3.5709e-04\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 6: val_loss improved from 0.35062 to 0.31142, saving model to outputs/step4_final_loso\\models\\fold_15\\best_model.keras\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9223 - loss: 0.2707 - val_accuracy: 0.9090 - val_loss: 0.3114 - learning_rate: 3.5709e-04\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.31142\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9306 - loss: 0.2451 - val_accuracy: 0.8978 - val_loss: 0.3515 - learning_rate: 3.5709e-04\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 8: val_loss improved from 0.31142 to 0.29676, saving model to outputs/step4_final_loso\\models\\fold_15\\best_model.keras\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9391 - loss: 0.2198 - val_accuracy: 0.9189 - val_loss: 0.2968 - learning_rate: 3.5709e-04\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.29676\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9443 - loss: 0.2024 - val_accuracy: 0.9234 - val_loss: 0.3066 - learning_rate: 3.5709e-04\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.29676\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9515 - loss: 0.1805 - val_accuracy: 0.9145 - val_loss: 0.3430 - learning_rate: 3.5709e-04\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 11: val_loss improved from 0.29676 to 0.26397, saving model to outputs/step4_final_loso\\models\\fold_15\\best_model.keras\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9542 - loss: 0.1738 - val_accuracy: 0.9272 - val_loss: 0.2640 - learning_rate: 3.5709e-04\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 12: val_loss improved from 0.26397 to 0.23440, saving model to outputs/step4_final_loso\\models\\fold_15\\best_model.keras\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9558 - loss: 0.1656 - val_accuracy: 0.9394 - val_loss: 0.2344 - learning_rate: 3.5709e-04\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.23440\n",
      "591/591 - 11s - 18ms/step - accuracy: 0.9606 - loss: 0.1512 - val_accuracy: 0.9299 - val_loss: 0.2654 - learning_rate: 3.5709e-04\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.23440\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9629 - loss: 0.1409 - val_accuracy: 0.9403 - val_loss: 0.2693 - learning_rate: 3.5709e-04\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 15: val_loss improved from 0.23440 to 0.23377, saving model to outputs/step4_final_loso\\models\\fold_15\\best_model.keras\n",
      "591/591 - 9s - 16ms/step - accuracy: 0.9630 - loss: 0.1407 - val_accuracy: 0.9431 - val_loss: 0.2338 - learning_rate: 3.5709e-04\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.23377\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9685 - loss: 0.1299 - val_accuracy: 0.9397 - val_loss: 0.3080 - learning_rate: 3.5709e-04\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.23377\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9675 - loss: 0.1271 - val_accuracy: 0.9412 - val_loss: 0.2381 - learning_rate: 3.5709e-04\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.23377\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9701 - loss: 0.1179 - val_accuracy: 0.9281 - val_loss: 0.3544 - learning_rate: 3.5709e-04\n",
      "Epoch 19/100\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.23377\n",
      "591/591 - 11s - 18ms/step - accuracy: 0.9716 - loss: 0.1137 - val_accuracy: 0.9424 - val_loss: 0.2592 - learning_rate: 3.5709e-04\n",
      "Epoch 20/100\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.23377\n",
      "591/591 - 11s - 19ms/step - accuracy: 0.9733 - loss: 0.1063 - val_accuracy: 0.9318 - val_loss: 0.3132 - learning_rate: 3.5709e-04\n",
      "Epoch 21/100\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.23377\n",
      "591/591 - 10s - 16ms/step - accuracy: 0.9743 - loss: 0.1045 - val_accuracy: 0.9422 - val_loss: 0.2813 - learning_rate: 3.5709e-04\n",
      "Epoch 22/100\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.23377\n",
      "591/591 - 10s - 16ms/step - accuracy: 0.9772 - loss: 0.0992 - val_accuracy: 0.9351 - val_loss: 0.3104 - learning_rate: 3.5709e-04\n",
      "Epoch 23/100\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.23377\n",
      "591/591 - 9s - 16ms/step - accuracy: 0.9760 - loss: 0.0992 - val_accuracy: 0.9510 - val_loss: 0.2466 - learning_rate: 3.5709e-04\n",
      "Epoch 24/100\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.23377\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9762 - loss: 0.0992 - val_accuracy: 0.9400 - val_loss: 0.2837 - learning_rate: 3.5709e-04\n",
      "Epoch 25/100\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.00017854664474725723.\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.23377\n",
      "591/591 - 10s - 16ms/step - accuracy: 0.9787 - loss: 0.0920 - val_accuracy: 0.9465 - val_loss: 0.2419 - learning_rate: 3.5709e-04\n",
      "Epoch 25: early stopping\n",
      "Restoring model weights from the end of the best epoch: 15.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\n",
      "Epoch 1: val_loss improved from None to 0.75143, saving model to outputs/step4_final_loso\\models\\fold_16\\best_model.keras\n",
      "591/591 - 12s - 20ms/step - accuracy: 0.5007 - loss: 1.5283 - val_accuracy: 0.7739 - val_loss: 0.7514 - learning_rate: 3.5709e-04\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 2: val_loss improved from 0.75143 to 0.48449, saving model to outputs/step4_final_loso\\models\\fold_16\\best_model.keras\n",
      "591/591 - 11s - 18ms/step - accuracy: 0.7665 - loss: 0.7379 - val_accuracy: 0.8528 - val_loss: 0.4845 - learning_rate: 3.5709e-04\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 3: val_loss improved from 0.48449 to 0.43304, saving model to outputs/step4_final_loso\\models\\fold_16\\best_model.keras\n",
      "591/591 - 10s - 16ms/step - accuracy: 0.8520 - loss: 0.4868 - val_accuracy: 0.8742 - val_loss: 0.4330 - learning_rate: 3.5709e-04\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 4: val_loss improved from 0.43304 to 0.35054, saving model to outputs/step4_final_loso\\models\\fold_16\\best_model.keras\n",
      "591/591 - 11s - 19ms/step - accuracy: 0.8889 - loss: 0.3772 - val_accuracy: 0.9017 - val_loss: 0.3505 - learning_rate: 3.5709e-04\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 5: val_loss improved from 0.35054 to 0.31363, saving model to outputs/step4_final_loso\\models\\fold_16\\best_model.keras\n",
      "591/591 - 10s - 16ms/step - accuracy: 0.9113 - loss: 0.3134 - val_accuracy: 0.9039 - val_loss: 0.3136 - learning_rate: 3.5709e-04\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.31363\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9265 - loss: 0.2640 - val_accuracy: 0.9038 - val_loss: 0.3328 - learning_rate: 3.5709e-04\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 7: val_loss improved from 0.31363 to 0.31093, saving model to outputs/step4_final_loso\\models\\fold_16\\best_model.keras\n",
      "591/591 - 10s - 18ms/step - accuracy: 0.9330 - loss: 0.2397 - val_accuracy: 0.9113 - val_loss: 0.3109 - learning_rate: 3.5709e-04\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.31093\n",
      "591/591 - 11s - 18ms/step - accuracy: 0.9440 - loss: 0.2089 - val_accuracy: 0.9170 - val_loss: 0.3128 - learning_rate: 3.5709e-04\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 9: val_loss improved from 0.31093 to 0.27856, saving model to outputs/step4_final_loso\\models\\fold_16\\best_model.keras\n",
      "591/591 - 11s - 18ms/step - accuracy: 0.9473 - loss: 0.1965 - val_accuracy: 0.9295 - val_loss: 0.2786 - learning_rate: 3.5709e-04\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.27856\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9552 - loss: 0.1741 - val_accuracy: 0.9185 - val_loss: 0.3419 - learning_rate: 3.5709e-04\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 11: val_loss improved from 0.27856 to 0.23367, saving model to outputs/step4_final_loso\\models\\fold_16\\best_model.keras\n",
      "591/591 - 10s - 16ms/step - accuracy: 0.9580 - loss: 0.1612 - val_accuracy: 0.9366 - val_loss: 0.2337 - learning_rate: 3.5709e-04\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.23367\n",
      "591/591 - 9s - 16ms/step - accuracy: 0.9615 - loss: 0.1532 - val_accuracy: 0.9391 - val_loss: 0.2407 - learning_rate: 3.5709e-04\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 13: val_loss improved from 0.23367 to 0.22511, saving model to outputs/step4_final_loso\\models\\fold_16\\best_model.keras\n",
      "591/591 - 9s - 15ms/step - accuracy: 0.9624 - loss: 0.1474 - val_accuracy: 0.9407 - val_loss: 0.2251 - learning_rate: 3.5709e-04\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.22511\n",
      "591/591 - 9s - 15ms/step - accuracy: 0.9668 - loss: 0.1355 - val_accuracy: 0.9430 - val_loss: 0.2695 - learning_rate: 3.5709e-04\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.22511\n",
      "591/591 - 9s - 15ms/step - accuracy: 0.9673 - loss: 0.1293 - val_accuracy: 0.9421 - val_loss: 0.2567 - learning_rate: 3.5709e-04\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.22511\n",
      "591/591 - 9s - 16ms/step - accuracy: 0.9714 - loss: 0.1216 - val_accuracy: 0.9355 - val_loss: 0.3072 - learning_rate: 3.5709e-04\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.22511\n",
      "591/591 - 9s - 15ms/step - accuracy: 0.9710 - loss: 0.1222 - val_accuracy: 0.9376 - val_loss: 0.3112 - learning_rate: 3.5709e-04\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.22511\n",
      "591/591 - 9s - 16ms/step - accuracy: 0.9738 - loss: 0.1092 - val_accuracy: 0.9462 - val_loss: 0.2626 - learning_rate: 3.5709e-04\n",
      "Epoch 19/100\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.22511\n",
      "591/591 - 9s - 16ms/step - accuracy: 0.9755 - loss: 0.1076 - val_accuracy: 0.9419 - val_loss: 0.2596 - learning_rate: 3.5709e-04\n",
      "Epoch 20/100\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.22511\n",
      "591/591 - 9s - 16ms/step - accuracy: 0.9758 - loss: 0.1043 - val_accuracy: 0.9413 - val_loss: 0.2943 - learning_rate: 3.5709e-04\n",
      "Epoch 21/100\n",
      "\n",
      "Epoch 21: val_loss improved from 0.22511 to 0.22248, saving model to outputs/step4_final_loso\\models\\fold_16\\best_model.keras\n",
      "591/591 - 9s - 16ms/step - accuracy: 0.9763 - loss: 0.1005 - val_accuracy: 0.9538 - val_loss: 0.2225 - learning_rate: 3.5709e-04\n",
      "Epoch 22/100\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.22248\n",
      "591/591 - 9s - 16ms/step - accuracy: 0.9791 - loss: 0.0923 - val_accuracy: 0.9507 - val_loss: 0.2422 - learning_rate: 3.5709e-04\n",
      "Epoch 23/100\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.22248\n",
      "591/591 - 9s - 16ms/step - accuracy: 0.9797 - loss: 0.0919 - val_accuracy: 0.9439 - val_loss: 0.2888 - learning_rate: 3.5709e-04\n",
      "Epoch 24/100\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.22248\n",
      "591/591 - 10s - 16ms/step - accuracy: 0.9796 - loss: 0.0930 - val_accuracy: 0.9295 - val_loss: 0.4013 - learning_rate: 3.5709e-04\n",
      "Epoch 25/100\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.22248\n",
      "591/591 - 9s - 16ms/step - accuracy: 0.9806 - loss: 0.0866 - val_accuracy: 0.9540 - val_loss: 0.2534 - learning_rate: 3.5709e-04\n",
      "Epoch 26/100\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.22248\n",
      "591/591 - 10s - 16ms/step - accuracy: 0.9818 - loss: 0.0825 - val_accuracy: 0.9470 - val_loss: 0.2976 - learning_rate: 3.5709e-04\n",
      "Epoch 27/100\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.22248\n",
      "591/591 - 10s - 16ms/step - accuracy: 0.9826 - loss: 0.0796 - val_accuracy: 0.9516 - val_loss: 0.2716 - learning_rate: 3.5709e-04\n",
      "Epoch 28/100\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.22248\n",
      "591/591 - 9s - 16ms/step - accuracy: 0.9816 - loss: 0.0838 - val_accuracy: 0.9548 - val_loss: 0.2623 - learning_rate: 3.5709e-04\n",
      "Epoch 29/100\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.22248\n",
      "591/591 - 11s - 19ms/step - accuracy: 0.9843 - loss: 0.0728 - val_accuracy: 0.9537 - val_loss: 0.2933 - learning_rate: 3.5709e-04\n",
      "Epoch 30/100\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.22248\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9832 - loss: 0.0770 - val_accuracy: 0.9514 - val_loss: 0.3262 - learning_rate: 3.5709e-04\n",
      "Epoch 31/100\n",
      "\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 0.00017854664474725723.\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.22248\n",
      "591/591 - 9s - 16ms/step - accuracy: 0.9826 - loss: 0.0778 - val_accuracy: 0.9470 - val_loss: 0.3089 - learning_rate: 3.5709e-04\n",
      "Epoch 31: early stopping\n",
      "Restoring model weights from the end of the best epoch: 21.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\n",
      "Epoch 1: val_loss improved from None to 0.74393, saving model to outputs/step4_final_loso\\models\\fold_17\\best_model.keras\n",
      "591/591 - 12s - 20ms/step - accuracy: 0.5006 - loss: 1.5276 - val_accuracy: 0.7613 - val_loss: 0.7439 - learning_rate: 3.5709e-04\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 2: val_loss improved from 0.74393 to 0.48081, saving model to outputs/step4_final_loso\\models\\fold_17\\best_model.keras\n",
      "591/591 - 9s - 16ms/step - accuracy: 0.7739 - loss: 0.7123 - val_accuracy: 0.8432 - val_loss: 0.4808 - learning_rate: 3.5709e-04\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 3: val_loss improved from 0.48081 to 0.41668, saving model to outputs/step4_final_loso\\models\\fold_17\\best_model.keras\n",
      "591/591 - 9s - 16ms/step - accuracy: 0.8555 - loss: 0.4738 - val_accuracy: 0.8760 - val_loss: 0.4167 - learning_rate: 3.5709e-04\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 4: val_loss improved from 0.41668 to 0.35735, saving model to outputs/step4_final_loso\\models\\fold_17\\best_model.keras\n",
      "591/591 - 9s - 16ms/step - accuracy: 0.8913 - loss: 0.3629 - val_accuracy: 0.8907 - val_loss: 0.3574 - learning_rate: 3.5709e-04\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 5: val_loss improved from 0.35735 to 0.28778, saving model to outputs/step4_final_loso\\models\\fold_17\\best_model.keras\n",
      "591/591 - 9s - 15ms/step - accuracy: 0.9100 - loss: 0.3091 - val_accuracy: 0.9106 - val_loss: 0.2878 - learning_rate: 3.5709e-04\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.28778\n",
      "591/591 - 9s - 16ms/step - accuracy: 0.9278 - loss: 0.2591 - val_accuracy: 0.9017 - val_loss: 0.3230 - learning_rate: 3.5709e-04\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.28778\n",
      "591/591 - 9s - 15ms/step - accuracy: 0.9348 - loss: 0.2357 - val_accuracy: 0.9128 - val_loss: 0.3239 - learning_rate: 3.5709e-04\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.28778\n",
      "591/591 - 9s - 16ms/step - accuracy: 0.9447 - loss: 0.2056 - val_accuracy: 0.9162 - val_loss: 0.3113 - learning_rate: 3.5709e-04\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.28778\n",
      "591/591 - 11s - 19ms/step - accuracy: 0.9503 - loss: 0.1881 - val_accuracy: 0.8929 - val_loss: 0.4674 - learning_rate: 3.5709e-04\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.28778\n",
      "591/591 - 9s - 15ms/step - accuracy: 0.9548 - loss: 0.1713 - val_accuracy: 0.9272 - val_loss: 0.3103 - learning_rate: 3.5709e-04\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.28778\n",
      "591/591 - 9s - 15ms/step - accuracy: 0.9585 - loss: 0.1560 - val_accuracy: 0.9155 - val_loss: 0.3495 - learning_rate: 3.5709e-04\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 12: val_loss improved from 0.28778 to 0.27751, saving model to outputs/step4_final_loso\\models\\fold_17\\best_model.keras\n",
      "591/591 - 9s - 15ms/step - accuracy: 0.9610 - loss: 0.1546 - val_accuracy: 0.9345 - val_loss: 0.2775 - learning_rate: 3.5709e-04\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.27751\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9647 - loss: 0.1385 - val_accuracy: 0.9250 - val_loss: 0.3057 - learning_rate: 3.5709e-04\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 14: val_loss improved from 0.27751 to 0.27596, saving model to outputs/step4_final_loso\\models\\fold_17\\best_model.keras\n",
      "591/591 - 11s - 19ms/step - accuracy: 0.9666 - loss: 0.1316 - val_accuracy: 0.9361 - val_loss: 0.2760 - learning_rate: 3.5709e-04\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.27596\n",
      "591/591 - 11s - 19ms/step - accuracy: 0.9685 - loss: 0.1259 - val_accuracy: 0.9394 - val_loss: 0.2876 - learning_rate: 3.5709e-04\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.27596\n",
      "591/591 - 11s - 19ms/step - accuracy: 0.9692 - loss: 0.1268 - val_accuracy: 0.9300 - val_loss: 0.3246 - learning_rate: 3.5709e-04\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.27596\n",
      "591/591 - 11s - 18ms/step - accuracy: 0.9723 - loss: 0.1143 - val_accuracy: 0.9427 - val_loss: 0.3065 - learning_rate: 3.5709e-04\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.27596\n",
      "591/591 - 11s - 19ms/step - accuracy: 0.9751 - loss: 0.1072 - val_accuracy: 0.9394 - val_loss: 0.3431 - learning_rate: 3.5709e-04\n",
      "Epoch 19/100\n",
      "\n",
      "Epoch 19: val_loss improved from 0.27596 to 0.25703, saving model to outputs/step4_final_loso\\models\\fold_17\\best_model.keras\n",
      "591/591 - 11s - 19ms/step - accuracy: 0.9739 - loss: 0.1069 - val_accuracy: 0.9479 - val_loss: 0.2570 - learning_rate: 3.5709e-04\n",
      "Epoch 20/100\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.25703\n",
      "591/591 - 11s - 19ms/step - accuracy: 0.9751 - loss: 0.1072 - val_accuracy: 0.9467 - val_loss: 0.2922 - learning_rate: 3.5709e-04\n",
      "Epoch 21/100\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.25703\n",
      "591/591 - 11s - 18ms/step - accuracy: 0.9780 - loss: 0.0981 - val_accuracy: 0.9401 - val_loss: 0.3012 - learning_rate: 3.5709e-04\n",
      "Epoch 22/100\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.25703\n",
      "591/591 - 11s - 18ms/step - accuracy: 0.9781 - loss: 0.0944 - val_accuracy: 0.9330 - val_loss: 0.3821 - learning_rate: 3.5709e-04\n",
      "Epoch 23/100\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.25703\n",
      "591/591 - 11s - 19ms/step - accuracy: 0.9795 - loss: 0.0901 - val_accuracy: 0.9425 - val_loss: 0.3134 - learning_rate: 3.5709e-04\n",
      "Epoch 24/100\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.25703\n",
      "591/591 - 11s - 18ms/step - accuracy: 0.9804 - loss: 0.0877 - val_accuracy: 0.9427 - val_loss: 0.3068 - learning_rate: 3.5709e-04\n",
      "Epoch 25/100\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.25703\n",
      "591/591 - 9s - 16ms/step - accuracy: 0.9805 - loss: 0.0851 - val_accuracy: 0.9447 - val_loss: 0.3274 - learning_rate: 3.5709e-04\n",
      "Epoch 26/100\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.25703\n",
      "591/591 - 9s - 15ms/step - accuracy: 0.9819 - loss: 0.0818 - val_accuracy: 0.9464 - val_loss: 0.3160 - learning_rate: 3.5709e-04\n",
      "Epoch 27/100\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.25703\n",
      "591/591 - 9s - 15ms/step - accuracy: 0.9821 - loss: 0.0803 - val_accuracy: 0.9442 - val_loss: 0.3539 - learning_rate: 3.5709e-04\n",
      "Epoch 28/100\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.25703\n",
      "591/591 - 9s - 15ms/step - accuracy: 0.9828 - loss: 0.0798 - val_accuracy: 0.9455 - val_loss: 0.3048 - learning_rate: 3.5709e-04\n",
      "Epoch 29/100\n",
      "\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 0.00017854664474725723.\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.25703\n",
      "591/591 - 9s - 15ms/step - accuracy: 0.9827 - loss: 0.0785 - val_accuracy: 0.9412 - val_loss: 0.3333 - learning_rate: 3.5709e-04\n",
      "Epoch 29: early stopping\n",
      "Restoring model weights from the end of the best epoch: 19.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\n",
      "Epoch 1: val_loss improved from None to 0.76481, saving model to outputs/step4_final_loso\\models\\fold_18\\best_model.keras\n",
      "591/591 - 11s - 18ms/step - accuracy: 0.4927 - loss: 1.5529 - val_accuracy: 0.7616 - val_loss: 0.7648 - learning_rate: 3.5709e-04\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 2: val_loss improved from 0.76481 to 0.47006, saving model to outputs/step4_final_loso\\models\\fold_18\\best_model.keras\n",
      "591/591 - 9s - 16ms/step - accuracy: 0.7600 - loss: 0.7575 - val_accuracy: 0.8442 - val_loss: 0.4701 - learning_rate: 3.5709e-04\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 3: val_loss improved from 0.47006 to 0.40295, saving model to outputs/step4_final_loso\\models\\fold_18\\best_model.keras\n",
      "591/591 - 9s - 16ms/step - accuracy: 0.8461 - loss: 0.5047 - val_accuracy: 0.8699 - val_loss: 0.4029 - learning_rate: 3.5709e-04\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 4: val_loss improved from 0.40295 to 0.33569, saving model to outputs/step4_final_loso\\models\\fold_18\\best_model.keras\n",
      "591/591 - 9s - 16ms/step - accuracy: 0.8846 - loss: 0.3881 - val_accuracy: 0.9012 - val_loss: 0.3357 - learning_rate: 3.5709e-04\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 5: val_loss improved from 0.33569 to 0.32503, saving model to outputs/step4_final_loso\\models\\fold_18\\best_model.keras\n",
      "591/591 - 9s - 15ms/step - accuracy: 0.9049 - loss: 0.3243 - val_accuracy: 0.8980 - val_loss: 0.3250 - learning_rate: 3.5709e-04\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.32503\n",
      "591/591 - 9s - 15ms/step - accuracy: 0.9241 - loss: 0.2725 - val_accuracy: 0.8983 - val_loss: 0.3526 - learning_rate: 3.5709e-04\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 7: val_loss improved from 0.32503 to 0.28208, saving model to outputs/step4_final_loso\\models\\fold_18\\best_model.keras\n",
      "591/591 - 9s - 16ms/step - accuracy: 0.9316 - loss: 0.2490 - val_accuracy: 0.9098 - val_loss: 0.2821 - learning_rate: 3.5709e-04\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.28208\n",
      "591/591 - 9s - 16ms/step - accuracy: 0.9413 - loss: 0.2171 - val_accuracy: 0.9205 - val_loss: 0.3017 - learning_rate: 3.5709e-04\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.28208\n",
      "591/591 - 9s - 16ms/step - accuracy: 0.9481 - loss: 0.1989 - val_accuracy: 0.9113 - val_loss: 0.4053 - learning_rate: 3.5709e-04\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.28208\n",
      "591/591 - 9s - 16ms/step - accuracy: 0.9526 - loss: 0.1761 - val_accuracy: 0.9199 - val_loss: 0.3501 - learning_rate: 3.5709e-04\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 11: val_loss improved from 0.28208 to 0.25160, saving model to outputs/step4_final_loso\\models\\fold_18\\best_model.keras\n",
      "591/591 - 12s - 20ms/step - accuracy: 0.9569 - loss: 0.1650 - val_accuracy: 0.9317 - val_loss: 0.2516 - learning_rate: 3.5709e-04\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.25160\n",
      "591/591 - 11s - 19ms/step - accuracy: 0.9588 - loss: 0.1579 - val_accuracy: 0.9338 - val_loss: 0.2854 - learning_rate: 3.5709e-04\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.25160\n",
      "591/591 - 12s - 20ms/step - accuracy: 0.9649 - loss: 0.1398 - val_accuracy: 0.9363 - val_loss: 0.2758 - learning_rate: 3.5709e-04\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.25160\n",
      "591/591 - 11s - 19ms/step - accuracy: 0.9678 - loss: 0.1316 - val_accuracy: 0.9418 - val_loss: 0.3147 - learning_rate: 3.5709e-04\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.25160\n",
      "591/591 - 12s - 20ms/step - accuracy: 0.9687 - loss: 0.1265 - val_accuracy: 0.9306 - val_loss: 0.3168 - learning_rate: 3.5709e-04\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.25160\n",
      "591/591 - 15s - 25ms/step - accuracy: 0.9701 - loss: 0.1224 - val_accuracy: 0.9357 - val_loss: 0.3656 - learning_rate: 3.5709e-04\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.25160\n",
      "591/591 - 14s - 25ms/step - accuracy: 0.9717 - loss: 0.1161 - val_accuracy: 0.9413 - val_loss: 0.2790 - learning_rate: 3.5709e-04\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.25160\n",
      "591/591 - 22s - 37ms/step - accuracy: 0.9757 - loss: 0.1025 - val_accuracy: 0.9413 - val_loss: 0.3115 - learning_rate: 3.5709e-04\n",
      "Epoch 19/100\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.25160\n",
      "591/591 - 16s - 27ms/step - accuracy: 0.9753 - loss: 0.1058 - val_accuracy: 0.9514 - val_loss: 0.2615 - learning_rate: 3.5709e-04\n",
      "Epoch 20/100\n",
      "\n",
      "Epoch 20: val_loss improved from 0.25160 to 0.24238, saving model to outputs/step4_final_loso\\models\\fold_18\\best_model.keras\n",
      "591/591 - 20s - 34ms/step - accuracy: 0.9750 - loss: 0.1047 - val_accuracy: 0.9537 - val_loss: 0.2424 - learning_rate: 3.5709e-04\n",
      "Epoch 21/100\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.24238\n",
      "591/591 - 16s - 26ms/step - accuracy: 0.9774 - loss: 0.0954 - val_accuracy: 0.9467 - val_loss: 0.2744 - learning_rate: 3.5709e-04\n",
      "Epoch 22/100\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.24238\n",
      "591/591 - 16s - 26ms/step - accuracy: 0.9781 - loss: 0.0941 - val_accuracy: 0.9547 - val_loss: 0.2532 - learning_rate: 3.5709e-04\n",
      "Epoch 23/100\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.24238\n",
      "591/591 - 15s - 25ms/step - accuracy: 0.9792 - loss: 0.0928 - val_accuracy: 0.9499 - val_loss: 0.2677 - learning_rate: 3.5709e-04\n",
      "Epoch 24/100\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.24238\n",
      "591/591 - 14s - 24ms/step - accuracy: 0.9801 - loss: 0.0885 - val_accuracy: 0.9434 - val_loss: 0.3134 - learning_rate: 3.5709e-04\n",
      "Epoch 25/100\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.24238\n",
      "591/591 - 14s - 24ms/step - accuracy: 0.9817 - loss: 0.0842 - val_accuracy: 0.9556 - val_loss: 0.2566 - learning_rate: 3.5709e-04\n",
      "Epoch 26/100\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.24238\n",
      "591/591 - 15s - 25ms/step - accuracy: 0.9835 - loss: 0.0789 - val_accuracy: 0.9514 - val_loss: 0.2870 - learning_rate: 3.5709e-04\n",
      "Epoch 27/100\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.24238\n",
      "591/591 - 15s - 25ms/step - accuracy: 0.9811 - loss: 0.0858 - val_accuracy: 0.9473 - val_loss: 0.2876 - learning_rate: 3.5709e-04\n",
      "Epoch 28/100\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.24238\n",
      "591/591 - 15s - 26ms/step - accuracy: 0.9829 - loss: 0.0817 - val_accuracy: 0.9497 - val_loss: 0.2625 - learning_rate: 3.5709e-04\n",
      "Epoch 29/100\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.24238\n",
      "591/591 - 15s - 25ms/step - accuracy: 0.9838 - loss: 0.0774 - val_accuracy: 0.9431 - val_loss: 0.3404 - learning_rate: 3.5709e-04\n",
      "Epoch 30/100\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 0.00017854664474725723.\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.24238\n",
      "591/591 - 15s - 25ms/step - accuracy: 0.9835 - loss: 0.0771 - val_accuracy: 0.9508 - val_loss: 0.2852 - learning_rate: 3.5709e-04\n",
      "Epoch 30: early stopping\n",
      "Restoring model weights from the end of the best epoch: 20.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\n",
      "Epoch 1: val_loss improved from None to 0.74955, saving model to outputs/step4_final_loso\\models\\fold_19\\best_model.keras\n",
      "591/591 - 17s - 30ms/step - accuracy: 0.4976 - loss: 1.5428 - val_accuracy: 0.7597 - val_loss: 0.7495 - learning_rate: 3.5709e-04\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 2: val_loss improved from 0.74955 to 0.50686, saving model to outputs/step4_final_loso\\models\\fold_19\\best_model.keras\n",
      "591/591 - 20s - 34ms/step - accuracy: 0.7688 - loss: 0.7378 - val_accuracy: 0.8302 - val_loss: 0.5069 - learning_rate: 3.5709e-04\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 3: val_loss improved from 0.50686 to 0.44036, saving model to outputs/step4_final_loso\\models\\fold_19\\best_model.keras\n",
      "591/591 - 15s - 25ms/step - accuracy: 0.8509 - loss: 0.4856 - val_accuracy: 0.8608 - val_loss: 0.4404 - learning_rate: 3.5709e-04\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 4: val_loss improved from 0.44036 to 0.33821, saving model to outputs/step4_final_loso\\models\\fold_19\\best_model.keras\n",
      "591/591 - 15s - 25ms/step - accuracy: 0.8858 - loss: 0.3836 - val_accuracy: 0.9002 - val_loss: 0.3382 - learning_rate: 3.5709e-04\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 5: val_loss improved from 0.33821 to 0.32221, saving model to outputs/step4_final_loso\\models\\fold_19\\best_model.keras\n",
      "591/591 - 14s - 24ms/step - accuracy: 0.9079 - loss: 0.3173 - val_accuracy: 0.8987 - val_loss: 0.3222 - learning_rate: 3.5709e-04\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 6: val_loss improved from 0.32221 to 0.29131, saving model to outputs/step4_final_loso\\models\\fold_19\\best_model.keras\n",
      "591/591 - 21s - 35ms/step - accuracy: 0.9215 - loss: 0.2780 - val_accuracy: 0.9134 - val_loss: 0.2913 - learning_rate: 3.5709e-04\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.29131\n",
      "591/591 - 15s - 25ms/step - accuracy: 0.9319 - loss: 0.2455 - val_accuracy: 0.9100 - val_loss: 0.3144 - learning_rate: 3.5709e-04\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 8: val_loss improved from 0.29131 to 0.27665, saving model to outputs/step4_final_loso\\models\\fold_19\\best_model.keras\n",
      "591/591 - 15s - 25ms/step - accuracy: 0.9358 - loss: 0.2270 - val_accuracy: 0.9260 - val_loss: 0.2767 - learning_rate: 3.5709e-04\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.27665\n",
      "591/591 - 15s - 25ms/step - accuracy: 0.9460 - loss: 0.2025 - val_accuracy: 0.9116 - val_loss: 0.3639 - learning_rate: 3.5709e-04\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.27665\n",
      "591/591 - 14s - 24ms/step - accuracy: 0.9505 - loss: 0.1813 - val_accuracy: 0.9222 - val_loss: 0.3137 - learning_rate: 3.5709e-04\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.27665\n",
      "591/591 - 15s - 26ms/step - accuracy: 0.9536 - loss: 0.1727 - val_accuracy: 0.9266 - val_loss: 0.2936 - learning_rate: 3.5709e-04\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.27665\n",
      "591/591 - 20s - 34ms/step - accuracy: 0.9574 - loss: 0.1633 - val_accuracy: 0.9391 - val_loss: 0.3036 - learning_rate: 3.5709e-04\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.27665\n",
      "591/591 - 14s - 24ms/step - accuracy: 0.9599 - loss: 0.1511 - val_accuracy: 0.9228 - val_loss: 0.3181 - learning_rate: 3.5709e-04\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.27665\n",
      "591/591 - 14s - 24ms/step - accuracy: 0.9635 - loss: 0.1420 - val_accuracy: 0.9342 - val_loss: 0.3361 - learning_rate: 3.5709e-04\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.27665\n",
      "591/591 - 14s - 24ms/step - accuracy: 0.9655 - loss: 0.1369 - val_accuracy: 0.9330 - val_loss: 0.3138 - learning_rate: 3.5709e-04\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.27665\n",
      "591/591 - 14s - 24ms/step - accuracy: 0.9667 - loss: 0.1349 - val_accuracy: 0.9244 - val_loss: 0.3415 - learning_rate: 3.5709e-04\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.27665\n",
      "591/591 - 14s - 24ms/step - accuracy: 0.9694 - loss: 0.1217 - val_accuracy: 0.9332 - val_loss: 0.3141 - learning_rate: 3.5709e-04\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.00017854664474725723.\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.27665\n",
      "591/591 - 21s - 35ms/step - accuracy: 0.9735 - loss: 0.1118 - val_accuracy: 0.9321 - val_loss: 0.3477 - learning_rate: 3.5709e-04\n",
      "Epoch 18: early stopping\n",
      "Restoring model weights from the end of the best epoch: 8.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\n",
      "Epoch 1: val_loss improved from None to 0.75576, saving model to outputs/step4_final_loso\\models\\fold_20\\best_model.keras\n",
      "591/591 - 19s - 32ms/step - accuracy: 0.4937 - loss: 1.5514 - val_accuracy: 0.7500 - val_loss: 0.7558 - learning_rate: 3.5709e-04\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 2: val_loss improved from 0.75576 to 0.49678, saving model to outputs/step4_final_loso\\models\\fold_20\\best_model.keras\n",
      "591/591 - 19s - 33ms/step - accuracy: 0.7614 - loss: 0.7489 - val_accuracy: 0.8310 - val_loss: 0.4968 - learning_rate: 3.5709e-04\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 3: val_loss improved from 0.49678 to 0.44008, saving model to outputs/step4_final_loso\\models\\fold_20\\best_model.keras\n",
      "591/591 - 21s - 35ms/step - accuracy: 0.8485 - loss: 0.4976 - val_accuracy: 0.8503 - val_loss: 0.4401 - learning_rate: 3.5709e-04\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 4: val_loss improved from 0.44008 to 0.34712, saving model to outputs/step4_final_loso\\models\\fold_20\\best_model.keras\n",
      "591/591 - 20s - 34ms/step - accuracy: 0.8854 - loss: 0.3836 - val_accuracy: 0.8917 - val_loss: 0.3471 - learning_rate: 3.5709e-04\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 5: val_loss improved from 0.34712 to 0.31571, saving model to outputs/step4_final_loso\\models\\fold_20\\best_model.keras\n",
      "591/591 - 21s - 36ms/step - accuracy: 0.9068 - loss: 0.3248 - val_accuracy: 0.8956 - val_loss: 0.3157 - learning_rate: 3.5709e-04\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 6: val_loss improved from 0.31571 to 0.30901, saving model to outputs/step4_final_loso\\models\\fold_20\\best_model.keras\n",
      "591/591 - 14s - 23ms/step - accuracy: 0.9205 - loss: 0.2772 - val_accuracy: 0.9115 - val_loss: 0.3090 - learning_rate: 3.5709e-04\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 7: val_loss improved from 0.30901 to 0.29279, saving model to outputs/step4_final_loso\\models\\fold_20\\best_model.keras\n",
      "591/591 - 14s - 23ms/step - accuracy: 0.9292 - loss: 0.2471 - val_accuracy: 0.9122 - val_loss: 0.2928 - learning_rate: 3.5709e-04\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 8: val_loss improved from 0.29279 to 0.26777, saving model to outputs/step4_final_loso\\models\\fold_20\\best_model.keras\n",
      "591/591 - 14s - 23ms/step - accuracy: 0.9363 - loss: 0.2289 - val_accuracy: 0.9240 - val_loss: 0.2678 - learning_rate: 3.5709e-04\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.26777\n",
      "591/591 - 14s - 23ms/step - accuracy: 0.9436 - loss: 0.2067 - val_accuracy: 0.9231 - val_loss: 0.2939 - learning_rate: 3.5709e-04\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.26777\n",
      "591/591 - 21s - 35ms/step - accuracy: 0.9514 - loss: 0.1801 - val_accuracy: 0.9196 - val_loss: 0.3318 - learning_rate: 3.5709e-04\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.26777\n",
      "591/591 - 20s - 34ms/step - accuracy: 0.9525 - loss: 0.1751 - val_accuracy: 0.9153 - val_loss: 0.3185 - learning_rate: 3.5709e-04\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.26777\n",
      "591/591 - 13s - 23ms/step - accuracy: 0.9577 - loss: 0.1624 - val_accuracy: 0.9265 - val_loss: 0.2855 - learning_rate: 3.5709e-04\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 13: val_loss improved from 0.26777 to 0.23256, saving model to outputs/step4_final_loso\\models\\fold_20\\best_model.keras\n",
      "591/591 - 14s - 24ms/step - accuracy: 0.9598 - loss: 0.1544 - val_accuracy: 0.9443 - val_loss: 0.2326 - learning_rate: 3.5709e-04\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.23256\n",
      "591/591 - 20s - 34ms/step - accuracy: 0.9640 - loss: 0.1395 - val_accuracy: 0.9369 - val_loss: 0.3017 - learning_rate: 3.5709e-04\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.23256\n",
      "591/591 - 21s - 35ms/step - accuracy: 0.9646 - loss: 0.1383 - val_accuracy: 0.9321 - val_loss: 0.3006 - learning_rate: 3.5709e-04\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.23256\n",
      "591/591 - 20s - 34ms/step - accuracy: 0.9683 - loss: 0.1281 - val_accuracy: 0.9404 - val_loss: 0.2713 - learning_rate: 3.5709e-04\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.23256\n",
      "591/591 - 14s - 23ms/step - accuracy: 0.9674 - loss: 0.1305 - val_accuracy: 0.9352 - val_loss: 0.2574 - learning_rate: 3.5709e-04\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.23256\n",
      "591/591 - 21s - 35ms/step - accuracy: 0.9725 - loss: 0.1160 - val_accuracy: 0.9360 - val_loss: 0.2982 - learning_rate: 3.5709e-04\n",
      "Epoch 19/100\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.23256\n",
      "591/591 - 20s - 34ms/step - accuracy: 0.9712 - loss: 0.1172 - val_accuracy: 0.9379 - val_loss: 0.2773 - learning_rate: 3.5709e-04\n",
      "Epoch 20/100\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.23256\n",
      "591/591 - 21s - 35ms/step - accuracy: 0.9746 - loss: 0.1070 - val_accuracy: 0.9407 - val_loss: 0.2719 - learning_rate: 3.5709e-04\n",
      "Epoch 21/100\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.23256\n",
      "591/591 - 21s - 35ms/step - accuracy: 0.9754 - loss: 0.1012 - val_accuracy: 0.9373 - val_loss: 0.3316 - learning_rate: 3.5709e-04\n",
      "Epoch 22/100\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.23256\n",
      "591/591 - 13s - 23ms/step - accuracy: 0.9757 - loss: 0.1038 - val_accuracy: 0.9404 - val_loss: 0.2841 - learning_rate: 3.5709e-04\n",
      "Epoch 23/100\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.00017854664474725723.\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.23256\n",
      "591/591 - 13s - 23ms/step - accuracy: 0.9764 - loss: 0.0991 - val_accuracy: 0.9445 - val_loss: 0.2859 - learning_rate: 3.5709e-04\n",
      "Epoch 23: early stopping\n",
      "Restoring model weights from the end of the best epoch: 13.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\n",
      "Epoch 1: val_loss improved from None to 0.75923, saving model to outputs/step4_final_loso\\models\\fold_21\\best_model.keras\n",
      "591/591 - 18s - 31ms/step - accuracy: 0.5001 - loss: 1.5359 - val_accuracy: 0.7558 - val_loss: 0.7592 - learning_rate: 3.5709e-04\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 2: val_loss improved from 0.75923 to 0.48978, saving model to outputs/step4_final_loso\\models\\fold_21\\best_model.keras\n",
      "591/591 - 20s - 33ms/step - accuracy: 0.7612 - loss: 0.7625 - val_accuracy: 0.8455 - val_loss: 0.4898 - learning_rate: 3.5709e-04\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 3: val_loss improved from 0.48978 to 0.45986, saving model to outputs/step4_final_loso\\models\\fold_21\\best_model.keras\n",
      "591/591 - 21s - 35ms/step - accuracy: 0.8434 - loss: 0.5100 - val_accuracy: 0.8500 - val_loss: 0.4599 - learning_rate: 3.5709e-04\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 4: val_loss improved from 0.45986 to 0.35479, saving model to outputs/step4_final_loso\\models\\fold_21\\best_model.keras\n",
      "591/591 - 17s - 28ms/step - accuracy: 0.8814 - loss: 0.3974 - val_accuracy: 0.8891 - val_loss: 0.3548 - learning_rate: 3.5709e-04\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.35479\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9033 - loss: 0.3317 - val_accuracy: 0.8867 - val_loss: 0.3674 - learning_rate: 3.5709e-04\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 6: val_loss improved from 0.35479 to 0.31594, saving model to outputs/step4_final_loso\\models\\fold_21\\best_model.keras\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9206 - loss: 0.2769 - val_accuracy: 0.9051 - val_loss: 0.3159 - learning_rate: 3.5709e-04\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 7: val_loss improved from 0.31594 to 0.28936, saving model to outputs/step4_final_loso\\models\\fold_21\\best_model.keras\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9300 - loss: 0.2461 - val_accuracy: 0.9217 - val_loss: 0.2894 - learning_rate: 3.5709e-04\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.28936\n",
      "591/591 - 9s - 16ms/step - accuracy: 0.9382 - loss: 0.2213 - val_accuracy: 0.9191 - val_loss: 0.3008 - learning_rate: 3.5709e-04\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.28936\n",
      "591/591 - 9s - 14ms/step - accuracy: 0.9448 - loss: 0.2014 - val_accuracy: 0.9253 - val_loss: 0.2912 - learning_rate: 3.5709e-04\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.28936\n",
      "591/591 - 9s - 14ms/step - accuracy: 0.9519 - loss: 0.1797 - val_accuracy: 0.9128 - val_loss: 0.3572 - learning_rate: 3.5709e-04\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 11: val_loss improved from 0.28936 to 0.27515, saving model to outputs/step4_final_loso\\models\\fold_21\\best_model.keras\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9542 - loss: 0.1689 - val_accuracy: 0.9308 - val_loss: 0.2752 - learning_rate: 3.5709e-04\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 12: val_loss improved from 0.27515 to 0.25029, saving model to outputs/step4_final_loso\\models\\fold_21\\best_model.keras\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9584 - loss: 0.1590 - val_accuracy: 0.9479 - val_loss: 0.2503 - learning_rate: 3.5709e-04\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.25029\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9620 - loss: 0.1475 - val_accuracy: 0.9312 - val_loss: 0.3164 - learning_rate: 3.5709e-04\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.25029\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9653 - loss: 0.1372 - val_accuracy: 0.9488 - val_loss: 0.2895 - learning_rate: 3.5709e-04\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.25029\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9676 - loss: 0.1315 - val_accuracy: 0.9495 - val_loss: 0.2536 - learning_rate: 3.5709e-04\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.25029\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9696 - loss: 0.1248 - val_accuracy: 0.9462 - val_loss: 0.2571 - learning_rate: 3.5709e-04\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.25029\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9692 - loss: 0.1212 - val_accuracy: 0.9477 - val_loss: 0.2905 - learning_rate: 3.5709e-04\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 18: val_loss improved from 0.25029 to 0.24340, saving model to outputs/step4_final_loso\\models\\fold_21\\best_model.keras\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9746 - loss: 0.1108 - val_accuracy: 0.9482 - val_loss: 0.2434 - learning_rate: 3.5709e-04\n",
      "Epoch 19/100\n",
      "\n",
      "Epoch 19: val_loss improved from 0.24340 to 0.22131, saving model to outputs/step4_final_loso\\models\\fold_21\\best_model.keras\n",
      "591/591 - 11s - 18ms/step - accuracy: 0.9746 - loss: 0.1062 - val_accuracy: 0.9562 - val_loss: 0.2213 - learning_rate: 3.5709e-04\n",
      "Epoch 20/100\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.22131\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9734 - loss: 0.1069 - val_accuracy: 0.9455 - val_loss: 0.3051 - learning_rate: 3.5709e-04\n",
      "Epoch 21/100\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.22131\n",
      "591/591 - 10s - 18ms/step - accuracy: 0.9773 - loss: 0.0975 - val_accuracy: 0.9419 - val_loss: 0.3530 - learning_rate: 3.5709e-04\n",
      "Epoch 22/100\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.22131\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9781 - loss: 0.0972 - val_accuracy: 0.9516 - val_loss: 0.3343 - learning_rate: 3.5709e-04\n",
      "Epoch 23/100\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.22131\n",
      "591/591 - 10s - 17ms/step - accuracy: 0.9792 - loss: 0.0916 - val_accuracy: 0.9574 - val_loss: 0.2691 - learning_rate: 3.5709e-04\n",
      "Epoch 24/100\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.22131\n",
      "591/591 - 8s - 14ms/step - accuracy: 0.9787 - loss: 0.0942 - val_accuracy: 0.9525 - val_loss: 0.2804 - learning_rate: 3.5709e-04\n",
      "Epoch 25/100\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.22131\n",
      "591/591 - 8s - 14ms/step - accuracy: 0.9803 - loss: 0.0836 - val_accuracy: 0.9554 - val_loss: 0.2730 - learning_rate: 3.5709e-04\n",
      "Epoch 26/100\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.22131\n",
      "591/591 - 8s - 14ms/step - accuracy: 0.9819 - loss: 0.0818 - val_accuracy: 0.9502 - val_loss: 0.2964 - learning_rate: 3.5709e-04\n",
      "Epoch 27/100\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.22131\n",
      "591/591 - 8s - 14ms/step - accuracy: 0.9802 - loss: 0.0837 - val_accuracy: 0.9458 - val_loss: 0.2714 - learning_rate: 3.5709e-04\n",
      "Epoch 28/100\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.22131\n",
      "591/591 - 8s - 14ms/step - accuracy: 0.9819 - loss: 0.0844 - val_accuracy: 0.9465 - val_loss: 0.3144 - learning_rate: 3.5709e-04\n",
      "Epoch 29/100\n",
      "\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 0.00017854664474725723.\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.22131\n",
      "591/591 - 8s - 14ms/step - accuracy: 0.9815 - loss: 0.0811 - val_accuracy: 0.9595 - val_loss: 0.2657 - learning_rate: 3.5709e-04\n",
      "Epoch 29: early stopping\n",
      "Restoring model weights from the end of the best epoch: 19.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\n",
      "Epoch 1: val_loss improved from None to 0.92062, saving model to outputs/step4_final_loso\\models\\fold_22\\best_model.keras\n",
      "592/592 - 12s - 21ms/step - accuracy: 0.4797 - loss: 1.5890 - val_accuracy: 0.7040 - val_loss: 0.9206 - learning_rate: 3.5709e-04\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 2: val_loss improved from 0.92062 to 0.48657, saving model to outputs/step4_final_loso\\models\\fold_22\\best_model.keras\n",
      "592/592 - 11s - 18ms/step - accuracy: 0.7549 - loss: 0.7763 - val_accuracy: 0.8426 - val_loss: 0.4866 - learning_rate: 3.5709e-04\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 3: val_loss improved from 0.48657 to 0.40843, saving model to outputs/step4_final_loso\\models\\fold_22\\best_model.keras\n",
      "592/592 - 11s - 18ms/step - accuracy: 0.8434 - loss: 0.5154 - val_accuracy: 0.8727 - val_loss: 0.4084 - learning_rate: 3.5709e-04\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.40843\n",
      "592/592 - 11s - 18ms/step - accuracy: 0.8852 - loss: 0.3962 - val_accuracy: 0.8606 - val_loss: 0.4620 - learning_rate: 3.5709e-04\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 5: val_loss improved from 0.40843 to 0.36590, saving model to outputs/step4_final_loso\\models\\fold_22\\best_model.keras\n",
      "592/592 - 11s - 18ms/step - accuracy: 0.9052 - loss: 0.3345 - val_accuracy: 0.8928 - val_loss: 0.3659 - learning_rate: 3.5709e-04\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 6: val_loss improved from 0.36590 to 0.29093, saving model to outputs/step4_final_loso\\models\\fold_22\\best_model.keras\n",
      "592/592 - 9s - 15ms/step - accuracy: 0.9191 - loss: 0.2784 - val_accuracy: 0.9133 - val_loss: 0.2909 - learning_rate: 3.5709e-04\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.29093\n",
      "592/592 - 9s - 14ms/step - accuracy: 0.9275 - loss: 0.2549 - val_accuracy: 0.9153 - val_loss: 0.3231 - learning_rate: 3.5709e-04\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 8: val_loss improved from 0.29093 to 0.24602, saving model to outputs/step4_final_loso\\models\\fold_22\\best_model.keras\n",
      "592/592 - 8s - 14ms/step - accuracy: 0.9362 - loss: 0.2280 - val_accuracy: 0.9284 - val_loss: 0.2460 - learning_rate: 3.5709e-04\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 9: val_loss improved from 0.24602 to 0.24480, saving model to outputs/step4_final_loso\\models\\fold_22\\best_model.keras\n",
      "592/592 - 9s - 14ms/step - accuracy: 0.9433 - loss: 0.2085 - val_accuracy: 0.9334 - val_loss: 0.2448 - learning_rate: 3.5709e-04\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 10: val_loss improved from 0.24480 to 0.20657, saving model to outputs/step4_final_loso\\models\\fold_22\\best_model.keras\n",
      "592/592 - 8s - 14ms/step - accuracy: 0.9481 - loss: 0.1908 - val_accuracy: 0.9441 - val_loss: 0.2066 - learning_rate: 3.5709e-04\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.20657\n",
      "592/592 - 8s - 14ms/step - accuracy: 0.9534 - loss: 0.1753 - val_accuracy: 0.9393 - val_loss: 0.2313 - learning_rate: 3.5709e-04\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.20657\n",
      "592/592 - 8s - 14ms/step - accuracy: 0.9569 - loss: 0.1641 - val_accuracy: 0.9272 - val_loss: 0.3388 - learning_rate: 3.5709e-04\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.20657\n",
      "592/592 - 8s - 14ms/step - accuracy: 0.9599 - loss: 0.1532 - val_accuracy: 0.9545 - val_loss: 0.2120 - learning_rate: 3.5709e-04\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.20657\n",
      "592/592 - 8s - 14ms/step - accuracy: 0.9626 - loss: 0.1450 - val_accuracy: 0.9433 - val_loss: 0.2473 - learning_rate: 3.5709e-04\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.20657\n",
      "592/592 - 8s - 14ms/step - accuracy: 0.9646 - loss: 0.1393 - val_accuracy: 0.9485 - val_loss: 0.2573 - learning_rate: 3.5709e-04\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.20657\n",
      "592/592 - 9s - 14ms/step - accuracy: 0.9651 - loss: 0.1351 - val_accuracy: 0.9463 - val_loss: 0.2616 - learning_rate: 3.5709e-04\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.20657\n",
      "592/592 - 8s - 14ms/step - accuracy: 0.9683 - loss: 0.1227 - val_accuracy: 0.9414 - val_loss: 0.2601 - learning_rate: 3.5709e-04\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.20657\n",
      "592/592 - 8s - 14ms/step - accuracy: 0.9709 - loss: 0.1167 - val_accuracy: 0.9469 - val_loss: 0.2220 - learning_rate: 3.5709e-04\n",
      "Epoch 19/100\n",
      "\n",
      "Epoch 19: val_loss improved from 0.20657 to 0.19621, saving model to outputs/step4_final_loso\\models\\fold_22\\best_model.keras\n",
      "592/592 - 9s - 14ms/step - accuracy: 0.9701 - loss: 0.1176 - val_accuracy: 0.9525 - val_loss: 0.1962 - learning_rate: 3.5709e-04\n",
      "Epoch 20/100\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.19621\n",
      "592/592 - 9s - 15ms/step - accuracy: 0.9737 - loss: 0.1088 - val_accuracy: 0.9529 - val_loss: 0.2418 - learning_rate: 3.5709e-04\n",
      "Epoch 21/100\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.19621\n",
      "592/592 - 11s - 19ms/step - accuracy: 0.9731 - loss: 0.1104 - val_accuracy: 0.9503 - val_loss: 0.2479 - learning_rate: 3.5709e-04\n",
      "Epoch 22/100\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.19621\n",
      "592/592 - 11s - 19ms/step - accuracy: 0.9756 - loss: 0.1007 - val_accuracy: 0.9529 - val_loss: 0.2363 - learning_rate: 3.5709e-04\n",
      "Epoch 23/100\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.19621\n",
      "592/592 - 11s - 19ms/step - accuracy: 0.9750 - loss: 0.1035 - val_accuracy: 0.9573 - val_loss: 0.1980 - learning_rate: 3.5709e-04\n",
      "Epoch 24/100\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.19621\n",
      "592/592 - 11s - 19ms/step - accuracy: 0.9761 - loss: 0.0960 - val_accuracy: 0.9598 - val_loss: 0.2348 - learning_rate: 3.5709e-04\n",
      "Epoch 25/100\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.19621\n",
      "592/592 - 11s - 19ms/step - accuracy: 0.9783 - loss: 0.0947 - val_accuracy: 0.9576 - val_loss: 0.2468 - learning_rate: 3.5709e-04\n",
      "Epoch 26/100\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.19621\n",
      "592/592 - 12s - 19ms/step - accuracy: 0.9800 - loss: 0.0893 - val_accuracy: 0.9565 - val_loss: 0.2615 - learning_rate: 3.5709e-04\n",
      "Epoch 27/100\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.19621\n",
      "592/592 - 10s - 17ms/step - accuracy: 0.9796 - loss: 0.0875 - val_accuracy: 0.9535 - val_loss: 0.2621 - learning_rate: 3.5709e-04\n",
      "Epoch 28/100\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.19621\n",
      "592/592 - 8s - 14ms/step - accuracy: 0.9805 - loss: 0.0889 - val_accuracy: 0.9415 - val_loss: 0.3441 - learning_rate: 3.5709e-04\n",
      "Epoch 29/100\n",
      "\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 0.00017854664474725723.\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.19621\n",
      "592/592 - 8s - 14ms/step - accuracy: 0.9818 - loss: 0.0824 - val_accuracy: 0.9399 - val_loss: 0.3852 - learning_rate: 3.5709e-04\n",
      "Epoch 29: early stopping\n",
      "Restoring model weights from the end of the best epoch: 19.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\n",
      "Epoch 1: val_loss improved from None to 0.99920, saving model to outputs/step4_final_loso\\models\\fold_23\\best_model.keras\n",
      "534/534 - 9s - 18ms/step - accuracy: 0.4564 - loss: 1.6609 - val_accuracy: 0.7120 - val_loss: 0.9992 - learning_rate: 3.5709e-04\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 2: val_loss improved from 0.99920 to 0.67149, saving model to outputs/step4_final_loso\\models\\fold_23\\best_model.keras\n",
      "534/534 - 10s - 19ms/step - accuracy: 0.7122 - loss: 0.8680 - val_accuracy: 0.8470 - val_loss: 0.6715 - learning_rate: 3.5709e-04\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 3: val_loss improved from 0.67149 to 0.60817, saving model to outputs/step4_final_loso\\models\\fold_23\\best_model.keras\n",
      "534/534 - 8s - 15ms/step - accuracy: 0.8081 - loss: 0.5888 - val_accuracy: 0.8605 - val_loss: 0.6082 - learning_rate: 3.5709e-04\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 4: val_loss improved from 0.60817 to 0.56892, saving model to outputs/step4_final_loso\\models\\fold_23\\best_model.keras\n",
      "534/534 - 8s - 14ms/step - accuracy: 0.8544 - loss: 0.4631 - val_accuracy: 0.8810 - val_loss: 0.5689 - learning_rate: 3.5709e-04\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 5: val_loss improved from 0.56892 to 0.52589, saving model to outputs/step4_final_loso\\models\\fold_23\\best_model.keras\n",
      "534/534 - 8s - 14ms/step - accuracy: 0.8840 - loss: 0.3792 - val_accuracy: 0.8986 - val_loss: 0.5259 - learning_rate: 3.5709e-04\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 6: val_loss improved from 0.52589 to 0.51953, saving model to outputs/step4_final_loso\\models\\fold_23\\best_model.keras\n",
      "534/534 - 8s - 15ms/step - accuracy: 0.9006 - loss: 0.3254 - val_accuracy: 0.8935 - val_loss: 0.5195 - learning_rate: 3.5709e-04\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.51953\n",
      "534/534 - 8s - 14ms/step - accuracy: 0.9193 - loss: 0.2786 - val_accuracy: 0.9029 - val_loss: 0.5229 - learning_rate: 3.5709e-04\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 8: val_loss improved from 0.51953 to 0.46840, saving model to outputs/step4_final_loso\\models\\fold_23\\best_model.keras\n",
      "534/534 - 8s - 14ms/step - accuracy: 0.9270 - loss: 0.2521 - val_accuracy: 0.9228 - val_loss: 0.4684 - learning_rate: 3.5709e-04\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.46840\n",
      "534/534 - 8s - 14ms/step - accuracy: 0.9343 - loss: 0.2305 - val_accuracy: 0.9157 - val_loss: 0.5207 - learning_rate: 3.5709e-04\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.46840\n",
      "534/534 - 7s - 14ms/step - accuracy: 0.9410 - loss: 0.2084 - val_accuracy: 0.9214 - val_loss: 0.5145 - learning_rate: 3.5709e-04\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.46840\n",
      "534/534 - 8s - 14ms/step - accuracy: 0.9474 - loss: 0.1936 - val_accuracy: 0.9167 - val_loss: 0.4899 - learning_rate: 3.5709e-04\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.46840\n",
      "534/534 - 8s - 14ms/step - accuracy: 0.9491 - loss: 0.1837 - val_accuracy: 0.9207 - val_loss: 0.4964 - learning_rate: 3.5709e-04\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 13: val_loss improved from 0.46840 to 0.45901, saving model to outputs/step4_final_loso\\models\\fold_23\\best_model.keras\n",
      "534/534 - 8s - 14ms/step - accuracy: 0.9567 - loss: 0.1610 - val_accuracy: 0.9323 - val_loss: 0.4590 - learning_rate: 3.5709e-04\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.45901\n",
      "534/534 - 8s - 15ms/step - accuracy: 0.9582 - loss: 0.1574 - val_accuracy: 0.9348 - val_loss: 0.4806 - learning_rate: 3.5709e-04\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.45901\n",
      "534/534 - 8s - 15ms/step - accuracy: 0.9630 - loss: 0.1427 - val_accuracy: 0.9273 - val_loss: 0.4977 - learning_rate: 3.5709e-04\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 16: val_loss improved from 0.45901 to 0.44649, saving model to outputs/step4_final_loso\\models\\fold_23\\best_model.keras\n",
      "534/534 - 8s - 14ms/step - accuracy: 0.9619 - loss: 0.1439 - val_accuracy: 0.9401 - val_loss: 0.4465 - learning_rate: 3.5709e-04\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.44649\n",
      "534/534 - 8s - 15ms/step - accuracy: 0.9651 - loss: 0.1336 - val_accuracy: 0.9366 - val_loss: 0.4613 - learning_rate: 3.5709e-04\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 18: val_loss improved from 0.44649 to 0.42068, saving model to outputs/step4_final_loso\\models\\fold_23\\best_model.keras\n",
      "534/534 - 8s - 14ms/step - accuracy: 0.9670 - loss: 0.1318 - val_accuracy: 0.9494 - val_loss: 0.4207 - learning_rate: 3.5709e-04\n",
      "Epoch 19/100\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.42068\n",
      "534/534 - 8s - 14ms/step - accuracy: 0.9672 - loss: 0.1282 - val_accuracy: 0.9336 - val_loss: 0.5191 - learning_rate: 3.5709e-04\n",
      "Epoch 20/100\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.42068\n",
      "534/534 - 7s - 14ms/step - accuracy: 0.9707 - loss: 0.1206 - val_accuracy: 0.9489 - val_loss: 0.4696 - learning_rate: 3.5709e-04\n",
      "Epoch 21/100\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.42068\n",
      "534/534 - 7s - 14ms/step - accuracy: 0.9739 - loss: 0.1061 - val_accuracy: 0.9409 - val_loss: 0.5052 - learning_rate: 3.5709e-04\n",
      "Epoch 22/100\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.42068\n",
      "534/534 - 7s - 14ms/step - accuracy: 0.9741 - loss: 0.1090 - val_accuracy: 0.9489 - val_loss: 0.5061 - learning_rate: 3.5709e-04\n",
      "Epoch 23/100\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.42068\n",
      "534/534 - 8s - 14ms/step - accuracy: 0.9747 - loss: 0.1061 - val_accuracy: 0.9452 - val_loss: 0.5042 - learning_rate: 3.5709e-04\n",
      "Epoch 24/100\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.42068\n",
      "534/534 - 8s - 14ms/step - accuracy: 0.9758 - loss: 0.1009 - val_accuracy: 0.9489 - val_loss: 0.4999 - learning_rate: 3.5709e-04\n",
      "Epoch 25/100\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.42068\n",
      "534/534 - 8s - 14ms/step - accuracy: 0.9759 - loss: 0.1030 - val_accuracy: 0.9494 - val_loss: 0.4875 - learning_rate: 3.5709e-04\n",
      "Epoch 26/100\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.42068\n",
      "534/534 - 8s - 14ms/step - accuracy: 0.9783 - loss: 0.0950 - val_accuracy: 0.9462 - val_loss: 0.5133 - learning_rate: 3.5709e-04\n",
      "Epoch 27/100\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.42068\n",
      "534/534 - 8s - 14ms/step - accuracy: 0.9802 - loss: 0.0882 - val_accuracy: 0.9472 - val_loss: 0.5459 - learning_rate: 3.5709e-04\n",
      "Epoch 28/100\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 0.00017854664474725723.\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.42068\n",
      "534/534 - 8s - 14ms/step - accuracy: 0.9782 - loss: 0.0934 - val_accuracy: 0.9443 - val_loss: 0.5377 - learning_rate: 3.5709e-04\n",
      "Epoch 28: early stopping\n",
      "Restoring model weights from the end of the best epoch: 18.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\n",
      "Epoch 1: val_loss improved from None to 0.74717, saving model to outputs/step4_final_loso\\models\\fold_24\\best_model.keras\n",
      "532/532 - 9s - 17ms/step - accuracy: 0.4624 - loss: 1.6383 - val_accuracy: 0.7737 - val_loss: 0.7472 - learning_rate: 3.5709e-04\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 2: val_loss improved from 0.74717 to 0.40365, saving model to outputs/step4_final_loso\\models\\fold_24\\best_model.keras\n",
      "532/532 - 7s - 14ms/step - accuracy: 0.7203 - loss: 0.8429 - val_accuracy: 0.8648 - val_loss: 0.4037 - learning_rate: 3.5709e-04\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 3: val_loss improved from 0.40365 to 0.29031, saving model to outputs/step4_final_loso\\models\\fold_24\\best_model.keras\n",
      "532/532 - 7s - 14ms/step - accuracy: 0.8097 - loss: 0.5887 - val_accuracy: 0.9044 - val_loss: 0.2903 - learning_rate: 3.5709e-04\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 4: val_loss improved from 0.29031 to 0.27331, saving model to outputs/step4_final_loso\\models\\fold_24\\best_model.keras\n",
      "532/532 - 7s - 14ms/step - accuracy: 0.8565 - loss: 0.4564 - val_accuracy: 0.9089 - val_loss: 0.2733 - learning_rate: 3.5709e-04\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 5: val_loss improved from 0.27331 to 0.22946, saving model to outputs/step4_final_loso\\models\\fold_24\\best_model.keras\n",
      "532/532 - 7s - 13ms/step - accuracy: 0.8837 - loss: 0.3731 - val_accuracy: 0.9187 - val_loss: 0.2295 - learning_rate: 3.5709e-04\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 6: val_loss improved from 0.22946 to 0.20728, saving model to outputs/step4_final_loso\\models\\fold_24\\best_model.keras\n",
      "532/532 - 7s - 13ms/step - accuracy: 0.8999 - loss: 0.3310 - val_accuracy: 0.9345 - val_loss: 0.2073 - learning_rate: 3.5709e-04\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 7: val_loss improved from 0.20728 to 0.20495, saving model to outputs/step4_final_loso\\models\\fold_24\\best_model.keras\n",
      "532/532 - 7s - 14ms/step - accuracy: 0.9137 - loss: 0.2878 - val_accuracy: 0.9292 - val_loss: 0.2050 - learning_rate: 3.5709e-04\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 8: val_loss improved from 0.20495 to 0.20178, saving model to outputs/step4_final_loso\\models\\fold_24\\best_model.keras\n",
      "532/532 - 7s - 14ms/step - accuracy: 0.9239 - loss: 0.2578 - val_accuracy: 0.9345 - val_loss: 0.2018 - learning_rate: 3.5709e-04\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 9: val_loss improved from 0.20178 to 0.19658, saving model to outputs/step4_final_loso\\models\\fold_24\\best_model.keras\n",
      "532/532 - 7s - 13ms/step - accuracy: 0.9323 - loss: 0.2364 - val_accuracy: 0.9370 - val_loss: 0.1966 - learning_rate: 3.5709e-04\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 10: val_loss improved from 0.19658 to 0.19323, saving model to outputs/step4_final_loso\\models\\fold_24\\best_model.keras\n",
      "532/532 - 7s - 13ms/step - accuracy: 0.9391 - loss: 0.2138 - val_accuracy: 0.9472 - val_loss: 0.1932 - learning_rate: 3.5709e-04\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 11: val_loss improved from 0.19323 to 0.16981, saving model to outputs/step4_final_loso\\models\\fold_24\\best_model.keras\n",
      "532/532 - 7s - 14ms/step - accuracy: 0.9432 - loss: 0.2001 - val_accuracy: 0.9564 - val_loss: 0.1698 - learning_rate: 3.5709e-04\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.16981\n",
      "532/532 - 8s - 15ms/step - accuracy: 0.9484 - loss: 0.1872 - val_accuracy: 0.9477 - val_loss: 0.1754 - learning_rate: 3.5709e-04\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.16981\n",
      "532/532 - 9s - 17ms/step - accuracy: 0.9543 - loss: 0.1691 - val_accuracy: 0.9438 - val_loss: 0.2064 - learning_rate: 3.5709e-04\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 14: val_loss improved from 0.16981 to 0.14759, saving model to outputs/step4_final_loso\\models\\fold_24\\best_model.keras\n",
      "532/532 - 9s - 17ms/step - accuracy: 0.9566 - loss: 0.1587 - val_accuracy: 0.9657 - val_loss: 0.1476 - learning_rate: 3.5709e-04\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.14759\n",
      "532/532 - 9s - 18ms/step - accuracy: 0.9601 - loss: 0.1523 - val_accuracy: 0.9567 - val_loss: 0.1807 - learning_rate: 3.5709e-04\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.14759\n",
      "532/532 - 9s - 17ms/step - accuracy: 0.9621 - loss: 0.1443 - val_accuracy: 0.9542 - val_loss: 0.1745 - learning_rate: 3.5709e-04\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 17: val_loss improved from 0.14759 to 0.14365, saving model to outputs/step4_final_loso\\models\\fold_24\\best_model.keras\n",
      "532/532 - 9s - 17ms/step - accuracy: 0.9646 - loss: 0.1351 - val_accuracy: 0.9628 - val_loss: 0.1437 - learning_rate: 3.5709e-04\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.14365\n",
      "532/532 - 10s - 18ms/step - accuracy: 0.9673 - loss: 0.1283 - val_accuracy: 0.9603 - val_loss: 0.1826 - learning_rate: 3.5709e-04\n",
      "Epoch 19/100\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.14365\n",
      "532/532 - 10s - 19ms/step - accuracy: 0.9690 - loss: 0.1230 - val_accuracy: 0.9481 - val_loss: 0.2308 - learning_rate: 3.5709e-04\n",
      "Epoch 20/100\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.14365\n",
      "532/532 - 8s - 16ms/step - accuracy: 0.9666 - loss: 0.1260 - val_accuracy: 0.9584 - val_loss: 0.1712 - learning_rate: 3.5709e-04\n",
      "Epoch 21/100\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.14365\n",
      "532/532 - 9s - 17ms/step - accuracy: 0.9723 - loss: 0.1118 - val_accuracy: 0.9640 - val_loss: 0.1589 - learning_rate: 3.5709e-04\n",
      "Epoch 22/100\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.14365\n",
      "532/532 - 9s - 18ms/step - accuracy: 0.9740 - loss: 0.1063 - val_accuracy: 0.9519 - val_loss: 0.2186 - learning_rate: 3.5709e-04\n",
      "Epoch 23/100\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.14365\n",
      "532/532 - 9s - 17ms/step - accuracy: 0.9726 - loss: 0.1119 - val_accuracy: 0.9522 - val_loss: 0.2427 - learning_rate: 3.5709e-04\n",
      "Epoch 24/100\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.14365\n",
      "532/532 - 9s - 16ms/step - accuracy: 0.9759 - loss: 0.1011 - val_accuracy: 0.9516 - val_loss: 0.2063 - learning_rate: 3.5709e-04\n",
      "Epoch 25/100\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.14365\n",
      "532/532 - 9s - 16ms/step - accuracy: 0.9771 - loss: 0.0976 - val_accuracy: 0.9620 - val_loss: 0.1468 - learning_rate: 3.5709e-04\n",
      "Epoch 26/100\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.14365\n",
      "532/532 - 9s - 17ms/step - accuracy: 0.9759 - loss: 0.0995 - val_accuracy: 0.9610 - val_loss: 0.1679 - learning_rate: 3.5709e-04\n",
      "Epoch 27/100\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 0.00017854664474725723.\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.14365\n",
      "532/532 - 10s - 18ms/step - accuracy: 0.9770 - loss: 0.0998 - val_accuracy: 0.9618 - val_loss: 0.1642 - learning_rate: 3.5709e-04\n",
      "Epoch 27: early stopping\n",
      "Restoring model weights from the end of the best epoch: 17.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\n",
      "Epoch 1: val_loss improved from None to 0.81637, saving model to outputs/step4_final_loso\\models\\fold_25\\best_model.keras\n",
      "534/534 - 12s - 22ms/step - accuracy: 0.4507 - loss: 1.6760 - val_accuracy: 0.7657 - val_loss: 0.8164 - learning_rate: 3.5709e-04\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 2: val_loss improved from 0.81637 to 0.47956, saving model to outputs/step4_final_loso\\models\\fold_25\\best_model.keras\n",
      "534/534 - 10s - 18ms/step - accuracy: 0.7200 - loss: 0.8806 - val_accuracy: 0.8545 - val_loss: 0.4796 - learning_rate: 3.5709e-04\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 3: val_loss improved from 0.47956 to 0.38970, saving model to outputs/step4_final_loso\\models\\fold_25\\best_model.keras\n",
      "534/534 - 7s - 14ms/step - accuracy: 0.8172 - loss: 0.5925 - val_accuracy: 0.8649 - val_loss: 0.3897 - learning_rate: 3.5709e-04\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 4: val_loss improved from 0.38970 to 0.31643, saving model to outputs/step4_final_loso\\models\\fold_25\\best_model.keras\n",
      "534/534 - 7s - 13ms/step - accuracy: 0.8617 - loss: 0.4527 - val_accuracy: 0.9001 - val_loss: 0.3164 - learning_rate: 3.5709e-04\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 5: val_loss improved from 0.31643 to 0.26955, saving model to outputs/step4_final_loso\\models\\fold_25\\best_model.keras\n",
      "534/534 - 7s - 13ms/step - accuracy: 0.8860 - loss: 0.3806 - val_accuracy: 0.9136 - val_loss: 0.2696 - learning_rate: 3.5709e-04\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.26955\n",
      "534/534 - 7s - 14ms/step - accuracy: 0.9036 - loss: 0.3298 - val_accuracy: 0.9174 - val_loss: 0.2812 - learning_rate: 3.5709e-04\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 7: val_loss improved from 0.26955 to 0.22305, saving model to outputs/step4_final_loso\\models\\fold_25\\best_model.keras\n",
      "534/534 - 10s - 20ms/step - accuracy: 0.9182 - loss: 0.2881 - val_accuracy: 0.9340 - val_loss: 0.2231 - learning_rate: 3.5709e-04\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.22305\n",
      "534/534 - 21s - 40ms/step - accuracy: 0.9266 - loss: 0.2621 - val_accuracy: 0.9325 - val_loss: 0.2658 - learning_rate: 3.5709e-04\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.22305\n",
      "534/534 - 20s - 38ms/step - accuracy: 0.9365 - loss: 0.2349 - val_accuracy: 0.9381 - val_loss: 0.2253 - learning_rate: 3.5709e-04\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 10: val_loss improved from 0.22305 to 0.21919, saving model to outputs/step4_final_loso\\models\\fold_25\\best_model.keras\n",
      "534/534 - 11s - 21ms/step - accuracy: 0.9401 - loss: 0.2168 - val_accuracy: 0.9373 - val_loss: 0.2192 - learning_rate: 3.5709e-04\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.21919\n",
      "534/534 - 16s - 30ms/step - accuracy: 0.9448 - loss: 0.2036 - val_accuracy: 0.9489 - val_loss: 0.2316 - learning_rate: 3.5709e-04\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 12: val_loss improved from 0.21919 to 0.18199, saving model to outputs/step4_final_loso\\models\\fold_25\\best_model.keras\n",
      "534/534 - 7s - 13ms/step - accuracy: 0.9498 - loss: 0.1875 - val_accuracy: 0.9499 - val_loss: 0.1820 - learning_rate: 3.5709e-04\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 13: val_loss improved from 0.18199 to 0.18016, saving model to outputs/step4_final_loso\\models\\fold_25\\best_model.keras\n",
      "534/534 - 7s - 13ms/step - accuracy: 0.9536 - loss: 0.1738 - val_accuracy: 0.9580 - val_loss: 0.1802 - learning_rate: 3.5709e-04\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.18016\n",
      "534/534 - 7s - 13ms/step - accuracy: 0.9578 - loss: 0.1624 - val_accuracy: 0.9457 - val_loss: 0.2452 - learning_rate: 3.5709e-04\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.18016\n",
      "534/534 - 7s - 13ms/step - accuracy: 0.9601 - loss: 0.1553 - val_accuracy: 0.9516 - val_loss: 0.2326 - learning_rate: 3.5709e-04\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 16: val_loss improved from 0.18016 to 0.17199, saving model to outputs/step4_final_loso\\models\\fold_25\\best_model.keras\n",
      "534/534 - 7s - 13ms/step - accuracy: 0.9613 - loss: 0.1484 - val_accuracy: 0.9569 - val_loss: 0.1720 - learning_rate: 3.5709e-04\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.17199\n",
      "534/534 - 7s - 13ms/step - accuracy: 0.9636 - loss: 0.1427 - val_accuracy: 0.9511 - val_loss: 0.2220 - learning_rate: 3.5709e-04\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.17199\n",
      "534/534 - 7s - 13ms/step - accuracy: 0.9655 - loss: 0.1361 - val_accuracy: 0.9540 - val_loss: 0.2086 - learning_rate: 3.5709e-04\n",
      "Epoch 19/100\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.17199\n",
      "534/534 - 7s - 13ms/step - accuracy: 0.9667 - loss: 0.1270 - val_accuracy: 0.9547 - val_loss: 0.2285 - learning_rate: 3.5709e-04\n",
      "Epoch 20/100\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.17199\n",
      "534/534 - 7s - 13ms/step - accuracy: 0.9681 - loss: 0.1282 - val_accuracy: 0.9610 - val_loss: 0.1988 - learning_rate: 3.5709e-04\n",
      "Epoch 21/100\n",
      "\n",
      "Epoch 21: val_loss improved from 0.17199 to 0.16281, saving model to outputs/step4_final_loso\\models\\fold_25\\best_model.keras\n",
      "534/534 - 7s - 13ms/step - accuracy: 0.9701 - loss: 0.1232 - val_accuracy: 0.9662 - val_loss: 0.1628 - learning_rate: 3.5709e-04\n",
      "Epoch 22/100\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.16281\n",
      "534/534 - 7s - 13ms/step - accuracy: 0.9718 - loss: 0.1170 - val_accuracy: 0.9642 - val_loss: 0.1696 - learning_rate: 3.5709e-04\n",
      "Epoch 23/100\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.16281\n",
      "534/534 - 7s - 13ms/step - accuracy: 0.9747 - loss: 0.1113 - val_accuracy: 0.9605 - val_loss: 0.2288 - learning_rate: 3.5709e-04\n",
      "Epoch 24/100\n",
      "\n",
      "Epoch 24: val_loss improved from 0.16281 to 0.13694, saving model to outputs/step4_final_loso\\models\\fold_25\\best_model.keras\n",
      "534/534 - 7s - 13ms/step - accuracy: 0.9744 - loss: 0.1063 - val_accuracy: 0.9668 - val_loss: 0.1369 - learning_rate: 3.5709e-04\n",
      "Epoch 25/100\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.13694\n",
      "534/534 - 7s - 13ms/step - accuracy: 0.9760 - loss: 0.1028 - val_accuracy: 0.9520 - val_loss: 0.2298 - learning_rate: 3.5709e-04\n",
      "Epoch 26/100\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.13694\n",
      "534/534 - 7s - 13ms/step - accuracy: 0.9766 - loss: 0.1011 - val_accuracy: 0.9637 - val_loss: 0.1863 - learning_rate: 3.5709e-04\n",
      "Epoch 27/100\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.13694\n",
      "534/534 - 7s - 13ms/step - accuracy: 0.9754 - loss: 0.1061 - val_accuracy: 0.9623 - val_loss: 0.1938 - learning_rate: 3.5709e-04\n",
      "Epoch 28/100\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.13694\n",
      "534/534 - 7s - 13ms/step - accuracy: 0.9772 - loss: 0.0992 - val_accuracy: 0.9720 - val_loss: 0.1621 - learning_rate: 3.5709e-04\n",
      "Epoch 29/100\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.13694\n",
      "534/534 - 7s - 13ms/step - accuracy: 0.9797 - loss: 0.0889 - val_accuracy: 0.9608 - val_loss: 0.1958 - learning_rate: 3.5709e-04\n",
      "Epoch 30/100\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.13694\n",
      "534/534 - 7s - 13ms/step - accuracy: 0.9798 - loss: 0.0905 - val_accuracy: 0.9643 - val_loss: 0.1538 - learning_rate: 3.5709e-04\n",
      "Epoch 31/100\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.13694\n",
      "534/534 - 7s - 13ms/step - accuracy: 0.9787 - loss: 0.0914 - val_accuracy: 0.9622 - val_loss: 0.2315 - learning_rate: 3.5709e-04\n",
      "Epoch 32/100\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.13694\n",
      "534/534 - 7s - 13ms/step - accuracy: 0.9802 - loss: 0.0898 - val_accuracy: 0.9698 - val_loss: 0.1775 - learning_rate: 3.5709e-04\n",
      "Epoch 33/100\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.13694\n",
      "534/534 - 7s - 13ms/step - accuracy: 0.9807 - loss: 0.0875 - val_accuracy: 0.9670 - val_loss: 0.1690 - learning_rate: 3.5709e-04\n",
      "Epoch 34/100\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 0.00017854664474725723.\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.13694\n",
      "534/534 - 7s - 13ms/step - accuracy: 0.9805 - loss: 0.0844 - val_accuracy: 0.9545 - val_loss: 0.1894 - learning_rate: 3.5709e-04\n",
      "Epoch 34: early stopping\n",
      "Restoring model weights from the end of the best epoch: 24.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\n",
      "Epoch 1: val_loss improved from None to 0.84556, saving model to outputs/step4_final_loso\\models\\fold_26\\best_model.keras\n",
      "574/574 - 9s - 16ms/step - accuracy: 0.4680 - loss: 1.6048 - val_accuracy: 0.7202 - val_loss: 0.8456 - learning_rate: 3.5709e-04\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 2: val_loss improved from 0.84556 to 0.46452, saving model to outputs/step4_final_loso\\models\\fold_26\\best_model.keras\n",
      "574/574 - 8s - 13ms/step - accuracy: 0.7397 - loss: 0.8022 - val_accuracy: 0.8508 - val_loss: 0.4645 - learning_rate: 3.5709e-04\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 3: val_loss improved from 0.46452 to 0.32821, saving model to outputs/step4_final_loso\\models\\fold_26\\best_model.keras\n",
      "574/574 - 8s - 13ms/step - accuracy: 0.8367 - loss: 0.5151 - val_accuracy: 0.8936 - val_loss: 0.3282 - learning_rate: 3.5709e-04\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 4: val_loss improved from 0.32821 to 0.26594, saving model to outputs/step4_final_loso\\models\\fold_26\\best_model.keras\n",
      "574/574 - 8s - 13ms/step - accuracy: 0.8757 - loss: 0.4042 - val_accuracy: 0.9193 - val_loss: 0.2659 - learning_rate: 3.5709e-04\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.26594\n",
      "574/574 - 8s - 13ms/step - accuracy: 0.9000 - loss: 0.3318 - val_accuracy: 0.9202 - val_loss: 0.2719 - learning_rate: 3.5709e-04\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.26594\n",
      "574/574 - 8s - 13ms/step - accuracy: 0.9182 - loss: 0.2795 - val_accuracy: 0.9205 - val_loss: 0.2730 - learning_rate: 3.5709e-04\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 7: val_loss improved from 0.26594 to 0.23698, saving model to outputs/step4_final_loso\\models\\fold_26\\best_model.keras\n",
      "574/574 - 8s - 13ms/step - accuracy: 0.9281 - loss: 0.2494 - val_accuracy: 0.9358 - val_loss: 0.2370 - learning_rate: 3.5709e-04\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.23698\n",
      "574/574 - 8s - 13ms/step - accuracy: 0.9371 - loss: 0.2251 - val_accuracy: 0.9243 - val_loss: 0.2635 - learning_rate: 3.5709e-04\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 9: val_loss improved from 0.23698 to 0.21326, saving model to outputs/step4_final_loso\\models\\fold_26\\best_model.keras\n",
      "574/574 - 8s - 13ms/step - accuracy: 0.9443 - loss: 0.2006 - val_accuracy: 0.9390 - val_loss: 0.2133 - learning_rate: 3.5709e-04\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 10: val_loss improved from 0.21326 to 0.20457, saving model to outputs/step4_final_loso\\models\\fold_26\\best_model.keras\n",
      "574/574 - 8s - 13ms/step - accuracy: 0.9493 - loss: 0.1820 - val_accuracy: 0.9498 - val_loss: 0.2046 - learning_rate: 3.5709e-04\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.20457\n",
      "574/574 - 7s - 13ms/step - accuracy: 0.9535 - loss: 0.1713 - val_accuracy: 0.9443 - val_loss: 0.2298 - learning_rate: 3.5709e-04\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 12: val_loss improved from 0.20457 to 0.17461, saving model to outputs/step4_final_loso\\models\\fold_26\\best_model.keras\n",
      "574/574 - 8s - 14ms/step - accuracy: 0.9578 - loss: 0.1594 - val_accuracy: 0.9594 - val_loss: 0.1746 - learning_rate: 3.5709e-04\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.17461\n",
      "574/574 - 8s - 13ms/step - accuracy: 0.9592 - loss: 0.1530 - val_accuracy: 0.9511 - val_loss: 0.2124 - learning_rate: 3.5709e-04\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.17461\n",
      "574/574 - 8s - 13ms/step - accuracy: 0.9633 - loss: 0.1379 - val_accuracy: 0.9606 - val_loss: 0.1755 - learning_rate: 3.5709e-04\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.17461\n",
      "574/574 - 8s - 13ms/step - accuracy: 0.9646 - loss: 0.1370 - val_accuracy: 0.9511 - val_loss: 0.1875 - learning_rate: 3.5709e-04\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.17461\n",
      "574/574 - 8s - 13ms/step - accuracy: 0.9671 - loss: 0.1276 - val_accuracy: 0.9548 - val_loss: 0.2053 - learning_rate: 3.5709e-04\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 17: val_loss improved from 0.17461 to 0.16968, saving model to outputs/step4_final_loso\\models\\fold_26\\best_model.keras\n",
      "574/574 - 8s - 13ms/step - accuracy: 0.9687 - loss: 0.1244 - val_accuracy: 0.9636 - val_loss: 0.1697 - learning_rate: 3.5709e-04\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.16968\n",
      "574/574 - 8s - 13ms/step - accuracy: 0.9707 - loss: 0.1168 - val_accuracy: 0.9598 - val_loss: 0.2098 - learning_rate: 3.5709e-04\n",
      "Epoch 19/100\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.16968\n",
      "574/574 - 8s - 13ms/step - accuracy: 0.9735 - loss: 0.1071 - val_accuracy: 0.9527 - val_loss: 0.2478 - learning_rate: 3.5709e-04\n",
      "Epoch 20/100\n",
      "\n",
      "Epoch 20: val_loss improved from 0.16968 to 0.15887, saving model to outputs/step4_final_loso\\models\\fold_26\\best_model.keras\n",
      "574/574 - 8s - 13ms/step - accuracy: 0.9746 - loss: 0.1057 - val_accuracy: 0.9670 - val_loss: 0.1589 - learning_rate: 3.5709e-04\n",
      "Epoch 21/100\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.15887\n",
      "574/574 - 8s - 14ms/step - accuracy: 0.9740 - loss: 0.1077 - val_accuracy: 0.9621 - val_loss: 0.1838 - learning_rate: 3.5709e-04\n",
      "Epoch 22/100\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.15887\n",
      "574/574 - 8s - 14ms/step - accuracy: 0.9751 - loss: 0.0991 - val_accuracy: 0.9568 - val_loss: 0.2105 - learning_rate: 3.5709e-04\n",
      "Epoch 23/100\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.15887\n",
      "574/574 - 8s - 13ms/step - accuracy: 0.9771 - loss: 0.0957 - val_accuracy: 0.9642 - val_loss: 0.1750 - learning_rate: 3.5709e-04\n",
      "Epoch 24/100\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.15887\n",
      "574/574 - 8s - 13ms/step - accuracy: 0.9779 - loss: 0.0919 - val_accuracy: 0.9606 - val_loss: 0.2200 - learning_rate: 3.5709e-04\n",
      "Epoch 25/100\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.15887\n",
      "574/574 - 8s - 13ms/step - accuracy: 0.9762 - loss: 0.0969 - val_accuracy: 0.9673 - val_loss: 0.1811 - learning_rate: 3.5709e-04\n",
      "Epoch 26/100\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.15887\n",
      "574/574 - 8s - 13ms/step - accuracy: 0.9805 - loss: 0.0859 - val_accuracy: 0.9559 - val_loss: 0.2943 - learning_rate: 3.5709e-04\n",
      "Epoch 27/100\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.15887\n",
      "574/574 - 8s - 13ms/step - accuracy: 0.9811 - loss: 0.0845 - val_accuracy: 0.9575 - val_loss: 0.2216 - learning_rate: 3.5709e-04\n",
      "Epoch 28/100\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.15887\n",
      "574/574 - 8s - 14ms/step - accuracy: 0.9804 - loss: 0.0826 - val_accuracy: 0.9629 - val_loss: 0.1928 - learning_rate: 3.5709e-04\n",
      "Epoch 29/100\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.15887\n",
      "574/574 - 8s - 14ms/step - accuracy: 0.9820 - loss: 0.0817 - val_accuracy: 0.9607 - val_loss: 0.2028 - learning_rate: 3.5709e-04\n",
      "Epoch 30/100\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 0.00017854664474725723.\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.15887\n",
      "574/574 - 8s - 13ms/step - accuracy: 0.9818 - loss: 0.0807 - val_accuracy: 0.9685 - val_loss: 0.1759 - learning_rate: 3.5709e-04\n",
      "Epoch 30: early stopping\n",
      "Restoring model weights from the end of the best epoch: 20.\n",
      "최종 LOSO 교차 검증 완료.\n",
      "\n",
      "===== 성능 비교 결과 =====\n",
      "Fold 수: 26\n",
      "베이스라인 평균 F1: 0.7706 (표준편차: 0.1395)\n",
      "최종 모델 평균 F1: 0.7892 (표준편차: 0.1471)\n",
      "\n",
      "대응표본 t-test: t = -1.3486, p = 0.189542\n",
      "Wilcoxon signed-rank: W = 125.0000, p = 0.207854\n",
      "\n",
      "효과크기(Cohen's dz): 0.264\n",
      "평균 차이(Optimized - Baseline): 0.0186\n",
      "부트스트랩 95% CI (평균 차이): [-0.0079, 0.0462]\n",
      "결론(t-test): 두 모델 간 성능 차이는 통계적으로 유의하지 않습니다. ➖\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2400x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== 최종 모델 종합 학습 곡선 (모든 Fold 평균) =====\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==== Imports (필요 모듈) =====================================================\n",
    "import os, copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from IPython.display import display, Image\n",
    "import optuna\n",
    "\n",
    "# 선택 방식: 'best' | 'rank' | 'number'\n",
    "SELECT_BY = 'rank'          # <- 여기만 바꾸면 돼요\n",
    "SELECTED_RANK = 6           # ex) 5위 트라이얼 사용\n",
    "SELECTED_TRIAL_NUMBER = 5   # ex) trial number가 6인 트라이얼 사용\n",
    "\n",
    "# 스터디에서 전체 trial 테이블 불러오고, 완료된 것만 정렬\n",
    "df = study.trials_dataframe(attrs=(\n",
    "    \"number\",\"value\",\"state\",\"params\",\"datetime_start\",\"datetime_complete\"\n",
    "))\n",
    "df = df[df[\"state\"] == \"COMPLETE\"].copy()\n",
    "df = df.sort_values(\"value\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# 행 선택\n",
    "if SELECT_BY == 'best':\n",
    "    row = df.iloc[0]\n",
    "elif SELECT_BY == 'rank':\n",
    "    if not (1 <= SELECTED_RANK <= len(df)):\n",
    "        raise ValueError(f\"SELECTED_RANK가 범위를 벗어났습니다: 1..{len(df)}\")\n",
    "    row = df.iloc[SELECTED_RANK - 1]\n",
    "elif SELECT_BY == 'number':\n",
    "    if SELECTED_TRIAL_NUMBER not in df[\"number\"].values:\n",
    "        raise ValueError(f\"trial number {SELECTED_TRIAL_NUMBER} 를 찾을 수 없습니다.\")\n",
    "    row = df[df[\"number\"] == SELECTED_TRIAL_NUMBER].iloc[0]\n",
    "else:\n",
    "    raise ValueError(\"SELECT_BY must be one of: 'best' | 'rank' | 'number'\")\n",
    "\n",
    "# params_* 컬럼만 추출 → 딕셔너리로 변환\n",
    "param_cols = [c for c in df.columns if c.startswith(\"params_\")]\n",
    "\n",
    "best_params_from_optuna = {}\n",
    "for c in param_cols:\n",
    "    key = c.replace(\"params_\", \"\")   # 예: 'params_cf_filters' -> 'cf_filters'\n",
    "    val = row[c]\n",
    "\n",
    "    # 타입 캐스팅 (정수 하이퍼파라미터 목록)\n",
    "    int_keys = {\n",
    "        \"WINDOW_SIZE\", \"STRIDE\", \"batch_size\",\n",
    "        \"gru1_units\", \"gru2_units\", \"dense_units\",\n",
    "        \"cf_filters\", \"cf_kernel\", \"cf_pool\"\n",
    "    }\n",
    "    try:\n",
    "        if key in int_keys:\n",
    "            best_params_from_optuna[key] = int(val)\n",
    "        else:\n",
    "            best_params_from_optuna[key] = float(val)\n",
    "    except Exception:\n",
    "        # 숫자로 변환 불가(희귀케이스)는 원값 넣기\n",
    "        best_params_from_optuna[key] = val\n",
    "\n",
    "print(\"✅ 선택된 트라이얼 파라미터로 진행합니다.\")\n",
    "print(\"   SELECT_BY =\", SELECT_BY, \"| trial number =\", int(row[\"number\"]), \"| rank =\", (df.index[df[\"number\"] == row[\"number\"]][0] + 1))\n",
    "print(\"   F1(value) =\", float(row[\"value\"]))\n",
    "\n",
    "# ==== 2) 최종 설정 구성 (키 안전 + Optuna 결과 반영) ==========================\n",
    "final_optimized_config = copy.deepcopy(best_config_base)  # base에는 best_architecture/feature/temporal 반영되어 있음\n",
    "final_optimized_config.setdefault('paths', {})\n",
    "final_optimized_config.setdefault('settings', {})\n",
    "final_optimized_config.setdefault('feature_extraction', {})\n",
    "final_optimized_config.setdefault('model_arch', {})\n",
    "final_optimized_config['model_arch'].setdefault('conv_frontend', {})\n",
    "final_optimized_config.setdefault('training', {})\n",
    "\n",
    "# 출력 디렉토리 및 시각화 저장 설정\n",
    "final_optimized_config['paths']['OUTPUT_DIR'] = 'outputs/step4_final_loso'\n",
    "final_optimized_config['settings']['SAVE_VISUALIZATIONS'] = True\n",
    "os.makedirs(final_optimized_config['paths']['OUTPUT_DIR'], exist_ok=True)\n",
    "\n",
    "# Optuna 결과 적용\n",
    "fe_updates = {k: v for k, v in best_params_from_optuna.items() if k in ['WINDOW_SIZE', 'STRIDE']}\n",
    "ma_updates = {k: v for k, v in best_params_from_optuna.items() if k in ['gru1_units', 'gru2_units', 'dense_units', 'dropout_rate', 'l2']}\n",
    "cf_updates = {k.replace('cf_', ''): v for k, v in best_params_from_optuna.items() if k.startswith('cf_')}\n",
    "tr_updates = {k: v for k, v in best_params_from_optuna.items() if k in ['learning_rate', 'batch_size']}\n",
    "\n",
    "final_optimized_config['feature_extraction'].update(fe_updates)\n",
    "final_optimized_config['model_arch'].update(ma_updates)\n",
    "final_optimized_config['model_arch']['conv_frontend'].update(cf_updates)\n",
    "final_optimized_config['training'].update(tr_updates)\n",
    "\n",
    "# ==== 2-1) FEATURES_PATH 자동 채우기 (KeyError 해결) =========================\n",
    "# - pipeline.run_pipeline에서 cfg['paths']['FEATURES_PATH']를 요구\n",
    "# - 없거나 경로가 유효하지 않으면 runner.get_or_create_features로 생성/가져오기\n",
    "features_path = final_optimized_config['paths'].get('FEATURES_PATH')\n",
    "if not features_path or not os.path.exists(features_path):\n",
    "    try:\n",
    "        # 우선 최종 config로 시도\n",
    "        features_path = runner.get_or_create_features(final_optimized_config)\n",
    "    except Exception:\n",
    "        # 실패 시 원래 MANUAL_CONFIG 기준으로 fallback\n",
    "        features_path = runner.get_or_create_features(config.MANUAL_CONFIG)\n",
    "    if not features_path or not os.path.exists(features_path):\n",
    "        raise FileNotFoundError(\"특징 파일 생성/탐색에 실패했습니다. get_or_create_features를 확인하세요.\")\n",
    "    final_optimized_config['paths']['FEATURES_PATH'] = features_path\n",
    "\n",
    "# ==== 3) 최종 LOSO 파이프라인 실행 ==========================================\n",
    "print(\"최종 최적화된 설정으로 전체 LOSO 교차 검증을 시작합니다. 시간이 많이 소요됩니다...\")\n",
    "pipeline.run_pipeline(final_optimized_config)\n",
    "print(\"최종 LOSO 교차 검증 완료.\")\n",
    "\n",
    "# ==== 4) 베이스라인/최종 결과 로드 + fold 정합 ================================\n",
    "baseline_output_dir = 'outputs/step1_loso_test'\n",
    "optimized_output_dir = final_optimized_config['paths']['OUTPUT_DIR']\n",
    "baseline_report_path = os.path.join(baseline_output_dir, 'models', 'final_loso_cv_report.csv')\n",
    "optimized_report_path = os.path.join(optimized_output_dir, 'models', 'final_loso_cv_report.csv')\n",
    "\n",
    "baseline_df = pd.read_csv(baseline_report_path)\n",
    "optimized_df = pd.read_csv(optimized_report_path)\n",
    "\n",
    "b_macro = baseline_df[baseline_df['Unnamed: 0'] == 'macro avg'][['fold','f1-score']].rename(columns={'f1-score':'baseline_f1'})\n",
    "o_macro = optimized_df[optimized_df['Unnamed: 0'] == 'macro avg'][['fold','f1-score']].rename(columns={'f1-score':'optimized_f1'})\n",
    "\n",
    "# fold 타입/정렬 통일\n",
    "b_macro['fold'] = b_macro['fold'].astype(str)\n",
    "o_macro['fold'] = o_macro['fold'].astype(str)\n",
    "\n",
    "paired = pd.merge(b_macro, o_macro, on='fold', how='inner').sort_values('fold')\n",
    "paired['baseline_f1'] = pd.to_numeric(paired['baseline_f1'], errors='coerce')\n",
    "paired['optimized_f1'] = pd.to_numeric(paired['optimized_f1'], errors='coerce')\n",
    "paired = paired.dropna(subset=['baseline_f1','optimized_f1']).reset_index(drop=True)\n",
    "\n",
    "# 정합 경고\n",
    "missing_in_opt = set(b_macro['fold']) - set(o_macro['fold'])\n",
    "missing_in_base = set(o_macro['fold']) - set(b_macro['fold'])\n",
    "if missing_in_opt:\n",
    "    print(\"[경고] 최적화 결과에 없는 fold:\", sorted(missing_in_opt))\n",
    "if missing_in_base:\n",
    "    print(\"[경고] 베이스라인에 없는 fold:\", sorted(missing_in_base))\n",
    "\n",
    "baseline_scores = paired['baseline_f1'].values\n",
    "optimized_scores = paired['optimized_f1'].values\n",
    "\n",
    "# ==== 5) 통계적 유의성 검증 (대응 t-test + Wilcoxon + 효과크기 + CI) ==========\n",
    "t_stat, p_value = stats.ttest_rel(baseline_scores, optimized_scores)\n",
    "try:\n",
    "    w_stat, w_p = stats.wilcoxon(baseline_scores, optimized_scores, zero_method='wilcox', correction=False)\n",
    "except ValueError:\n",
    "    w_stat, w_p = (np.nan, np.nan)\n",
    "\n",
    "diff = paired['optimized_f1'] - paired['baseline_f1']\n",
    "n = len(diff)\n",
    "mean_diff = diff.mean()\n",
    "sd_diff = diff.std(ddof=1)\n",
    "cohens_dz = mean_diff / sd_diff if sd_diff > 0 else np.nan\n",
    "\n",
    "# 부트스트랩 95% CI (평균 차이)\n",
    "rng = np.random.default_rng(42)\n",
    "boot = [rng.choice(diff.values, size=n, replace=True).mean() for _ in range(10000)]\n",
    "ci_low, ci_high = np.percentile(boot, [2.5, 97.5])\n",
    "\n",
    "print(\"\\n===== 성능 비교 결과 =====\")\n",
    "print(f\"Fold 수: {n}\")\n",
    "print(f\"베이스라인 평균 F1: {baseline_scores.mean():.4f} (표준편차: {baseline_scores.std(ddof=1):.4f})\")\n",
    "print(f\"최종 모델 평균 F1: {optimized_scores.mean():.4f} (표준편차: {optimized_scores.std(ddof=1):.4f})\")\n",
    "print(f\"\\n대응표본 t-test: t = {t_stat:.4f}, p = {p_value:.6f}\")\n",
    "if not np.isnan(w_stat):\n",
    "    print(f\"Wilcoxon signed-rank: W = {w_stat:.4f}, p = {w_p:.6f}\")\n",
    "print(f\"\\n효과크기(Cohen's dz): {cohens_dz:.3f}\")\n",
    "print(f\"평균 차이(Optimized - Baseline): {mean_diff:.4f}\")\n",
    "print(f\"부트스트랩 95% CI (평균 차이): [{ci_low:.4f}, {ci_high:.4f}]\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"결론(t-test): 두 모델 간 성능 차이는 통계적으로 유의합니다. ✅\")\n",
    "else:\n",
    "    print(\"결론(t-test): 두 모델 간 성능 차이는 통계적으로 유의하지 않습니다. ➖\")\n",
    "\n",
    "# ==== 6) 성능 비교 시각화 (Box + 연결선) =====================================\n",
    "comparison_data = pd.DataFrame({\n",
    "    'Fold': paired['fold'],\n",
    "    'Baseline': paired['baseline_f1'],\n",
    "    'Optimized': paired['optimized_f1']\n",
    "})\n",
    "melted = comparison_data.melt(id_vars='Fold', var_name='Model', value_name='F1 Score')\n",
    "\n",
    "# 박스 + 스트립\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.boxplot(x='Model', y='F1 Score', data=melted)\n",
    "sns.stripplot(x='Model', y='F1 Score', data=melted, color=\".25\", alpha=0.6)\n",
    "plt.title('베이스라인 vs 최종 최적화 모델 성능 비교')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 연결 플롯(폴드별 변화)\n",
    "plt.figure(figsize=(10,7))\n",
    "for _, row in comparison_data.iterrows():\n",
    "    plt.plot(['Baseline','Optimized'], [row['Baseline'], row['Optimized']], marker='o', alpha=0.5)\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('Fold별 성능 변화(연결 플롯)')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ==== 7) 혼동 행렬 비교 (전체 fold 합산) ======================================\n",
    "def get_total_confusion_matrix(output_dir, class_names):\n",
    "    y_true_all, y_pred_all = [], []\n",
    "    models_dir = os.path.join(output_dir, 'models')\n",
    "    if not os.path.isdir(models_dir):\n",
    "        return None\n",
    "    for fold_dir in os.listdir(models_dir):\n",
    "        if fold_dir.startswith('fold_'):\n",
    "            pred_path = os.path.join(models_dir, fold_dir, 'raw_predictions.npz')\n",
    "            if os.path.exists(pred_path):\n",
    "                with np.load(pred_path, allow_pickle=True) as data:\n",
    "                    y_true_all.extend(data['y_true'])\n",
    "                    y_pred_all.extend(data['y_pred'])\n",
    "    if not y_true_all:\n",
    "        return None\n",
    "    cm = confusion_matrix(y_true_all, y_pred_all, labels=np.arange(len(class_names)))\n",
    "    return cm\n",
    "\n",
    "with np.load(runner.get_or_create_features(config.MANUAL_CONFIG), allow_pickle=True) as d:\n",
    "    class_names = d['class_names']\n",
    "\n",
    "cm_baseline = get_total_confusion_matrix(baseline_output_dir, class_names)\n",
    "cm_optimized = get_total_confusion_matrix(optimized_output_dir, class_names)\n",
    "\n",
    "if cm_baseline is not None and cm_optimized is not None:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(24, 10))\n",
    "    sns.heatmap(cm_baseline, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names, ax=axes[0])\n",
    "    axes[0].set_title('베이스라인 모델 혼동 행렬'); axes[0].set_xlabel('Predicted'); axes[0].set_ylabel('True')\n",
    "    sns.heatmap(cm_optimized, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names, ax=axes[1])\n",
    "    axes[1].set_title('최종 최적화 모델 혼동 행렬'); axes[1].set_xlabel('Predicted'); axes[1].set_ylabel('True')\n",
    "    plt.tight_layout(); plt.show()\n",
    "else:\n",
    "    print(\"혼동 행렬 파일(raw_predictions.npz)을 찾지 못해 비교를 건너뜁니다.\")\n",
    "\n",
    "# ==== 8) 최종 모델 학습 곡선(집계 이미지) =====================================\n",
    "print(\"\\n===== 최종 모델 종합 학습 곡선 (모든 Fold 평균) =====\")\n",
    "agg_history_plot_path = os.path.join(optimized_output_dir, 'models', 'aggregated_training_history.png')\n",
    "if os.path.exists(agg_history_plot_path):\n",
    "    display(Image(filename=agg_history_plot_path))\n",
    "else:\n",
    "    print(\"종합 학습 곡선 이미지 파일을 찾을 수 없습니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae89096",
   "metadata": {},
   "source": [
    "📦 최고 성능 Fold 모델 선택·복사 (배포용 싱글 모델 준비)\n",
    "\n",
    "step4_final_loso 결과에서 Macro F1이 가장 높은 Fold를 찾아,\n",
    "해당 Fold의 학습된 모델 파일과 주요 아티팩트를 step5_single_from_bestfold로 복사합니다.\n",
    "(추가 학습 없음)\n",
    "\n",
    "🔧 절차\n",
    "\n",
    "리포트 로드: final_loso_cv_report.csv에서 macro avg 행만 추출\n",
    "\n",
    "최고 Fold 선택: fold별 f1-score 평균을 기준으로 최상위 선택\n",
    "\n",
    "모델 복사: best_model.keras(우선) → 없으면 .h5 등 후보 순회\n",
    "\n",
    "아티팩트 복사(선택): history.csv, training_history.png, raw_predictions.npz, classification_report.csv\n",
    "\n",
    "클래스 이름 저장(선택): class_names.txt로 내보내 재현성 확보\n",
    "\n",
    "🛡️ 안전장치\n",
    "\n",
    "norm_fold_id()로 폴더명 'fold_7' 형식 통일 → fold_fold_7 같은 중복 접두 오류 방지\n",
    "\n",
    "경로/파일 존재 여부 확인 후 친절한 오류 메시지 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b909504c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[선택] 최고 성능 Fold = 26  |  Macro F1 = 0.9929\n",
      "[복사] outputs/step4_final_loso\\models\\fold_26\\best_model.keras  →  outputs/step5_single_from_bestfold\\best_model.keras\n",
      "[아티팩트 복사] history.csv, training_history.png, raw_predictions.npz, classification_report.csv\n",
      "[클래스 저장] outputs/step5_single_from_bestfold\\class_names.txt  (총 13개)\n",
      "\n",
      "[완료] step4_final_loso의 최고 성능 fold 모델을 단일 배포용으로 준비했습니다.\n",
      " - 배포 폴더: outputs/step5_single_from_bestfold\n",
      " - 모델 파일: best_model.keras\n"
     ]
    }
   ],
   "source": [
    "# === step4_final_loso에서 최고 성능 fold 모델 선택·복사 (학습 X) ===\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- 유틸: fold 표기 정규화 ---\n",
    "def norm_fold_id(fid):\n",
    "    s = str(fid)\n",
    "    return s if s.startswith(\"fold_\") else f\"fold_{s}\"\n",
    "\n",
    "# 0) 입력/출력 경로\n",
    "step4_dir = 'outputs/step4_final_loso'\n",
    "report_path = os.path.join(step4_dir, 'models', 'final_loso_cv_report.csv')\n",
    "deploy_dir = 'outputs/step5_single_from_bestfold'\n",
    "os.makedirs(deploy_dir, exist_ok=True)\n",
    "\n",
    "# 1) step4 결과 리포트에서 최고 성능 fold 찾기 (macro avg F1 기준)\n",
    "df = pd.read_csv(report_path)\n",
    "macro = df[df['Unnamed: 0'] == 'macro avg'].copy()\n",
    "\n",
    "# fold/점수 정리\n",
    "macro['fold'] = macro['fold'].astype(str)\n",
    "macro['f1-score'] = pd.to_numeric(macro['f1-score'], errors='coerce')\n",
    "\n",
    "# fold별 macro F1이 여러 행이면 평균\n",
    "best = (\n",
    "    macro.groupby('fold', as_index=False)['f1-score']\n",
    "         .mean()\n",
    "         .sort_values('f1-score', ascending=False)\n",
    "         .iloc[0]\n",
    ")\n",
    "\n",
    "best_fold_raw = best['fold']                 # 예: '7' 또는 'fold_7'\n",
    "best_fold = norm_fold_id(best_fold_raw)      # 'fold_7' 형식으로 통일\n",
    "best_f1 = float(best['f1-score'])\n",
    "\n",
    "print(f\"[선택] 최고 성능 Fold = {best_fold}  |  Macro F1 = {best_f1:.4f}\")\n",
    "\n",
    "# 2) 해당 fold의 모델 파일 찾기\n",
    "fold_model_dir = os.path.join(step4_dir, 'models', best_fold)\n",
    "candidates = ['best_model.keras', 'best_model.h5', 'model.keras', 'model.h5']\n",
    "\n",
    "src_model_path = next((os.path.join(fold_model_dir, name)\n",
    "                       for name in candidates\n",
    "                       if os.path.exists(os.path.join(fold_model_dir, name))), None)\n",
    "\n",
    "if src_model_path is None:\n",
    "    raise FileNotFoundError(\n",
    "        \"모델 파일을 찾지 못했습니다. 다음 중 하나가 존재해야 합니다: \"\n",
    "        f\"{', '.join(candidates)}\\n검색 경로: {fold_model_dir}\"\n",
    "    )\n",
    "\n",
    "# 3) 배포 폴더로 복사\n",
    "dst_model_path = os.path.join(deploy_dir, os.path.basename(src_model_path))\n",
    "shutil.copy2(src_model_path, dst_model_path)\n",
    "print(f\"[복사] {src_model_path}  →  {dst_model_path}\")\n",
    "\n",
    "# 4) (선택) raw_predictions/리포트/히스토리 등 유용 아티팩트 복사\n",
    "extra_files = [\n",
    "    'history.csv',\n",
    "    'training_history.png',\n",
    "    'raw_predictions.npz',\n",
    "    'classification_report.csv'\n",
    "]\n",
    "copied = []\n",
    "for fname in extra_files:\n",
    "    p = os.path.join(fold_model_dir, fname)\n",
    "    if os.path.exists(p):\n",
    "        dst = os.path.join(deploy_dir, f\"{best_fold}_{fname}\")\n",
    "        shutil.copy2(p, dst)\n",
    "        copied.append(fname)\n",
    "\n",
    "if copied:\n",
    "    print(\"[아티팩트 복사]\", \", \".join(copied))\n",
    "else:\n",
    "    print(\"[알림] 복사할 추가 아티팩트가 없습니다.\")\n",
    "\n",
    "# 5) (선택) 클래스 이름 저장 시도\n",
    "try:\n",
    "    # final_optimized_config/runner가 세션에 있다고 가정\n",
    "    features_path = None\n",
    "    if 'final_optimized_config' in globals():\n",
    "        features_path = final_optimized_config['paths'].get('FEATURES_PATH')\n",
    "\n",
    "    if not features_path or not os.path.exists(features_path):\n",
    "        # 동일 설정으로 재생성/탐색\n",
    "        features_path = runner.get_or_create_features(final_optimized_config)\n",
    "\n",
    "    class_names_txt = os.path.join(deploy_dir, 'class_names.txt')\n",
    "    with np.load(features_path, allow_pickle=True) as d:\n",
    "        class_names = d['class_names']\n",
    "\n",
    "    with open(class_names_txt, 'w', encoding='utf-8') as f:\n",
    "        for c in class_names:\n",
    "            f.write(str(c) + '\\n')\n",
    "\n",
    "    print(f\"[클래스 저장] {class_names_txt}  (총 {len(class_names)}개)\")\n",
    "except Exception as e:\n",
    "    print(f\"[경고] class_names 저장을 건너뜁니다: {e}\")\n",
    "\n",
    "print(\"\\n[완료] step4_final_loso의 최고 성능 fold 모델을 단일 배포용으로 준비했습니다.\")\n",
    "print(f\" - 배포 폴더: {deploy_dir}\")\n",
    "print(f\" - 모델 파일: {os.path.basename(dst_model_path)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51050deb",
   "metadata": {},
   "source": [
    "📦 TFLite 내보내기 (GRU 호환성까지 고려한 탄력 변환)\n",
    "\n",
    "이 스크립트는 Keras 모델을 3단계 전략으로 TFLite로 변환합니다.\n",
    "\n",
    "BUILTINS만으로 변환 → 2) SELECT_TF_OPS 허용 → 3) GRU를 unroll=True로 재구성 후 BUILTINS 재시도.\n",
    "GRU/텐서리스트 호환 문제로 TFLite 변환이 실패하는 경우까지 자동 복구 경로를 제공합니다.\n",
    "\n",
    "🔧 사용법\n",
    "\n",
    "__main__ 하단의 model_path, export_dir만 환경에 맞게 수정 후 실행\n",
    "\n",
    "(선택) INT8 양자화를 원하면 _safe_convert_tflite의 주석 처리된 구간 활성화 + representative_dataset 제공\n",
    "\n",
    "🧠 구현 포인트\n",
    "\n",
    "SELECT_TF_OPS: 변환은 성공해도 모바일 배포 시 바이너리/런타임 의존성이 커질 수 있음\n",
    "\n",
    "unroll=True 재구성: 순환 연산을 전개하여 TFLite BUILTINS 호환성 개선\n",
    "\n",
    "가중치 이식: 동일 레이어명 기준으로 가능한 범위에서 가중치 이식\n",
    "\n",
    "⚠️ 주의\n",
    "\n",
    "원 모델 구조가 복잡한 경우(커스텀 레이어 등) unroll 재구성으로 완벽한 동등성이 보장되지는 않습니다.\n",
    "\n",
    "변환 후에는 샘플 입력으로 추론 값 검증을 권장합니다. (예: 상위 N개 클래스/로짓 유사성 확인)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb92b9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[선택] 변환 대상 모델: outputs/step4_final_loso/models/fold_26/best_model.keras\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\ppenn\\AppData\\Local\\Temp\\tmpsfrn75r2\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\ppenn\\AppData\\Local\\Temp\\tmpsfrn75r2\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\ppenn\\AppData\\Local\\Temp\\tmpsfrn75r2'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 30, 108), dtype=tf.float32, name='input_layer')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 13), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2084970121168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2084976779760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2084976784688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2084976968272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2084976969856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2084976973552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2084976975136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2084976976016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2084976977072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2084976976192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2084976974960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2084976982880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2084976983584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2084976986768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2084976986240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "[실패] 기본 변환: <unknown>:0: error: loc(callsite(fused[\"TensorListReserve:\", \"functional_1/gru_1/TensorArrayV2_1@__inference_function_10020224\"] at callsite(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_10020295\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]))): 'tf.TensorListReserve' op requires element_shape to be static during TF Lite transformation pass\n",
      "<unknown>:0: note: loc(callsite(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_10020295\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): called from\n",
      "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
      "<unknown>:0: error: loc(callsite(fused[\"TensorListReserve:\", \"functional_1/gru_1/TensorArrayV2_1@__inference_function_10020224\"] at callsite(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_10020295\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]))): failed to legalize operation 'tf.TensorListReserve' that was explicitly marked illegal\n",
      "<unknown>:0: note: loc(callsite(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_10020295\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): called from\n",
      "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
      "<unknown>:0: error: Lowering tensor list ops is failed. Please consider using Select TF ops and disabling `_experimental_lower_tensor_list_ops` flag in the TFLite converter object. For example, converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\\n converter._experimental_lower_tensor_list_ops = False\n",
      "\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\ppenn\\AppData\\Local\\Temp\\tmp5q5jh2wc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\ppenn\\AppData\\Local\\Temp\\tmp5q5jh2wc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\ppenn\\AppData\\Local\\Temp\\tmp5q5jh2wc'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 30, 108), dtype=tf.float32, name='input_layer')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 13), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2084970121168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2084976779760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2084976784688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2084976968272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2084976969856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2084976973552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2084976975136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2084976976016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2084976977072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2084976976192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2084976974960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2084976982880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2084976983584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2084976986768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2084976986240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "[OK] TFLite 변환 성공(SELECT_TF_OPS): outputs/step4_final_loso/exports\\final_model.tflite\n",
      "[완료] 최종 TFLite 경로: outputs/step4_final_loso/exports\\final_model.tflite\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "def _safe_convert_tflite(\n",
    "    keras_model,\n",
    "    tflite_out_path: str,\n",
    "    use_select_tf_ops: bool = False,\n",
    "    representative_dataset=None,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Keras 모델을 TFLite로 안전하게 변환하여 저장합니다.\n",
    "\n",
    "    Args:\n",
    "        keras_model: tf.keras.Model 인스턴스\n",
    "        tflite_out_path: 저장할 .tflite 파일 경로\n",
    "        use_select_tf_ops: True면 SELECT_TF_OPS 허용\n",
    "        representative_dataset: INT8 양자화를 위한 대표 데이터셋 함수(옵션)\n",
    "\n",
    "    Returns:\n",
    "        저장된 .tflite 파일 경로\n",
    "    \"\"\"\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\n",
    "\n",
    "    # (옵션) 양자화 예시\n",
    "    # converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    # if representative_dataset is not None:\n",
    "    #     converter.representative_dataset = representative_dataset\n",
    "    #     converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "    #     converter.inference_input_type = tf.int8\n",
    "    #     converter.inference_output_type = tf.int8\n",
    "\n",
    "    if use_select_tf_ops:\n",
    "        converter.target_spec.supported_ops = [\n",
    "            tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "            tf.lite.OpsSet.SELECT_TF_OPS,\n",
    "        ]\n",
    "        # 일부 환경에서 TensorList 하향 변환 방지\n",
    "        try:\n",
    "            converter._experimental_lower_tensor_list_ops = False  # pylint: disable=protected-access\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    tflite_model = converter.convert()\n",
    "    os.makedirs(os.path.dirname(tflite_out_path), exist_ok=True)\n",
    "    with open(tflite_out_path, \"wb\") as f:\n",
    "        f.write(tflite_model)\n",
    "    return tflite_out_path\n",
    "\n",
    "\n",
    "def _rebuild_with_unroll_and_load_weights(original_model: tf.keras.Model, input_shape) -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    GRU 레이어를 unroll=True로 재구성한 새 모델을 만들고,\n",
    "    가능한 동일 이름 레이어의 가중치를 이식합니다.\n",
    "\n",
    "    Notes:\n",
    "        - 원 모델의 Conv 프런트엔드를 최대한 간단히 재현합니다.\n",
    "        - GRU 스택은 동일 유닛/return_sequences 구성으로 unroll=True로 재생성합니다.\n",
    "        - Dense/Dropout/Output은 구조를 추정하여 재구성합니다.\n",
    "    \"\"\"\n",
    "    from tensorflow.keras import layers, Model\n",
    "\n",
    "    inp = layers.Input(shape=input_shape[1:], dtype=tf.float32, name=\"input_layer\")\n",
    "    x = layers.Rescaling(1.0, dtype=\"float32\", name=\"cast_to_float32\")(inp)\n",
    "    x = layers.GaussianNoise(0.01)(x)\n",
    "\n",
    "    # --- Conv 프론트엔드 (있으면 간단 재현) ---\n",
    "    def _maybe_conv_frontend(x_in, m: tf.keras.Model):\n",
    "        has_conv = any(isinstance(lyr, (layers.Conv1D, layers.SeparableConv1D)) for lyr in m.layers)\n",
    "        if not has_conv:\n",
    "            return x_in\n",
    "        x_out = x_in\n",
    "        for lyr in m.layers:\n",
    "            if isinstance(lyr, layers.SeparableConv1D):\n",
    "                x_out = layers.SeparableConv1D(\n",
    "                    filters=lyr.filters,\n",
    "                    kernel_size=lyr.kernel_size[0],\n",
    "                    strides=lyr.strides[0],\n",
    "                    padding=lyr.padding,\n",
    "                    activation=\"relu\",\n",
    "                    depthwise_regularizer=lyr.depthwise_regularizer,\n",
    "                    pointwise_regularizer=lyr.pointwise_regularizer,\n",
    "                    name=lyr.name,\n",
    "                )(x_out)\n",
    "            elif isinstance(lyr, layers.Conv1D):\n",
    "                x_out = layers.Conv1D(\n",
    "                    filters=lyr.filters,\n",
    "                    kernel_size=lyr.kernel_size[0],\n",
    "                    strides=lyr.strides[0],\n",
    "                    padding=lyr.padding,\n",
    "                    activation=\"relu\",\n",
    "                    kernel_regularizer=lyr.kernel_regularizer,\n",
    "                    name=lyr.name,\n",
    "                )(x_out)\n",
    "            elif isinstance(lyr, layers.MaxPooling1D):\n",
    "                x_out = layers.MaxPooling1D(pool_size=lyr.pool_size[0], name=lyr.name)(x_out)\n",
    "            elif isinstance(lyr, layers.LayerNormalization):\n",
    "                x_out = layers.LayerNormalization(name=lyr.name)(x_out)\n",
    "            elif isinstance(lyr, layers.Dropout):\n",
    "                x_out = layers.Dropout(lyr.rate, name=lyr.name)(x_out)\n",
    "        return x_out\n",
    "\n",
    "    x = _maybe_conv_frontend(x, original_model)\n",
    "\n",
    "    # --- GRU 스택: unroll=True로 재구성 ---\n",
    "    gru_cfg = []\n",
    "    for lyr in original_model.layers:\n",
    "        if isinstance(lyr, tf.keras.layers.GRU):\n",
    "            gru_cfg.append(\n",
    "                dict(\n",
    "                    units=lyr.units,\n",
    "                    return_sequences=lyr.return_sequences,\n",
    "                    dropout=lyr.dropout,\n",
    "                    recurrent_dropout=lyr.recurrent_dropout,\n",
    "                    kernel_regularizer=lyr.kernel_regularizer,\n",
    "                    name=lyr.name,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    for cfg in gru_cfg:\n",
    "        x = tf.keras.layers.GRU(\n",
    "            units=cfg[\"units\"],\n",
    "            return_sequences=cfg[\"return_sequences\"],\n",
    "            dropout=cfg[\"dropout\"],\n",
    "            recurrent_dropout=cfg[\"recurrent_dropout\"],\n",
    "            kernel_regularizer=cfg[\"kernel_regularizer\"],\n",
    "            unroll=True,\n",
    "            name=cfg[\"name\"],\n",
    "        )(x)\n",
    "\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D(name=\"rebuild_gap\")(x)\n",
    "\n",
    "    # --- Dense/Dropout/Output 재현 ---\n",
    "    dense_units = None\n",
    "    dropout_rate = 0.0\n",
    "    n_classes = original_model.output_shape[-1]\n",
    "\n",
    "    for l in original_model.layers:\n",
    "        if isinstance(l, tf.keras.layers.Dense) and getattr(l.activation, \"__name__\", \"\") == \"relu\":\n",
    "            dense_units = l.units\n",
    "        if isinstance(l, tf.keras.layers.Dropout):\n",
    "            dropout_rate = l.rate\n",
    "\n",
    "    dense_units = dense_units or 64\n",
    "\n",
    "    x = tf.keras.layers.Dense(dense_units, activation=\"relu\", name=\"rebuild_dense\")(x)\n",
    "    if dropout_rate and dropout_rate > 1e-6:\n",
    "        x = tf.keras.layers.Dropout(dropout_rate, name=\"rebuild_dropout_final\")(x)\n",
    "    out = tf.keras.layers.Dense(n_classes, activation=\"softmax\", dtype=tf.float32, name=\"pred\")(x)\n",
    "\n",
    "    new_model = Model(inp, out, name=\"rebuilt_unrolled_model\")\n",
    "\n",
    "    # 동일 이름 레이어 가중치 이식 (가능한 범위)\n",
    "    for l_old in original_model.layers:\n",
    "        try:\n",
    "            l_new = new_model.get_layer(l_old.name)\n",
    "            l_new.set_weights(l_old.get_weights())\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    print(\"[INFO] unroll=True 모델에 가중치 이식 완료(가능한 범위).\")\n",
    "    return new_model\n",
    "\n",
    "\n",
    "def export_tflite_resilient(model_path: str, export_dir: str) -> str:\n",
    "    \"\"\"\n",
    "    3단계 전략으로 Keras 모델을 TFLite로 변환합니다.\n",
    "      1) BUILTINS만 사용\n",
    "      2) SELECT_TF_OPS 허용\n",
    "      3) GRU를 unroll=True로 재구성한 뒤 BUILTINS로 재시도\n",
    "    \"\"\"\n",
    "    os.makedirs(export_dir, exist_ok=True)\n",
    "    tflite_path = os.path.join(export_dir, \"final_model.tflite\")\n",
    "\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "    # 1) BUILTINS만으로 시도\n",
    "    try:\n",
    "        _safe_convert_tflite(model, tflite_path, use_select_tf_ops=False)\n",
    "        print(f\"[OK] TFLite 변환 성공(기본 BUILTINS): {tflite_path}\")\n",
    "        return tflite_path\n",
    "    except Exception as e1:\n",
    "        print(f\"[실패] 기본 BUILTINS 변환: {e1}\")\n",
    "\n",
    "    # 2) SELECT_TF_OPS 허용\n",
    "    try:\n",
    "        _safe_convert_tflite(model, tflite_path, use_select_tf_ops=True)\n",
    "        print(f\"[OK] TFLite 변환 성공(SELECT_TF_OPS): {tflite_path}\")\n",
    "        return tflite_path\n",
    "    except Exception as e2:\n",
    "        print(f\"[실패] SELECT_TF_OPS 변환: {e2}\")\n",
    "\n",
    "    # 3) GRU unroll=True 재구성 후 BUILTINS로 시도\n",
    "    try:\n",
    "        input_shape = model.inputs[0].shape\n",
    "        unrolled = _rebuild_with_unroll_and_load_weights(model, input_shape)\n",
    "        _safe_convert_tflite(unrolled, tflite_path, use_select_tf_ops=False)\n",
    "        print(f\"[OK] TFLite 변환 성공(unroll=True + BUILTINS): {tflite_path}\")\n",
    "        return tflite_path\n",
    "    except Exception as e3:\n",
    "        print(f\"[실패] unroll=True 변환: {e3}\")\n",
    "        raise RuntimeError(\n",
    "            \"TFLite 변환에 최종 실패했습니다. GRU 구성을 더 단순화하거나 LSTM/Conv-only로 검토하세요.\"\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ↓↓↓ 환경에 맞게 수정 ↓↓↓\n",
    "    model_path = r\"outputs/step4_final_loso/models/fold_26/best_model.keras\"\n",
    "    export_dir = r\"outputs/step4_final_loso/exports\"\n",
    "    # ↑↑↑ 환경에 맞게 수정 ↑↑↑\n",
    "\n",
    "    print(f\"[선택] 변환 대상 모델: {model_path}\")\n",
    "    out_path = export_tflite_resilient(model_path, export_dir)\n",
    "    print(\"[완료] 최종 TFLite 경로:\", out_path)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chicachu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
